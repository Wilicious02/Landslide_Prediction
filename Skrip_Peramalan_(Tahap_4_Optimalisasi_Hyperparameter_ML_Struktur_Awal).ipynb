{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "================================================================================\n",
        "                      SKRIP PERAMALAN SPASIAL-TEMPORAL V3.1 - TAHAP 4\n",
        "\n",
        "Versi Skrip: 3.1 - Tahap 4 (Optimalisasi Parameter & Struktur Hyperparameter ML)\n",
        "\n",
        "Deskripsi Umum:\n",
        "Skrip ini dirancang untuk melakukan peramalan time series spasial pada data NetCDF.\n",
        "Tujuannya adalah untuk memprediksi nilai variabel target (misalnya, jumlah kejadian)\n",
        "di setiap piksel grid untuk beberapa langkah waktu ke depan.\n",
        "\n",
        "Fitur Utama:\n",
        "1.  Model Peramalan:\n",
        "    - ARIMA: Menggunakan `pmdarima.auto_arima` untuk pemilihan order (p,d,q) otomatis.\n",
        "    - LSTM (Long Short-Term Memory): Jaringan Neural Rekuren.\n",
        "    - ANN (Artificial Neural Network): Jaringan Neural Tiruan standar (Feedforward).\n",
        "    - RF (Random Forest): Model ensemble berbasis Pohon Keputusan.\n",
        "2.  Input Data:\n",
        "    - Data utama dari file NetCDF (.nc).\n",
        "    - Fitur tambahan dari data titik longsor (Shapefile .shp), dikonversi ke grid.\n",
        "3.  Fitur Tambahan yang Dihasilkan:\n",
        "    - Jumlah Kejadian Longsor per Piksel per Bulan.\n",
        "    - Rata-rata Spasial Tetangga (Spatial Average Lag-1): Nilai rata-rata dari 8 piksel\n",
        "      tetangga pada langkah waktu sebelumnya (t-1) untuk variabel target.\n",
        "4.  Evaluasi Model:\n",
        "    - Dilakukan pada periode data historis terakhir.\n",
        "    - Metrik: RMSE (Root Mean Squared Error), R2-score.\n",
        "    - Uji Diagnostik Residual:\n",
        "        - ARIMA: Uji Ljung-Box pada residual in-sample.\n",
        "        - LSTM, ANN, RF: Uji Ljung-Box pada residual in-sample (data yang di-scaled).\n",
        "5.  Peramalan Masa Depan:\n",
        "    - Menghasilkan peramalan untuk periode masa depan setelah model dilatih\n",
        "      pada seluruh data historis.\n",
        "6.  Output:\n",
        "    - Peta metrik evaluasi (RMSE, R2, Ljung-Box p-value) dalam format GeoTIFF dan PNG.\n",
        "    - Plot time series sampel untuk perbandingan observasi vs. prediksi.\n",
        "    - Animasi GIF dari peta peramalan (evaluasi dan masa depan).\n",
        "    - Data peramalan (evaluasi dan masa depan) dalam format GeoTIFF per langkah waktu.\n",
        "    - Data peramalan dan metrik evaluasi dalam format NetCDF.\n",
        "    - Peta orde differencing 'd' yang digunakan oleh ARIMA (dari auto_arima).\n",
        "7.  Penanganan Spasial:\n",
        "    - Pengaturan Coordinate Reference System (CRS) target (EPSG:32749 - WGS 84 / UTM Zone 49S).\n",
        "    - Reprojeksi data longsor ke CRS target.\n",
        "    - Output geospasial disimpan dengan informasi CRS yang sesuai.\n",
        "8.  Struktur Hyperparameter (Tahap 4):\n",
        "    - Fungsi peramalan untuk model ML (LSTM, ANN, RF) dimodifikasi untuk menerima\n",
        "      kamus hyperparameter, memungkinkan fleksibilitas dalam eksperimen.\n",
        "    - Set hyperparameter awal didefinisikan dalam konfigurasi.\n",
        "9.  Lain-lain:\n",
        "    - Output prediksi dipastikan tidak negatif.\n",
        "    - Menggunakan pemrosesan paralel (joblib) untuk mempercepat perhitungan per piksel.\n",
        "    - Logging untuk melacak proses eksekusi.\n"
      ],
      "metadata": {
        "id": "xQvikmP7f0FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMPORTS ---\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "import optuna\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio import features\n",
        "from affine import Affine\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from typing import List, Tuple, Optional, Callable, Dict, Any\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from statsmodels.tsa.arima.model import ARIMA as StatsmodelsARIMA # Alias\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from scipy.ndimage import convolve\n",
        "import tensorflow as tf\n",
        "import rioxarray\n",
        "from pandas.tseries.offsets import DateOffset\n",
        "import pmdarima as pm"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "HCcsd9Y3cRo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- KONFIGURASI ---\n",
        "BASE_DIR = \"D:\\\\DataPenelitian\\\\Longsor\" # GANTI DENGAN DIREKTORI ANDA\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    logging.warning(f\"BASE_DIR '{BASE_DIR}' tidak ditemukan. Menggunakan direktori kerja saat ini.\")\n",
        "    BASE_DIR = \".\"\n",
        "try:\n",
        "    os.chdir(BASE_DIR)\n",
        "    logging.info(f\"Direktori kerja diubah ke: {os.getcwd()}\")\n",
        "except FileNotFoundError:\n",
        "    logging.error(f\"Tidak dapat mengubah direktori ke {BASE_DIR}. Pastikan path benar.\")\n",
        "    # raise SystemExit(f\"Gagal mengubah direktori ke {BASE_DIR}\")\n",
        "\n",
        "TARGET_VARIABLE = 'COUNT'\n",
        "FORECAST_STEPS = 3\n",
        "N_LAG = 24\n",
        "SAMPLE_COORDS = [(0, 1), (35, 14), (0, 0), (27, 6), (35, 11)]\n",
        "ADF_P_THRESHOLD = 0.05\n",
        "MAX_DIFFERENCING = 2\n",
        "LJUNG_BOX_LAGS_CONFIG = 10\n",
        "NC_FILE = \"nc_20250426_1M.nc\"\n",
        "LANDSLIDE_SHP_FILE = \"data_fix/TitikLongsor_Magelang_2025.shp\"\n",
        "LANDSLIDE_DATE_COLUMN = \"Date\"\n",
        "TARGET_CRS = \"EPSG:32749\"\n",
        "\n",
        "OUTPUT_BASE_DIR_EVAL = \"1_Month/forecast_evaluation_1M_spatial_v3.1_tests_tahap4_optim\"\n",
        "OUTPUT_BASE_DIR_FUTURE = \"1_Month/forecast_future_1M_spatial_v3.1_tests_tahap4_optim\"\n",
        "OUTPUT_DIR_ACTUAL_AGG = \"1_Month/actual_aggregation_1M\"\n",
        "\n",
        "LANDSLIDE_FEATURE_NAME = \"LANDSLIDE_COUNT\"\n",
        "SPATIAL_FEATURE_NAME = f\"{TARGET_VARIABLE}_SPATIAL_AVG_LAG1\"\n",
        "\n",
        "HYPERPARAMS_LSTM = {\n",
        "    \"lstm_units\": 32, \"lstm_activation\": 'relu', \"learning_rate\": 0.01,\n",
        "    \"epochs\": 50, \"batch_size_val\": None\n",
        "}\n",
        "HYPERPARAMS_ANN = {\n",
        "    \"ann_layers_units\": [64, 32], \"ann_activation\": 'relu', \"learning_rate\": 0.01,\n",
        "    \"epochs\": 50, \"batch_size_val\": None\n",
        "}\n",
        "HYPERPARAMS_RF = {\n",
        "    \"n_estimators\": 100, \"rf_random_state\": 42, \"max_depth\": None,\n",
        "    \"min_samples_split\": 2, \"min_samples_leaf\": 1\n",
        "}"
      ],
      "metadata": {
        "id": "y22QisCwgQ0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNGSI PEMROSESAN DATA ---\n",
        "def process_landslide_data(\n",
        "    shp_path: str, date_column: str, ds_template: xr.Dataset, feature_name: str = \"LANDSLIDE_COUNT\"\n",
        ") -> Optional[xr.DataArray]:\n",
        "    logging.info(f\"--- Processing Landslide Data from: {shp_path} ---\")\n",
        "    try:\n",
        "        landslide_gdf = gpd.read_file(shp_path)\n",
        "        logging.info(f\"Loaded {len(landslide_gdf)} landslide points from {os.path.basename(shp_path)}.\")\n",
        "        if date_column not in landslide_gdf.columns:\n",
        "            logging.error(f\"❌ Date column '{date_column}' not found in Shapefile.\")\n",
        "            return None\n",
        "        try:\n",
        "            landslide_gdf[date_column] = pd.to_datetime(landslide_gdf[date_column], errors='coerce')\n",
        "            original_count = len(landslide_gdf)\n",
        "            landslide_gdf = landslide_gdf.dropna(subset=[date_column])\n",
        "            if len(landslide_gdf) < original_count:\n",
        "                logging.warning(f\"Removed {original_count - len(landslide_gdf)} points with invalid dates.\")\n",
        "            if landslide_gdf.empty:\n",
        "                 logging.warning(\"No valid landslide points remaining after date processing. Returning zero array.\")\n",
        "                 return xr.DataArray(\n",
        "                    np.zeros((len(ds_template['time']), ds_template.dims['y'], ds_template.dims['x']), dtype=np.int32),\n",
        "                    coords=ds_template.coords, dims=['time', 'y', 'x'], name=feature_name,\n",
        "                    attrs={'long_name': 'Monthly landslide event count', 'units': 'count'}\n",
        "                 )\n",
        "        except Exception as date_err:\n",
        "            logging.error(f\"❌ Error parsing date column '{date_column}': {date_err}\")\n",
        "            return None\n",
        "        ds_template_crs = None\n",
        "        try:\n",
        "            if hasattr(ds_template, 'rio') and ds_template.rio.crs:\n",
        "                ds_template_crs = ds_template.rio.crs\n",
        "                logging.info(f\"Template NetCDF CRS: {ds_template_crs}\")\n",
        "            else:\n",
        "                logging.warning(\"Template NetCDF does not have CRS information via rio accessor. Landslide data will not be reprojected unless TARGET_CRS is used.\")\n",
        "                if TARGET_CRS:\n",
        "                    ds_template_crs = rasterio.crs.CRS.from_string(TARGET_CRS)\n",
        "                    logging.info(f\"Using globally defined TARGET_CRS for template: {ds_template_crs}\")\n",
        "        except Exception as crs_read_err:\n",
        "             logging.warning(f\"Could not read CRS from NetCDF template: {crs_read_err}. Landslide data will not be reprojected unless TARGET_CRS is used.\")\n",
        "             if TARGET_CRS:\n",
        "                ds_template_crs = rasterio.crs.CRS.from_string(TARGET_CRS)\n",
        "                logging.info(f\"Using globally defined TARGET_CRS for template due to error: {ds_template_crs}\")\n",
        "        shp_crs = landslide_gdf.crs\n",
        "        logging.info(f\"Landslide Shapefile CRS: {shp_crs}\")\n",
        "        if ds_template_crs and shp_crs and shp_crs != ds_template_crs:\n",
        "            logging.info(f\"Reprojecting landslide data from {shp_crs} to {ds_template_crs}...\")\n",
        "            try:\n",
        "                landslide_gdf = landslide_gdf.to_crs(ds_template_crs)\n",
        "                logging.info(f\"Landslide data reprojected successfully to {landslide_gdf.crs}.\")\n",
        "            except Exception as crs_err:\n",
        "                logging.error(f\"❌ Failed to reproject landslide data: {crs_err}. Proceeding without reprojection.\")\n",
        "        elif not ds_template_crs: logging.info(\"Skipping landslide reprojection as NetCDF CRS is not set/available.\")\n",
        "        elif not shp_crs: logging.info(\"Skipping landslide reprojection as Shapefile CRS is unknown.\")\n",
        "        elif shp_crs == ds_template_crs: logging.info(\"Landslide data CRS matches template CRS. No reprojection needed.\")\n",
        "\n",
        "        ny, nx = ds_template.dims['y'], ds_template.dims['x']\n",
        "        grid_shape = (ny, nx)\n",
        "        transform = ds_template.rio.transform()\n",
        "        if transform.is_identity and ds_template_crs:\n",
        "            logging.warning(\"NetCDF transform is identity despite having CRS. Rasterization might be incorrect if coordinates are not pixel indices.\")\n",
        "            if 'x' in ds_template.coords and 'y' in ds_template.coords:\n",
        "                 try:\n",
        "                     x_coords_val = ds_template['x'].values; y_coords_val = ds_template['y'].values\n",
        "                     x_res = (x_coords_val[-1] - x_coords_val[0]) / (nx -1) if nx > 1 else 1\n",
        "                     y_res_calc = (y_coords_val[-1] - y_coords_val[0]) / (ny -1) if ny > 1 else 1\n",
        "                     y_origin_affine = y_coords_val[0]\n",
        "                     y_res_affine = y_res_calc\n",
        "                     if y_coords_val[0] > y_coords_val[-1]: y_res_affine = -abs(y_res_calc) if y_res_calc > 0 else y_res_calc\n",
        "                     else: y_res_affine = abs(y_res_calc) if y_res_calc < 0 else y_res_calc\n",
        "                     transform_new = Affine(x_res, 0.0, x_coords_val[0], 0.0, y_res_affine, y_origin_affine)\n",
        "                     if abs(y_res_affine) > 1e-9 : logging.info(f\"Attempting to use calculated transform: {transform_new}\"); transform = transform_new\n",
        "                     else: logging.warning(\"Calculated y_res is zero or too small, cannot form a valid fallback transform.\")\n",
        "                 except Exception as e_trans_calc: logging.warning(f\"Could not calculate fallback transform: {e_trans_calc}\")\n",
        "\n",
        "        time_coords = ds_template['time']\n",
        "        landslide_grid_monthly = np.zeros((len(time_coords), ny, nx), dtype=np.int32)\n",
        "        logging.info(\"Rasterizing landslide points to monthly grid...\")\n",
        "        landslide_gdf['YearMonth'] = landslide_gdf[date_column].dt.to_period('M')\n",
        "        for t_idx, timestamp in enumerate(tqdm(time_coords.values, desc=\"Rasterizing Months\")):\n",
        "            current_month = pd.Timestamp(timestamp).to_period('M')\n",
        "            monthly_points = landslide_gdf[landslide_gdf['YearMonth'] == current_month]\n",
        "            if not monthly_points.empty:\n",
        "                shapes = [(geom, 1) for geom in monthly_points.geometry]\n",
        "                try:\n",
        "                    monthly_raster = features.rasterize(\n",
        "                        shapes=shapes, out_shape=grid_shape, transform=transform,\n",
        "                        fill=0, merge_alg=rasterio.enums.MergeAlg.add, dtype=np.int32)\n",
        "                    landslide_grid_monthly[t_idx, :, :] = monthly_raster\n",
        "                except Exception as raster_err: logging.error(f\"❌ Error during rasterization for month {current_month}: {raster_err}\")\n",
        "        landslide_da = xr.DataArray(\n",
        "            landslide_grid_monthly, coords={'time': time_coords, 'y': ds_template['y'], 'x': ds_template['x']},\n",
        "            dims=['time', 'y', 'x'], name=feature_name,\n",
        "            attrs={'long_name': 'Monthly landslide event count', 'units': 'count', 'source_shapefile': os.path.basename(shp_path)})\n",
        "        logging.info(f\"--- Landslide Data Processing Complete ---\")\n",
        "        return landslide_da\n",
        "    except ImportError as imp_err: logging.error(f\"❌ Missing library for landslide processing: {imp_err}. Please install geopandas and rasterio.\"); return None\n",
        "    except FileNotFoundError: logging.error(f\"❌ Landslide Shapefile not found at: {shp_path}\"); return None\n",
        "    except Exception as e: logging.error(f\"❌ An unexpected error occurred during landslide data processing: {e}\", exc_info=True); return None\n",
        "\n",
        "# --- FUNGSI FITUR SPASIAL ---\n",
        "def calculate_spatial_avg_lag1(\n",
        "    data_array: xr.DataArray, feature_name: str = \"SPATIAL_AVG_LAG1\"\n",
        ") -> xr.DataArray:\n",
        "    logging.info(f\"Calculating spatial feature: {feature_name}\")\n",
        "    kernel = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=np.float32)\n",
        "    values = data_array.values.astype(np.float32)\n",
        "    spatial_avg_lag1 = np.full_like(values, np.nan)\n",
        "    for t in range(1, values.shape[0]):\n",
        "        data_t_minus_1 = values[t-1, :, :]\n",
        "        nan_mask_t_minus_1 = np.isnan(data_t_minus_1)\n",
        "        neighbor_counts = convolve(~nan_mask_t_minus_1, kernel, mode='constant', cval=0.0)\n",
        "        data_t_minus_1_nan_as_zero = np.nan_to_num(data_t_minus_1, nan=0.0)\n",
        "        neighbor_sum = convolve(data_t_minus_1_nan_as_zero, kernel, mode='constant', cval=0.0)\n",
        "        valid_neighbors_mask = neighbor_counts > 0\n",
        "        avg_values = np.full_like(neighbor_sum, np.nan)\n",
        "        avg_values[valid_neighbors_mask] = neighbor_sum[valid_neighbors_mask] / neighbor_counts[valid_neighbors_mask]\n",
        "        spatial_avg_lag1[t, :, :] = avg_values\n",
        "    spatial_da = xr.DataArray(\n",
        "        spatial_avg_lag1, coords=data_array.coords, dims=data_array.dims, name=feature_name,\n",
        "        attrs={\n",
        "            'long_name': f'Average of 8 spatial neighbors at lag 1 for {data_array.name}',\n",
        "            'units': data_array.attrs.get('units', 'unknown'),\n",
        "            'calculation': 'Convolution with 3x3 queen kernel (center 0), NaN handled by averaging only valid neighbors'\n",
        "        }\n",
        "    )\n",
        "    logging.info(f\"Spatial feature calculation complete.\")\n",
        "    return spatial_da\n"
      ],
      "metadata": {
        "id": "phvYa9qIgV0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNGSI UTILS VISUALISASI & PENYIMPANAN ---\n",
        "def plot_accuracy_maps(metrics: dict, outdir: str):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    for name, data in metrics.items():\n",
        "        if data is None or np.isnan(data).all():\n",
        "            logging.warning(f\"Skipping map for metric '{name}' as data is missing or all NaN.\")\n",
        "            continue\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        if 'rmse' in name.lower(): cmap = 'hot'; vmin, vmax = np.nanmin(data), np.nanmax(data)\n",
        "        elif 'r2' in name.lower(): cmap = 'viridis'; vmin, vmax = max(0, np.nanmin(data)) if not np.isnan(np.nanmin(data)) else 0, 1.0\n",
        "        elif 'ljung_box_pvalue' in name.lower(): cmap = 'viridis_r'; vmin, vmax = 0.0, 1.0\n",
        "        else: cmap = 'viridis'; vmin, vmax = np.nanmin(data), np.nanmax(data)\n",
        "        im = plt.imshow(data, origin='upper', cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "        plt.title(f\"{name.replace('_', ' ').title()}\"); plt.colorbar(im, label=f\"{name} Value\")\n",
        "        plt.xlabel(\"X Coordinate Index\"); plt.ylabel(\"Y Coordinate Index\"); plt.tight_layout()\n",
        "        map_path = os.path.join(outdir, f\"{name.lower()}_map.png\"); plt.savefig(map_path); plt.close()\n",
        "        logging.info(f\"✅ {name} map saved to: {map_path}\")\n",
        "\n",
        "def plot_sample_timeseries(ds: xr.Dataset, forecast_stack: np.ndarray, time_index: pd.DatetimeIndex, time_test: pd.DatetimeIndex, n_lag: int, forecast_steps: int, sample_coords: List[Tuple[int, int]], outdir: str, target_var: str = 'COUNT'):\n",
        "    os.makedirs(outdir, exist_ok=True); target_data = ds[target_var].values\n",
        "    n_time_total = len(time_index); test_start_idx = n_time_total - forecast_steps\n",
        "    for i, j in sample_coords:\n",
        "        if 0 <= i < target_data.shape[1] and 0 <= j < target_data.shape[2]:\n",
        "            ts_all_pixel = target_data[:, i, j]; forecast_pixel = forecast_stack[:, i, j]\n",
        "            if not np.isnan(ts_all_pixel).all() or not np.isnan(forecast_pixel).all():\n",
        "                ts_full = pd.Series(ts_all_pixel, index=time_index); ts_test_obs = ts_full.iloc[test_start_idx:]\n",
        "                train_display_end_idx = test_start_idx; train_display_start_idx = max(0, train_display_end_idx - n_lag * 2)\n",
        "                ts_train_display = ts_full.iloc[train_display_start_idx:train_display_end_idx]\n",
        "                ts_pred = pd.Series(forecast_pixel, index=time_test); plt.figure(figsize=(12, 5))\n",
        "                if not ts_train_display.empty: ts_train_display.plot(label=f'Train ({target_var}, Last {len(ts_train_display)} steps)', color='gray', marker='.', linestyle='-')\n",
        "                if not ts_test_obs.empty: ts_test_obs.plot(label=f'Observed ({target_var}, Test Period)', color='blue', marker='o', linestyle='-')\n",
        "                if not ts_pred.empty and not ts_pred.isna().all(): ts_pred.plot(label=f'Forecast ({target_var})', color='red', linestyle='--', marker='x')\n",
        "                plt.title(f\"Evaluation: Forecast vs Observation ({target_var}) at Pixel (y={i}, x={j})\"); plt.xlabel(\"Time\"); plt.ylabel(f\"{target_var} Value\"); plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "                plot_filename = os.path.join(outdir, f\"eval_timeseries_{target_var}_y{i}_x{j}.png\"); plt.savefig(plot_filename); plt.close()\n",
        "                logging.info(f\"✅ Evaluation time series plot for {target_var} saved for (y={i}, x={j}) to: {plot_filename}\")\n",
        "        else: logging.warning(f\"Sample coordinate (y={i}, x={j}) is outside the data bounds.\")\n",
        "\n",
        "def save_animation(forecast_stack: np.ndarray, dates: pd.DatetimeIndex, outdir: str, name: str, var_name: str = \"Forecast\", prefix: str = \"\"):\n",
        "    os.makedirs(outdir, exist_ok=True); valid_frames_mask = ~np.isnan(forecast_stack).all(axis=(1, 2))\n",
        "    valid_indices = np.where(valid_frames_mask)[0]\n",
        "    if len(valid_indices) == 0: logging.warning(f\"Skipping animation for {name} ({var_name}) as all forecast frames contain only NaN.\"); return\n",
        "    first_valid_frame_idx = valid_indices[0]; initial_data = forecast_stack[first_valid_frame_idx]\n",
        "    vmin = np.nanmin(forecast_stack[valid_indices]); vmax = np.nanmax(forecast_stack[valid_indices])\n",
        "    vmin = max(0, vmin) if vmin is not np.nan else 0;\n",
        "    if vmax is not np.nan and vmin is not np.nan and vmax - vmin < 1e-6 : vmax = vmin + 1.0\n",
        "    elif vmax is np.nan and vmin is not np.nan : vmax = vmin + 1.0\n",
        "    elif vmin is np.nan: vmin = 0; vmax = 1.0\n",
        "    fig, ax = plt.subplots(figsize=(8, 6)); im = ax.imshow(initial_data, origin='upper', cmap='viridis', animated=True, vmin=vmin, vmax=vmax)\n",
        "    cbar = fig.colorbar(im, ax=ax, label=f\"{var_name} Value\"); title_prefix = f\"{prefix.capitalize()}: \" if prefix else \"\"\n",
        "    title = ax.set_title(f\"{title_prefix}{name} {var_name} – {dates[first_valid_frame_idx].strftime('%B %Y')}\")\n",
        "    ax.set_xlabel(\"X Coordinate Index\"); ax.set_ylabel(\"Y Coordinate Index\")\n",
        "    def update(frame_num): actual_frame_index = valid_indices[frame_num]; im.set_array(forecast_stack[actual_frame_index]); title.set_text(f\"{title_prefix}{name} {var_name} – {dates[actual_frame_index].strftime('%B %Y')}\"); return [im, title]\n",
        "    ani = animation.FuncAnimation(fig, update, frames=len(valid_indices), blit=True, interval=500)\n",
        "    filename_prefix = f\"{prefix}_\" if prefix else \"\"; gif_path = os.path.join(outdir, f\"{filename_prefix}{name.lower()}_{var_name.lower()}_animation.gif\")\n",
        "    try: ani.save(gif_path, writer='pillow', fps=2); logging.info(f\"✅ {prefix.capitalize()} animation for {name} ({var_name}) saved to: {gif_path}\")\n",
        "    except Exception as e: logging.error(f\"❌ Failed to save {prefix} animation for {name} ({var_name}) to {gif_path}: {e}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "def save_evaluation_metrics_geotiff(metrics: dict, ds_ref: xr.Dataset, outdir: str):\n",
        "    os.makedirs(outdir, exist_ok=True); saved_any = False\n",
        "    try:\n",
        "        if \"y\" not in ds_ref.coords or \"x\" not in ds_ref.coords: raise ValueError(\"Coords 'y' or 'x' missing in ds_ref\")\n",
        "        coords = {\"y\": ds_ref[\"y\"], \"x\": ds_ref[\"x\"]}; crs_from_ds, transform_from_ds = None, None\n",
        "        try:\n",
        "            if hasattr(ds_ref, 'rio') and ds_ref.rio.crs: crs_from_ds = ds_ref.rio.crs\n",
        "            else: logging.warning(\"CRS not found in ds_ref. GeoTIFFs will lack CRS.\")\n",
        "            if hasattr(ds_ref, 'rio') and ds_ref.rio.transform(): transform_from_ds = ds_ref.rio.transform()\n",
        "            if not transform_from_ds or transform_from_ds.is_identity:\n",
        "                logging.warning(\"Transform not found or is identity in ds_ref. GeoTIFFs may be incorrectly georeferenced if coordinates are not geographic/projected.\")\n",
        "        except Exception as rio_err: logging.warning(f\"Could not extract CRS/Transform from ds_ref: {rio_err}.\")\n",
        "        for name, data in metrics.items():\n",
        "            if data is None or np.isnan(data).all(): continue\n",
        "            try:\n",
        "                units = \"unknown\"\n",
        "                if 'rmse' in name.lower(): units = ds_ref[TARGET_VARIABLE].attrs.get('units', 'count')\n",
        "                elif 'r2' in name.lower(): units = \"1\"\n",
        "                elif 'ljung_box_pvalue' in name.lower(): units = \"p-value\"\n",
        "                da = xr.DataArray(data, dims=(\"y\", \"x\"), coords=coords, name=name, attrs={\"long_name\": name.replace('_', ' ').title(), \"units\": units})\n",
        "                if crs_from_ds: da = da.rio.write_crs(crs_from_ds, inplace=True)\n",
        "                if transform_from_ds and not transform_from_ds.is_identity: da = da.rio.write_transform(transform_from_ds, inplace=True)\n",
        "                da = da.rio.set_spatial_dims(x_dim='x', y_dim='y', inplace=True)\n",
        "                filepath = os.path.join(outdir, f\"{name.lower()}_map.tif\")\n",
        "                da.rio.to_raster(filepath, tiled=True, compress='LZW', num_threads='ALL_CPUS', nodata=np.nan); saved_any = True\n",
        "            except Exception as e_metric: logging.error(f\"❌ Failed to save metric GeoTIFF for {name}: {e_metric}\")\n",
        "        if saved_any: logging.info(f\"✅ Evaluation metric GeoTIFFs saved to directory: {outdir}\")\n",
        "        else: logging.warning(\"No valid evaluation metrics found to save as GeoTIFF.\")\n",
        "    except Exception as e_general: logging.error(f\"❌ General error in save_evaluation_metrics_geotiff: {e_general}\", exc_info=True)\n",
        "\n",
        "def save_forecast_geotiff(forecast_stack: np.ndarray, ds_ref: xr.Dataset, time_labels: pd.DatetimeIndex, outdir: str, var_name: str = \"Forecast\", prefix: str = \"\"):\n",
        "    os.makedirs(outdir, exist_ok=True); saved_any = False\n",
        "    try:\n",
        "        if \"y\" not in ds_ref.coords or \"x\" not in ds_ref.coords: raise ValueError(\"Coords 'y' or 'x' missing in ds_ref\")\n",
        "        coords = {\"y\": ds_ref[\"y\"], \"x\": ds_ref[\"x\"]}; crs_from_ds, transform_from_ds = None, None\n",
        "        try:\n",
        "            if hasattr(ds_ref, 'rio') and ds_ref.rio.crs: crs_from_ds = ds_ref.rio.crs\n",
        "            else: logging.warning(f\"CRS not found in ds_ref for {prefix} {var_name}. GeoTIFFs will lack CRS.\")\n",
        "            if hasattr(ds_ref, 'rio') and ds_ref.rio.transform(): transform_from_ds = ds_ref.rio.transform()\n",
        "            if not transform_from_ds or transform_from_ds.is_identity: logging.warning(f\"Transform not found or is identity in ds_ref for {prefix} {var_name}.\")\n",
        "        except Exception as rio_err: logging.warning(f\"Could not extract CRS/Transform for {prefix} {var_name} from ds_ref: {rio_err}.\")\n",
        "        filename_prefix = f\"{prefix}_\" if prefix else \"\"; target_units = ds_ref[TARGET_VARIABLE].attrs.get('units', 'unknown')\n",
        "        for t in range(forecast_stack.shape[0]):\n",
        "            forecast_slice = forecast_stack[t]\n",
        "            if np.isnan(forecast_slice).all(): continue\n",
        "            try:\n",
        "                timestamp_str = time_labels[t].strftime(\"%Y%m%d\")\n",
        "                da = xr.DataArray(forecast_slice, dims=(\"y\", \"x\"), coords=coords, name=f\"{var_name}_{timestamp_str}\", attrs={\"long_name\": f\"{prefix.capitalize()} {var_name} for {timestamp_str}\", \"units\": target_units})\n",
        "                if crs_from_ds: da = da.rio.write_crs(crs_from_ds, inplace=True)\n",
        "                if transform_from_ds and not transform_from_ds.is_identity: da = da.rio.write_transform(transform_from_ds, inplace=True)\n",
        "                da = da.rio.set_spatial_dims(x_dim='x', y_dim='y', inplace=True)\n",
        "                filepath = os.path.join(outdir, f\"{filename_prefix}{var_name.lower()}_{timestamp_str}.tif\")\n",
        "                da.rio.to_raster(filepath, tiled=True, compress='LZW', num_threads='ALL_CPUS', nodata=np.nan); saved_any = True\n",
        "            except Exception as e_slice: logging.error(f\"❌ Failed to save {prefix} forecast GeoTIFF for {time_labels[t].strftime('%Y%m%d')} ({var_name}): {e_slice}\")\n",
        "        if saved_any: logging.info(f\"✅ {prefix.capitalize()} forecast GeoTIFFs for {var_name} saved to directory: {outdir}\")\n",
        "        else: logging.warning(f\"No valid forecast slices found to save as GeoTIFF for {prefix} {var_name}.\")\n",
        "    except Exception as e_general: logging.error(f\"❌ General error in save_forecast_geotiff ({prefix} {var_name}): {e_general}\", exc_info=True)\n",
        "\n",
        "def save_aggregated_geotiff(aggregated_data: np.ndarray, ds_ref: xr.Dataset, time_period: pd.DatetimeIndex, outdir: str, var_name: str = \"Aggregated\", prefix: str = \"agg\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    if np.isnan(aggregated_data).all(): logging.warning(f\"Skipping saving aggregated GeoTIFF for {prefix} {var_name} as data contains only NaN.\"); return\n",
        "    try:\n",
        "        if \"y\" not in ds_ref.coords or \"x\" not in ds_ref.coords: raise ValueError(\"Coords 'y' or 'x' missing in ds_ref\")\n",
        "        coords = {\"y\": ds_ref[\"y\"], \"x\": ds_ref[\"x\"]}; crs_from_ds, transform_from_ds = None, None\n",
        "        try:\n",
        "            if hasattr(ds_ref, 'rio') and ds_ref.rio.crs: crs_from_ds = ds_ref.rio.crs\n",
        "            else: logging.warning(f\"CRS not found in ds_ref for {prefix} {var_name}. GeoTIFF will lack CRS.\")\n",
        "            if hasattr(ds_ref, 'rio') and ds_ref.rio.transform(): transform_from_ds = ds_ref.rio.transform()\n",
        "            if not transform_from_ds or transform_from_ds.is_identity: logging.warning(f\"Transform not found or is identity in ds_ref for {prefix} {var_name}.\")\n",
        "        except Exception as rio_err: logging.warning(f\"Could not extract CRS/Transform for {prefix} {var_name} from ds_ref: {rio_err}.\")\n",
        "        filename_prefix = f\"{prefix}_\" if prefix else \"\"; target_units = ds_ref[TARGET_VARIABLE].attrs.get('units', 'unknown')\n",
        "        start_date_str = time_period[0].strftime(\"%Y%m%d\"); end_date_str = time_period[-1].strftime(\"%Y%m%d\")\n",
        "        period_str = f\"{start_date_str}_to_{end_date_str}\"\n",
        "        da = xr.DataArray(aggregated_data, dims=(\"y\", \"x\"), coords=coords, name=f\"{var_name}_{period_str}\", attrs={\"long_name\": f\"{prefix.capitalize()} {var_name} from {start_date_str} to {end_date_str}\", \"units\": target_units})\n",
        "        if crs_from_ds: da = da.rio.write_crs(crs_from_ds, inplace=True)\n",
        "        if transform_from_ds and not transform_from_ds.is_identity: da = da.rio.write_transform(transform_from_ds, inplace=True)\n",
        "        da = da.rio.set_spatial_dims(x_dim='x', y_dim='y', inplace=True)\n",
        "        filepath = os.path.join(outdir, f\"{filename_prefix}{var_name.lower()}_{period_str}.tif\")\n",
        "        da.rio.to_raster(filepath, tiled=True, compress='LZW', num_threads='ALL_CPUS', nodata=np.nan)\n",
        "        logging.info(f\"✅ Aggregated GeoTIFF for {var_name} ({period_str}) saved to: {filepath}\")\n",
        "    except Exception as e_general: logging.error(f\"❌ Failed to save aggregated GeoTIFF ({prefix} {var_name}): {e_general}\", exc_info=True)\n",
        "\n",
        "def save_forecast_netcdf(\n",
        "    forecast_stack: np.ndarray, metrics: Optional[dict], ds_ref: xr.Dataset,\n",
        "    time_labels: pd.DatetimeIndex, out_path: str, target_var: str = 'COUNT', is_future: bool = False\n",
        "):\n",
        "    if np.isnan(forecast_stack).all(): logging.warning(f\"Skipping saving NetCDF to {out_path} as forecast stack contains only NaN.\"); return\n",
        "    metrics_valid = False\n",
        "    if not is_future and metrics:\n",
        "        for data in metrics.values():\n",
        "            if data is not None and not np.isnan(data).all(): metrics_valid = True; break\n",
        "    if not is_future and not metrics_valid and np.isnan(forecast_stack).all():\n",
        "         logging.warning(f\"Skipping saving evaluation NetCDF to {out_path} as both metrics and forecast are all NaN.\"); return\n",
        "    if not is_future and not metrics_valid and not np.isnan(forecast_stack).all():\n",
        "        logging.warning(f\"Metrics for evaluation NetCDF {out_path} are missing or all NaN. Saving forecast data only.\")\n",
        "    try:\n",
        "        y_coord = ds_ref[\"y\"] if \"y\" in ds_ref else xr.DataArray(np.arange(forecast_stack.shape[1]), dims=\"y\", name=\"y\")\n",
        "        x_coord = ds_ref[\"x\"] if \"x\" in ds_ref else xr.DataArray(np.arange(forecast_stack.shape[2]), dims=\"x\", name=\"x\")\n",
        "        time_coord = xr.DataArray(time_labels, dims=\"time\", name=\"time\", attrs={\"standard_name\": \"time\"})\n",
        "        coords_forecast = {\"time\": time_coord, \"y\": y_coord, \"x\": x_coord}\n",
        "        coords_metrics = {\"y\": y_coord, \"x\": x_coord}\n",
        "        target_units = ds_ref[target_var].attrs.get('units', 'unknown'); fill_value = np.nan\n",
        "        model_name_from_path = os.path.basename(os.path.dirname(out_path)); model_name = model_name_from_path.split('_')[-1].upper() if '_' in model_name_from_path else model_name_from_path.upper()\n",
        "        title_prefix = \"Future\" if is_future else \"Evaluation\"\n",
        "        global_attrs = {\n",
        "            \"Conventions\": \"CF-1.8\", \"title\": f\"{title_prefix} Spatial-Temporal Forecast Output ({model_name} Model)\",\n",
        "            \"institution\": \"Generated by Python Script\", \"source_data\": os.path.basename(NC_FILE),\n",
        "            \"history\": f\"Created on {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')} using {model_name} model.\",\n",
        "            \"forecast_steps\": FORECAST_STEPS, \"n_lag_used\": N_LAG, \"target_variable\": target_var,\n",
        "            \"time_coverage_start\": time_labels[0].strftime('%Y-%m-%d %H:%M:%S'), \"time_coverage_end\": time_labels[-1].strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        }\n",
        "        if hasattr(ds_ref.rio, 'crs') and ds_ref.rio.crs: global_attrs[\"geospatial_crs\"] = str(ds_ref.rio.crs)\n",
        "        if SPATIAL_FEATURE_NAME in ds_ref.variables: global_attrs[\"spatial_feature_used\"] = SPATIAL_FEATURE_NAME\n",
        "        if LANDSLIDE_FEATURE_NAME in ds_ref.variables:\n",
        "            global_attrs[\"landslide_data_source\"] = os.path.basename(LANDSLIDE_SHP_FILE)\n",
        "            global_attrs[\"landslide_feature_name\"] = LANDSLIDE_FEATURE_NAME\n",
        "        forecast_attrs = {\"long_name\": f\"{title_prefix} Forecasted {target_var}\", \"units\": target_units, \"coordinates\": \"time y x\", \"grid_mapping\": \"spatial_ref\"}\n",
        "        forecast_da = xr.DataArray(forecast_stack, coords=coords_forecast, dims=(\"time\", \"y\", \"x\"), name=\"forecast\", attrs=forecast_attrs)\n",
        "        ds_out_dict = {\"forecast\": forecast_da}\n",
        "        if not is_future and metrics_valid and metrics:\n",
        "            metric_names = []\n",
        "            for name, data in metrics.items():\n",
        "                 if data is not None and not np.isnan(data).all():\n",
        "                     metric_attrs = {\"long_name\": name.replace('_', ' ').title(), \"coordinates\": \"y x\", \"grid_mapping\": \"spatial_ref\"}\n",
        "                     if 'rmse' in name.lower(): metric_attrs[\"units\"] = target_units\n",
        "                     elif 'r2' in name.lower(): metric_attrs.update({\"units\": \"1\", \"valid_range\": [-np.inf, 1.0]})\n",
        "                     elif 'ljung_box_pvalue' in name.lower(): metric_attrs.update({\"units\": \"p-value\", \"valid_range\": [0.0, 1.0]})\n",
        "                     metric_da = xr.DataArray(data, coords=coords_metrics, dims=(\"y\", \"x\"), name=name, attrs=metric_attrs)\n",
        "                     ds_out_dict[name] = metric_da; metric_names.append(name)\n",
        "            if metric_names: global_attrs[\"evaluation_metrics\"] = \", \".join(metric_names)\n",
        "        ds_out = xr.Dataset(ds_out_dict, attrs=global_attrs)\n",
        "        grid_mapping_name = \"spatial_ref\"\n",
        "        try:\n",
        "            if hasattr(ds_ref, 'rio') and ds_ref.rio.crs:\n",
        "                ds_out.rio.write_crs(ds_ref.rio.crs, inplace=True)\n",
        "                ds_out.rio.write_coordinate_system(inplace=True)\n",
        "                logging.info(\"CRS and grid mapping variable added to output NetCDF.\")\n",
        "            else:\n",
        "                 logging.warning(\"CRS not found in ds_ref. Output NetCDF will lack full georeferencing information.\")\n",
        "                 for var_name_iter in list(ds_out.data_vars):\n",
        "                     if 'grid_mapping' in ds_out[var_name_iter].attrs: del ds_out[var_name_iter].attrs['grid_mapping']\n",
        "                 if \"spatial_ref\" in ds_out: del ds_out[\"spatial_ref\"]\n",
        "        except Exception as crs_err:\n",
        "            logging.warning(f\"Could not write CRS info to NetCDF: {crs_err}\")\n",
        "            for var_name_iter in list(ds_out.data_vars):\n",
        "                 if 'grid_mapping' in ds_out[var_name_iter].attrs: del ds_out[var_name_iter].attrs['grid_mapping']\n",
        "            if \"spatial_ref\" in ds_out: del ds_out[\"spatial_ref\"]\n",
        "        encoding = {}\n",
        "        for var_name_enc in ds_out.data_vars: encoding[var_name_enc] = {'_FillValue': fill_value, 'zlib': True, 'complevel': 4}\n",
        "        for coord_name_enc in ds_out.coords:\n",
        "             if ds_out[coord_name_enc].dtype.kind in 'ifc': encoding[coord_name_enc] = {'_FillValue': None}\n",
        "             elif ds_out[coord_name_enc].dtype.kind == 'M': encoding[coord_name_enc] = {'_FillValue': None, 'dtype': 'double', 'units': \"days since 1970-01-01\"}\n",
        "             elif ds_out[coord_name_enc].dtype.kind == 'O': pass\n",
        "        if \"spatial_ref\" in ds_out and isinstance(ds_out[\"spatial_ref\"], xr.DataArray):\n",
        "            encoding[\"spatial_ref\"] = {'_FillValue': None}\n",
        "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "        ds_out.to_netcdf(out_path, encoding=encoding, format='NETCDF4')\n",
        "        logging.info(f\"✅ Saved {'future' if is_future else 'evaluation'} forecast data to NetCDF: {out_path}\")\n",
        "    except Exception as e: logging.error(f\"❌ Failed to save {'future' if is_future else 'evaluation'} NetCDF file {out_path}: {e}\", exc_info=True)\n",
        "    finally:\n",
        "        if 'ds_out' in locals(): ds_out.close()"
      ],
      "metadata": {
        "id": "Ea9aOw3ahBq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNGSI MODEL FORECASTERS (EVALUASI) ---\n",
        "def get_stationarity(timeseries: np.ndarray, p_threshold: float = 0.05, max_diff: int = 2) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Determines the order of differencing 'd' required to make a time series stationary\n",
        "    based on the Augmented Dickey-Fuller (ADF) test.\n",
        "    \"\"\"\n",
        "    d = 0; ts_processed = timeseries.copy()\n",
        "    if len(ts_processed) < 10: return ts_processed, d\n",
        "    valid_indices = np.where(~np.isnan(ts_processed))[0]\n",
        "    if len(valid_indices) < 10 : return ts_processed, d\n",
        "    first_valid, last_valid = valid_indices[0], valid_indices[-1]\n",
        "    ts_processed = ts_processed[first_valid:last_valid+1]\n",
        "    if len(ts_processed) < 10: return ts_processed, d\n",
        "    s = pd.Series(ts_processed); s_interpolated = s.interpolate(method='linear', limit_direction='both').fillna(0); ts_processed = s_interpolated.values\n",
        "    if np.nanstd(ts_processed) < 1e-9: return ts_processed, d\n",
        "    try: p_value = adfuller(ts_processed, autolag='AIC')[1]\n",
        "    except Exception: p_value = 1.0\n",
        "    while p_value > p_threshold and d < max_diff:\n",
        "        d += 1; ts_processed = np.diff(ts_processed)\n",
        "        if len(ts_processed) < 10: break\n",
        "        if np.nanstd(ts_processed) < 1e-9: break\n",
        "        try: p_value = adfuller(ts_processed, autolag='AIC')[1]\n",
        "        except Exception: p_value = 1.0; break\n",
        "    return ts_processed, d\n",
        "\n",
        "def forecast_arima(ts_train: np.ndarray, ts_test_obs: np.ndarray, forecast_steps: int, ljung_box_lags_config: int) -> Tuple[np.ndarray, float, float, float, Optional[Tuple[int,int,int]]]:\n",
        "    \"\"\"\n",
        "    Performs ARIMA forecasting using pmdarima.auto_arima, calculates RMSE, R2 on test data,\n",
        "    and Ljung-Box p-value on in-sample residuals.\n",
        "    \"\"\"\n",
        "    y_pred = np.full(forecast_steps, np.nan); rmse, r2 = np.nan, np.nan\n",
        "    ljung_box_p_value_insample = np.nan; order_used = None\n",
        "    if ts_train.ndim > 1: ts_train = ts_train.flatten()\n",
        "    if ts_test_obs.ndim > 1: ts_test_obs = ts_test_obs.flatten()\n",
        "    valid_train_indices = ~np.isnan(ts_train); ts_train_valid = ts_train[valid_train_indices]\n",
        "    if len(ts_train_valid) < 20: # auto_arima works better with more data\n",
        "        return y_pred, rmse, r2, ljung_box_p_value_insample, order_used\n",
        "\n",
        "    # Impute series for auto_arima\n",
        "    series_train = pd.Series(ts_train_valid).interpolate(method='linear', limit_direction='both').fillna(method='bfill').fillna(method='ffill')\n",
        "    if series_train.isnull().any() or len(series_train) < 20:\n",
        "        return y_pred, rmse, r2, ljung_box_p_value_insample, order_used\n",
        "\n",
        "    fitted_model = None\n",
        "    try:\n",
        "        auto_model = pm.auto_arima(series_train,\n",
        "                                   start_p=1, start_q=1,\n",
        "                                   max_p=3, max_q=3,\n",
        "                                   d=None, # Let auto_arima determine d\n",
        "                                   max_d=MAX_DIFFERENCING,\n",
        "                                   seasonal=False, # Set to True and specify m if seasonality is expected\n",
        "                                   # m=12, D=None, max_D=1, start_P=0, start_Q=0, max_P=2, max_Q=2, seasonal=True,\n",
        "                                   stepwise=True,\n",
        "                                   suppress_warnings=True,\n",
        "                                   error_action='ignore', # Skip models that fail to fit\n",
        "                                   trace=False # Set to True to see search process\n",
        "                                  )\n",
        "        order_used = auto_model.order # (p,d,q)\n",
        "        fitted_model = auto_model\n",
        "\n",
        "        y_pred = fitted_model.predict(n_periods=forecast_steps)\n",
        "        y_pred[~np.isfinite(y_pred)] = np.nan\n",
        "\n",
        "        insample_residuals = fitted_model.resid()\n",
        "        valid_insample_residuals = insample_residuals[~np.isnan(insample_residuals)]\n",
        "\n",
        "        p, _, q = order_used # d is part of the model, not subtracted for model_df in pmdarima context\n",
        "        num_params = p + q # Number of AR and MA parameters\n",
        "\n",
        "        actual_lags_for_lb = min(ljung_box_lags_config, len(valid_insample_residuals) - 1)\n",
        "        if actual_lags_for_lb > num_params and np.std(valid_insample_residuals) > 1e-6:\n",
        "            try:\n",
        "                # model_df in acorr_ljungbox refers to the number of estimated parameters (p+q)\n",
        "                lb_test_df = acorr_ljungbox(valid_insample_residuals,\n",
        "                                            lags=[actual_lags_for_lb],\n",
        "                                            model_df=num_params,\n",
        "                                            return_df=True)\n",
        "                if not lb_test_df.empty:\n",
        "                    ljung_box_p_value_insample = lb_test_df['lb_pvalue'].iloc[0]\n",
        "            except ValueError: pass\n",
        "            except Exception: pass\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    y_pred = np.maximum(0, y_pred)\n",
        "    valid_pred_indices = ~np.isnan(y_pred); valid_test_indices = ~np.isnan(ts_test_obs)\n",
        "    common_valid_indices = valid_pred_indices & valid_test_indices\n",
        "    if np.any(common_valid_indices):\n",
        "        y_true_valid = ts_test_obs[common_valid_indices]; y_pred_valid = y_pred[common_valid_indices]\n",
        "        if len(y_true_valid) > 0:\n",
        "            try:\n",
        "                rmse = np.sqrt(mean_squared_error(y_true_valid, y_pred_valid))\n",
        "                if len(y_true_valid) > 1 and np.nanstd(y_true_valid) > 1e-6: r2 = r2_score(y_true_valid, y_pred_valid)\n",
        "            except Exception: pass\n",
        "    return y_pred, rmse, r2, ljung_box_p_value_insample, order_used\n",
        "\n",
        "def forecast_lstm(\n",
        "    ts_input: np.ndarray, n_lag: int, forecast_steps: int, n_features: int,\n",
        "    ljung_box_lags_config: int, hyperparams: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, float, float, float, Optional[MinMaxScaler]]:\n",
        "    scaler = None; y_pred_unscaled = np.full(forecast_steps, np.nan); rmse, r2 = np.nan, np.nan\n",
        "    ljung_box_p_value_insample = np.nan\n",
        "    lstm_units = hyperparams.get(\"lstm_units\", 32)\n",
        "    lstm_activation = hyperparams.get(\"lstm_activation\", 'relu')\n",
        "    learning_rate = hyperparams.get(\"learning_rate\", 0.01)\n",
        "    epochs = hyperparams.get(\"epochs\", 50)\n",
        "    batch_size_val = hyperparams.get(\"batch_size_val\", None)\n",
        "\n",
        "    if ts_input.ndim != 2 or ts_input.shape[1] != n_features or ts_input.shape[0] < n_lag + forecast_steps:\n",
        "        return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "    train_data_full = ts_input[:-forecast_steps, :]\n",
        "    target_actual_unscaled_test = ts_input[-forecast_steps:, 0]\n",
        "    train_data_nonan_for_scaler = train_data_full[~np.isnan(train_data_full).any(axis=1)]\n",
        "    if train_data_nonan_for_scaler.shape[0] < 2:\n",
        "        return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "    try:\n",
        "        scaler = MinMaxScaler(); scaler.fit(train_data_nonan_for_scaler)\n",
        "        ts_scaled = scaler.transform(ts_input)\n",
        "        ts_scaled_imputed_df = pd.DataFrame(ts_scaled); ts_scaled_imputed = ts_scaled_imputed_df.ffill().bfill().fillna(0.0).values\n",
        "        X_pred_seq_scaled = ts_scaled_imputed[-(n_lag + forecast_steps) : -forecast_steps, :]\n",
        "        if X_pred_seq_scaled.shape[0] != n_lag:\n",
        "            return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "        X_train_list, y_train_list_scaled = [], []\n",
        "        train_data_for_sequences_scaled = ts_scaled_imputed[:-forecast_steps, :]\n",
        "        if len(train_data_for_sequences_scaled) < n_lag + 1 :\n",
        "            return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "        for k in range(len(train_data_for_sequences_scaled) - n_lag):\n",
        "             input_window = train_data_for_sequences_scaled[k : k + n_lag, :]\n",
        "             target_val_scaled = train_data_for_sequences_scaled[k + n_lag, 0]\n",
        "             X_train_list.append(input_window); y_train_list_scaled.append(target_val_scaled)\n",
        "        if not X_train_list:\n",
        "            return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "        X_train_scaled = np.array(X_train_list); y_train_scaled = np.array(y_train_list_scaled)\n",
        "        tf.keras.backend.clear_session()\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(lstm_units, activation=lstm_activation, input_shape=(n_lag, n_features)),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
        "        current_batch_size = batch_size_val if batch_size_val is not None else min(16, len(X_train_scaled))\n",
        "        if current_batch_size == 0 and len(X_train_scaled) > 0: current_batch_size = 1\n",
        "        if len(X_train_scaled) == 0: return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "        model.fit(X_train_scaled, y_train_scaled, epochs=epochs, batch_size=current_batch_size, verbose=0)\n",
        "        if len(X_train_scaled) > 0:\n",
        "            insample_preds_scaled = model.predict(X_train_scaled, verbose=0).flatten()\n",
        "            insample_residuals_scaled = y_train_scaled - insample_preds_scaled\n",
        "            valid_insample_residuals_scaled = insample_residuals_scaled[~np.isnan(insample_residuals_scaled)]\n",
        "            actual_lags_lb_ml = min(ljung_box_lags_config, len(valid_insample_residuals_scaled) - 1)\n",
        "            if actual_lags_lb_ml > 0 and np.std(valid_insample_residuals_scaled) > 1e-6:\n",
        "                try:\n",
        "                    lb_df = acorr_ljungbox(valid_insample_residuals_scaled, lags=[actual_lags_lb_ml], return_df=True, model_df=0)\n",
        "                    if not lb_df.empty: ljung_box_p_value_insample = lb_df['lb_pvalue'].iloc[0]\n",
        "                except ValueError: pass\n",
        "                except Exception: pass\n",
        "        y_pred_scaled_list = []\n",
        "        current_input_scaled = X_pred_seq_scaled.reshape(1, n_lag, n_features)\n",
        "        for _ in range(forecast_steps):\n",
        "            current_input_imputed = np.nan_to_num(current_input_scaled, nan=0.0)\n",
        "            next_pred_target_scaled = model.predict(current_input_imputed, verbose=0)[0, 0]\n",
        "            y_pred_scaled_list.append(next_pred_target_scaled)\n",
        "            next_features_scaled = np.zeros(n_features); next_features_scaled[0] = next_pred_target_scaled\n",
        "            for feat_idx in range(1, n_features): next_features_scaled[feat_idx] = current_input_imputed[0, -1, feat_idx]\n",
        "            new_input_sequence = np.roll(current_input_imputed[0], -1, axis=0); new_input_sequence[-1, :] = next_features_scaled\n",
        "            current_input_scaled = new_input_sequence.reshape(1, n_lag, n_features)\n",
        "        y_pred_scaled = np.array(y_pred_scaled_list)\n",
        "        dummy_pred_scaled_for_inverse = np.zeros((forecast_steps, n_features)); dummy_pred_scaled_for_inverse[:, 0] = y_pred_scaled\n",
        "        if n_features > 1:\n",
        "            for feat_idx in range(1, n_features):\n",
        "                 last_known_vals = X_pred_seq_scaled[:, feat_idx]\n",
        "                 dummy_pred_scaled_for_inverse[:, feat_idx] = np.nanmean(last_known_vals) if not np.isnan(last_known_vals).all() else 0.0\n",
        "        y_pred_unscaled_full = scaler.inverse_transform(dummy_pred_scaled_for_inverse); y_pred_unscaled = y_pred_unscaled_full[:, 0]\n",
        "        y_pred_unscaled[~np.isfinite(y_pred_unscaled)] = np.nan\n",
        "    except Exception: y_pred_unscaled.fill(np.nan)\n",
        "    y_pred_unscaled = np.maximum(0, y_pred_unscaled)\n",
        "    valid_pred_idx = ~np.isnan(y_pred_unscaled); valid_target_idx = ~np.isnan(target_actual_unscaled_test)\n",
        "    common_valid_idx = valid_pred_idx & valid_target_idx\n",
        "    if np.any(common_valid_idx):\n",
        "        y_true_v = target_actual_unscaled_test[common_valid_idx]; y_pred_v = y_pred_unscaled[common_valid_idx]\n",
        "        if len(y_true_v) > 0:\n",
        "            try:\n",
        "                rmse = np.sqrt(mean_squared_error(y_true_v, y_pred_v))\n",
        "                if len(y_true_v) > 1 and np.nanstd(y_true_v) > 1e-6: r2 = r2_score(y_true_v, y_pred_v)\n",
        "            except Exception: pass\n",
        "    return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "\n",
        "def forecast_ann(\n",
        "    ts_input: np.ndarray, n_lag: int, forecast_steps: int, n_features: int,\n",
        "    ljung_box_lags_config: int, hyperparams: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, float, float, float, Optional[MinMaxScaler]]:\n",
        "    scaler = None; y_pred_unscaled = np.full(forecast_steps, np.nan); rmse, r2 = np.nan, np.nan\n",
        "    ljung_box_p_value_insample = np.nan\n",
        "    ann_layers_units = hyperparams.get(\"ann_layers_units\", [64, 32])\n",
        "    ann_activation = hyperparams.get(\"ann_activation\", 'relu')\n",
        "    learning_rate = hyperparams.get(\"learning_rate\", 0.01)\n",
        "    epochs = hyperparams.get(\"epochs\", 50)\n",
        "    batch_size_val = hyperparams.get(\"batch_size_val\", None)\n",
        "\n",
        "    if ts_input.ndim != 2 or ts_input.shape[1] != n_features or ts_input.shape[0] < n_lag + forecast_steps:\n",
        "        return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "    train_data_full = ts_input[:-forecast_steps, :]; target_actual_unscaled_test = ts_input[-forecast_steps:, 0]\n",
        "    train_data_nonan_for_scaler = train_data_full[~np.isnan(train_data_full).any(axis=1)]\n",
        "    if train_data_nonan_for_scaler.shape[0] < 2: return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "    try:\n",
        "        scaler = MinMaxScaler(); scaler.fit(train_data_nonan_for_scaler)\n",
        "        ts_scaled = scaler.transform(ts_input); ts_scaled_imputed_df = pd.DataFrame(ts_scaled)\n",
        "        ts_scaled_imputed = ts_scaled_imputed_df.ffill().bfill().fillna(0.0).values\n",
        "        X_pred_seq_scaled = ts_scaled_imputed[-(n_lag + forecast_steps) : -forecast_steps, :]\n",
        "        if X_pred_seq_scaled.shape[0] != n_lag: return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "        X_train_flat_list, y_train_list_scaled = [], []\n",
        "        train_data_for_sequences_scaled = ts_scaled_imputed[:-forecast_steps, :]\n",
        "        if len(train_data_for_sequences_scaled) < n_lag + 1 : return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "        for k in range(len(train_data_for_sequences_scaled) - n_lag):\n",
        "             input_window = train_data_for_sequences_scaled[k : k + n_lag, :]\n",
        "             target_val_scaled = train_data_for_sequences_scaled[k + n_lag, 0]\n",
        "             X_train_flat_list.append(input_window.flatten()); y_train_list_scaled.append(target_val_scaled)\n",
        "        if not X_train_flat_list: return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "        X_train_flat_scaled = np.array(X_train_flat_list); y_train_scaled = np.array(y_train_list_scaled)\n",
        "        input_shape_flat = n_lag * n_features; tf.keras.backend.clear_session()\n",
        "        model_layers = [tf.keras.layers.Dense(ann_layers_units[0], activation=ann_activation, input_shape=(input_shape_flat,))]\n",
        "        for units in ann_layers_units[1:]: model_layers.append(tf.keras.layers.Dense(units, activation=ann_activation))\n",
        "        model_layers.append(tf.keras.layers.Dense(1)); model = tf.keras.Sequential(model_layers)\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
        "        current_batch_size = batch_size_val if batch_size_val is not None else min(16, len(X_train_flat_scaled))\n",
        "        if current_batch_size == 0 and len(X_train_flat_scaled) > 0: current_batch_size = 1\n",
        "        if len(X_train_flat_scaled) == 0: return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "        model.fit(X_train_flat_scaled, y_train_scaled, epochs=epochs, batch_size=current_batch_size, verbose=0)\n",
        "        if len(X_train_flat_scaled) > 0:\n",
        "            insample_preds_scaled = model.predict(X_train_flat_scaled, verbose=0).flatten()\n",
        "            insample_residuals_scaled = y_train_scaled - insample_preds_scaled\n",
        "            valid_insample_residuals_scaled = insample_residuals_scaled[~np.isnan(insample_residuals_scaled)]\n",
        "            actual_lags_lb_ml = min(ljung_box_lags_config, len(valid_insample_residuals_scaled) -1)\n",
        "            if actual_lags_lb_ml > 0 and np.std(valid_insample_residuals_scaled) > 1e-6:\n",
        "                try:\n",
        "                    lb_df = acorr_ljungbox(valid_insample_residuals_scaled, lags=[actual_lags_lb_ml], return_df=True, model_df=0)\n",
        "                    if not lb_df.empty: ljung_box_p_value_insample = lb_df['lb_pvalue'].iloc[0]\n",
        "                except ValueError: pass\n",
        "                except Exception: pass\n",
        "        y_pred_scaled_list = []; current_input_sequence_scaled_2d = X_pred_seq_scaled\n",
        "        for _ in range(forecast_steps):\n",
        "            current_input_imputed_2d = np.nan_to_num(current_input_sequence_scaled_2d, nan=0.0)\n",
        "            current_input_flat_for_pred = current_input_imputed_2d.flatten().reshape(1, -1)\n",
        "            next_pred_target_scaled = model.predict(current_input_flat_for_pred, verbose=0)[0, 0]; y_pred_scaled_list.append(next_pred_target_scaled)\n",
        "            next_features_scaled = np.zeros(n_features); next_features_scaled[0] = next_pred_target_scaled\n",
        "            for feat_idx in range(1, n_features): next_features_scaled[feat_idx] = current_input_imputed_2d[-1, feat_idx]\n",
        "            new_input_sequence_2d = np.roll(current_input_imputed_2d, -1, axis=0); new_input_sequence_2d[-1, :] = next_features_scaled\n",
        "            current_input_sequence_scaled_2d = new_input_sequence_2d\n",
        "        y_pred_scaled = np.array(y_pred_scaled_list); dummy_pred_scaled_for_inverse = np.zeros((forecast_steps, n_features)); dummy_pred_scaled_for_inverse[:, 0] = y_pred_scaled\n",
        "        if n_features > 1:\n",
        "            for feat_idx in range(1, n_features):\n",
        "                 last_known_vals = X_pred_seq_scaled[:, feat_idx]\n",
        "                 dummy_pred_scaled_for_inverse[:, feat_idx] = np.nanmean(last_known_vals) if not np.isnan(last_known_vals).all() else 0.0\n",
        "        y_pred_unscaled_full = scaler.inverse_transform(dummy_pred_scaled_for_inverse); y_pred_unscaled = y_pred_unscaled_full[:, 0]\n",
        "        y_pred_unscaled[~np.isfinite(y_pred_unscaled)] = np.nan\n",
        "    except Exception: y_pred_unscaled.fill(np.nan)\n",
        "    y_pred_unscaled = np.maximum(0, y_pred_unscaled)\n",
        "    valid_pred_idx = ~np.isnan(y_pred_unscaled); valid_target_idx = ~np.isnan(target_actual_unscaled_test)\n",
        "    common_valid_idx = valid_pred_idx & valid_target_idx\n",
        "    if np.any(common_valid_idx):\n",
        "        y_true_v = target_actual_unscaled_test[common_valid_idx]; y_pred_v = y_pred_unscaled[common_valid_idx]\n",
        "        if len(y_true_v) > 0:\n",
        "             try:\n",
        "                 rmse = np.sqrt(mean_squared_error(y_true_v, y_pred_v))\n",
        "                 if len(y_true_v) > 1 and np.nanstd(y_true_v) > 1e-6: r2 = r2_score(y_true_v, y_pred_v)\n",
        "             except Exception: pass\n",
        "    return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "\n",
        "def forecast_rf(\n",
        "    ts_input: np.ndarray, n_lag: int, forecast_steps: int, n_features: int,\n",
        "    ljung_box_lags_config: int, hyperparams: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, float, float, float, Optional[MinMaxScaler]]:\n",
        "    scaler = None; y_pred_unscaled = np.full(forecast_steps, np.nan); rmse, r2 = np.nan, np.nan\n",
        "    ljung_box_p_value_insample = np.nan\n",
        "    n_estimators = hyperparams.get(\"n_estimators\", 100)\n",
        "    rf_random_state = hyperparams.get(\"rf_random_state\", 42)\n",
        "    max_depth = hyperparams.get(\"max_depth\", None)\n",
        "    min_samples_split = hyperparams.get(\"min_samples_split\", 2)\n",
        "    min_samples_leaf = hyperparams.get(\"min_samples_leaf\", 1)\n",
        "\n",
        "    if ts_input.ndim != 2 or ts_input.shape[1] != n_features or ts_input.shape[0] < n_lag + forecast_steps:\n",
        "        return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "    train_data_full = ts_input[:-forecast_steps, :]; target_actual_unscaled_test = ts_input[-forecast_steps:, 0]\n",
        "    train_data_target_nonan_for_scaler = train_data_full[~np.isnan(train_data_full[:,0])]\n",
        "    if train_data_target_nonan_for_scaler.shape[0] < 2 : return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "    train_data_nonan_for_scaler = train_data_full[~np.isnan(train_data_full).any(axis=1)]\n",
        "    if train_data_nonan_for_scaler.shape[0] < 2: return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "    try:\n",
        "        scaler = MinMaxScaler(); scaler.fit(train_data_nonan_for_scaler)\n",
        "        ts_scaled = scaler.transform(ts_input); ts_scaled_imputed_df = pd.DataFrame(ts_scaled)\n",
        "        ts_scaled_imputed = ts_scaled_imputed_df.ffill().bfill().fillna(0.0).values\n",
        "        X_pred_seq_scaled = ts_scaled_imputed[-(n_lag + forecast_steps) : -forecast_steps, :]\n",
        "        if X_pred_seq_scaled.shape[0] != n_lag: return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "        X_train_flat_list, y_train_list_scaled = [], []\n",
        "        train_data_for_sequences_scaled = ts_scaled_imputed[:-forecast_steps, :]\n",
        "        if len(train_data_for_sequences_scaled) < n_lag + 1: return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "        for k in range(len(train_data_for_sequences_scaled) - n_lag):\n",
        "             input_window = train_data_for_sequences_scaled[k : k + n_lag, :]\n",
        "             target_val_scaled = train_data_for_sequences_scaled[k + n_lag, 0]\n",
        "             X_train_flat_list.append(input_window.flatten()); y_train_list_scaled.append(target_val_scaled)\n",
        "        if not X_train_flat_list: return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler\n",
        "        X_train_flat_scaled = np.array(X_train_flat_list); y_train_scaled = np.array(y_train_list_scaled)\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=n_estimators, random_state=rf_random_state, max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, n_jobs=1\n",
        "        )\n",
        "        model.fit(X_train_flat_scaled, y_train_scaled)\n",
        "        if len(X_train_flat_scaled) > 0:\n",
        "            insample_preds_scaled = model.predict(X_train_flat_scaled).flatten()\n",
        "            insample_residuals_scaled = y_train_scaled - insample_preds_scaled\n",
        "            valid_insample_residuals_scaled = insample_residuals_scaled[~np.isnan(insample_residuals_scaled)]\n",
        "            actual_lags_lb_ml = min(ljung_box_lags_config, len(valid_insample_residuals_scaled) -1)\n",
        "            if actual_lags_lb_ml > 0 and np.std(valid_insample_residuals_scaled) > 1e-6:\n",
        "                try:\n",
        "                    lb_df = acorr_ljungbox(valid_insample_residuals_scaled, lags=[actual_lags_lb_ml], return_df=True, model_df=0)\n",
        "                    if not lb_df.empty: ljung_box_p_value_insample = lb_df['lb_pvalue'].iloc[0]\n",
        "                except ValueError: pass\n",
        "                except Exception: pass\n",
        "        y_pred_scaled_list = []; current_input_sequence_scaled_2d = X_pred_seq_scaled\n",
        "        for _ in range(forecast_steps):\n",
        "            current_input_imputed_2d = np.nan_to_num(current_input_sequence_scaled_2d, nan=0.0)\n",
        "            current_input_flat_for_pred = current_input_imputed_2d.flatten().reshape(1, -1)\n",
        "            next_pred_target_scaled = model.predict(current_input_flat_for_pred)[0]\n",
        "            y_pred_scaled_list.append(next_pred_target_scaled)\n",
        "            next_features_scaled = np.zeros(n_features); next_features_scaled[0] = next_pred_target_scaled\n",
        "            for feat_idx in range(1, n_features): next_features_scaled[feat_idx] = current_input_imputed_2d[-1, feat_idx]\n",
        "            new_input_sequence_2d = np.roll(current_input_imputed_2d, -1, axis=0)\n",
        "            new_input_sequence_2d[-1, :] = next_features_scaled\n",
        "            current_input_sequence_scaled_2d = new_input_sequence_2d\n",
        "        y_pred_scaled = np.array(y_pred_scaled_list)\n",
        "        dummy_pred_scaled_for_inverse = np.zeros((forecast_steps, n_features)); dummy_pred_scaled_for_inverse[:, 0] = y_pred_scaled\n",
        "        if n_features > 1:\n",
        "            for feat_idx in range(1, n_features):\n",
        "                 last_known_vals = X_pred_seq_scaled[:, feat_idx]\n",
        "                 dummy_pred_scaled_for_inverse[:, feat_idx] = np.nanmean(last_known_vals) if not np.isnan(last_known_vals).all() else 0.0\n",
        "        y_pred_unscaled_full = scaler.inverse_transform(dummy_pred_scaled_for_inverse); y_pred_unscaled = y_pred_unscaled_full[:, 0]\n",
        "        y_pred_unscaled[~np.isfinite(y_pred_unscaled)] = np.nan\n",
        "    except Exception: y_pred_unscaled.fill(np.nan)\n",
        "    y_pred_unscaled = np.maximum(0, y_pred_unscaled)\n",
        "    valid_pred_idx = ~np.isnan(y_pred_unscaled); valid_target_idx = ~np.isnan(target_actual_unscaled_test)\n",
        "    common_valid_idx = valid_pred_idx & valid_target_idx\n",
        "    if np.any(common_valid_idx):\n",
        "        y_true_v = target_actual_unscaled_test[common_valid_idx]; y_pred_v = y_pred_unscaled[common_valid_idx]\n",
        "        if len(y_true_v) > 0:\n",
        "             try:\n",
        "                 rmse = np.sqrt(mean_squared_error(y_true_v, y_pred_v))\n",
        "                 if len(y_true_v) > 1 and np.nanstd(y_true_v) > 1e-6: r2 = r2_score(y_true_v, y_pred_v)\n",
        "             except Exception: pass\n",
        "    return y_pred_unscaled, rmse, r2, ljung_box_p_value_insample, scaler"
      ],
      "metadata": {
        "id": "WYJoJEyqhUrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNGSI MODEL FORECASTERS (FUTURE FORECAST) ---\n",
        "def forecast_arima_future(ts_full_history: np.ndarray, n_lag: int, forecast_steps: int, n_features: int) -> np.ndarray:\n",
        "    y_pred_future = np.full(forecast_steps, np.nan)\n",
        "    ts_target_history = ts_full_history[:, 0] if ts_full_history.ndim > 1 and ts_full_history.shape[1] > 0 else ts_full_history.flatten()\n",
        "    valid_hist_indices = ~np.isnan(ts_target_history); ts_target_history_valid = ts_target_history[valid_hist_indices]\n",
        "    if len(ts_target_history_valid) < 20: return y_pred_future\n",
        "    series_history = pd.Series(ts_target_history_valid).interpolate(method='linear', limit_direction='both').fillna(method='bfill').fillna(method='ffill')\n",
        "    if series_history.isnull().any() or len(series_history) < 20: return y_pred_future\n",
        "    try:\n",
        "        auto_model_future = pm.auto_arima(series_history, start_p=1, start_q=1, max_p=3, max_q=3,\n",
        "                                          d=None, max_d=MAX_DIFFERENCING, seasonal=False,\n",
        "                                          stepwise=True, suppress_warnings=True, error_action='ignore', trace=False)\n",
        "        y_pred_future = auto_model_future.predict(n_periods=forecast_steps)\n",
        "        y_pred_future[~np.isfinite(y_pred_future)] = np.nan\n",
        "    except Exception: pass\n",
        "    y_pred_future = np.maximum(0, y_pred_future)\n",
        "    return y_pred_future\n",
        "\n",
        "def forecast_lstm_future(\n",
        "    ts_full_history: np.ndarray, n_lag: int, forecast_steps: int, n_features: int, hyperparams: Dict[str, Any]\n",
        ") -> np.ndarray:\n",
        "    scaler = None; y_pred_unscaled_future = np.full(forecast_steps, np.nan)\n",
        "    lstm_units = hyperparams.get(\"lstm_units\", 32)\n",
        "    lstm_activation = hyperparams.get(\"lstm_activation\", 'relu')\n",
        "    learning_rate = hyperparams.get(\"learning_rate\", 0.01)\n",
        "    epochs = hyperparams.get(\"epochs\", 50)\n",
        "    batch_size_val = hyperparams.get(\"batch_size_val\", None)\n",
        "    if ts_full_history.ndim != 2 or ts_full_history.shape[1] != n_features: return y_pred_unscaled_future\n",
        "    ts_history_nonan_for_scaler = ts_full_history[~np.isnan(ts_full_history).any(axis=1)]\n",
        "    if ts_history_nonan_for_scaler.shape[0] < n_lag + 1 : return y_pred_unscaled_future\n",
        "    try:\n",
        "        scaler = MinMaxScaler(); scaler.fit(ts_history_nonan_for_scaler)\n",
        "        ts_scaled = scaler.transform(ts_full_history); ts_scaled_imputed_df = pd.DataFrame(ts_scaled)\n",
        "        ts_scaled_imputed = ts_scaled_imputed_df.ffill().bfill().fillna(0.0).values\n",
        "        X_train_list, y_train_list_scaled = [], []\n",
        "        if len(ts_scaled_imputed) < n_lag + 1: return y_pred_unscaled_future\n",
        "        for k in range(len(ts_scaled_imputed) - n_lag):\n",
        "            input_window = ts_scaled_imputed[k : k + n_lag, :]; target_val_scaled = ts_scaled_imputed[k + n_lag, 0]\n",
        "            X_train_list.append(input_window); y_train_list_scaled.append(target_val_scaled)\n",
        "        if not X_train_list: return y_pred_unscaled_future\n",
        "        X_train_scaled = np.array(X_train_list); y_train_scaled = np.array(y_train_list_scaled)\n",
        "        tf.keras.backend.clear_session()\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.LSTM(lstm_units, activation=lstm_activation, input_shape=(n_lag, n_features)),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
        "        current_batch_size = batch_size_val if batch_size_val is not None else min(16, len(X_train_scaled))\n",
        "        if current_batch_size == 0 and len(X_train_scaled) > 0: current_batch_size = 1\n",
        "        if len(X_train_scaled) == 0: return y_pred_unscaled_future\n",
        "        model.fit(X_train_scaled, y_train_scaled, epochs=epochs, batch_size=current_batch_size, verbose=0)\n",
        "        last_known_sequence_scaled = ts_scaled_imputed[-n_lag:, :]; y_pred_scaled_future_list = []\n",
        "        current_input_scaled = last_known_sequence_scaled.reshape(1, n_lag, n_features)\n",
        "        for _ in range(forecast_steps):\n",
        "            current_input_imputed = np.nan_to_num(current_input_scaled, nan=0.0)\n",
        "            next_pred_target_scaled = model.predict(current_input_imputed, verbose=0)[0, 0]; y_pred_scaled_future_list.append(next_pred_target_scaled)\n",
        "            next_features_scaled = np.zeros(n_features); next_features_scaled[0] = next_pred_target_scaled\n",
        "            for feat_idx in range(1, n_features): next_features_scaled[feat_idx] = current_input_imputed[0, -1, feat_idx]\n",
        "            new_input_sequence = np.roll(current_input_imputed[0], -1, axis=0); new_input_sequence[-1, :] = next_features_scaled\n",
        "            current_input_scaled = new_input_sequence.reshape(1, n_lag, n_features)\n",
        "        y_pred_scaled_future = np.array(y_pred_scaled_future_list); dummy_pred_scaled_for_inverse = np.zeros((forecast_steps, n_features)); dummy_pred_scaled_for_inverse[:, 0] = y_pred_scaled_future\n",
        "        if n_features > 1:\n",
        "            for feat_idx in range(1, n_features):\n",
        "                 last_known_vals = last_known_sequence_scaled[:, feat_idx]\n",
        "                 dummy_pred_scaled_for_inverse[:, feat_idx] = np.nanmean(last_known_vals) if not np.isnan(last_known_vals).all() else 0.0\n",
        "        y_pred_unscaled_full = scaler.inverse_transform(dummy_pred_scaled_for_inverse); y_pred_unscaled_future = y_pred_unscaled_full[:, 0]\n",
        "        y_pred_unscaled_future[~np.isfinite(y_pred_unscaled_future)] = np.nan\n",
        "    except Exception: pass\n",
        "    y_pred_unscaled_future = np.maximum(0, y_pred_unscaled_future)\n",
        "    return y_pred_unscaled_future\n",
        "\n",
        "def forecast_ann_future(\n",
        "    ts_full_history: np.ndarray, n_lag: int, forecast_steps: int, n_features: int, hyperparams: Dict[str, Any]\n",
        ") -> np.ndarray:\n",
        "    scaler = None; y_pred_unscaled_future = np.full(forecast_steps, np.nan)\n",
        "    ann_layers_units = hyperparams.get(\"ann_layers_units\", [64, 32])\n",
        "    ann_activation = hyperparams.get(\"ann_activation\", 'relu')\n",
        "    learning_rate = hyperparams.get(\"learning_rate\", 0.01)\n",
        "    epochs = hyperparams.get(\"epochs\", 50)\n",
        "    batch_size_val = hyperparams.get(\"batch_size_val\", None)\n",
        "    if ts_full_history.ndim != 2 or ts_full_history.shape[1] != n_features: return y_pred_unscaled_future\n",
        "    ts_history_nonan_for_scaler = ts_full_history[~np.isnan(ts_full_history).any(axis=1)]\n",
        "    if ts_history_nonan_for_scaler.shape[0] < n_lag + 1: return y_pred_unscaled_future\n",
        "    try:\n",
        "        scaler = MinMaxScaler(); scaler.fit(ts_history_nonan_for_scaler)\n",
        "        ts_scaled = scaler.transform(ts_full_history); ts_scaled_imputed_df = pd.DataFrame(ts_scaled)\n",
        "        ts_scaled_imputed = ts_scaled_imputed_df.ffill().bfill().fillna(0.0).values\n",
        "        X_train_flat_list, y_train_list_scaled = [], []\n",
        "        if len(ts_scaled_imputed) < n_lag + 1: return y_pred_unscaled_future\n",
        "        for k in range(len(ts_scaled_imputed) - n_lag):\n",
        "            input_window = ts_scaled_imputed[k : k + n_lag, :]\n",
        "            target_val_scaled = ts_scaled_imputed[k + n_lag, 0]\n",
        "            X_train_flat_list.append(input_window.flatten()); y_train_list_scaled.append(target_val_scaled)\n",
        "        if not X_train_flat_list: return y_pred_unscaled_future\n",
        "        X_train_flat_scaled = np.array(X_train_flat_list); y_train_scaled = np.array(y_train_list_scaled)\n",
        "        input_shape_flat = n_lag * n_features; tf.keras.backend.clear_session()\n",
        "        model_layers = [tf.keras.layers.Dense(ann_layers_units[0], activation=ann_activation, input_shape=(input_shape_flat,))]\n",
        "        for units in ann_layers_units[1:]: model_layers.append(tf.keras.layers.Dense(units, activation=ann_activation))\n",
        "        model_layers.append(tf.keras.layers.Dense(1)); model = tf.keras.Sequential(model_layers)\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
        "        current_batch_size = batch_size_val if batch_size_val is not None else min(16, len(X_train_flat_scaled))\n",
        "        if current_batch_size == 0 and len(X_train_flat_scaled) > 0: current_batch_size = 1\n",
        "        if len(X_train_flat_scaled) == 0: return y_pred_unscaled_future\n",
        "        model.fit(X_train_flat_scaled, y_train_scaled, epochs=epochs, batch_size=current_batch_size, verbose=0)\n",
        "        last_known_sequence_scaled_2d = ts_scaled_imputed[-n_lag:, :]; y_pred_scaled_future_list = []\n",
        "        current_input_sequence_scaled_2d = last_known_sequence_scaled_2d\n",
        "        for _ in range(forecast_steps):\n",
        "            current_input_imputed_2d = np.nan_to_num(current_input_sequence_scaled_2d, nan=0.0)\n",
        "            current_input_flat_for_pred = current_input_imputed_2d.flatten().reshape(1, -1)\n",
        "            next_pred_target_scaled = model.predict(current_input_flat_for_pred, verbose=0)[0, 0]; y_pred_scaled_future_list.append(next_pred_target_scaled)\n",
        "            next_features_scaled = np.zeros(n_features); next_features_scaled[0] = next_pred_target_scaled\n",
        "            for feat_idx in range(1, n_features): next_features_scaled[feat_idx] = current_input_imputed_2d[-1, feat_idx]\n",
        "            new_input_sequence_2d = np.roll(current_input_imputed_2d, -1, axis=0); new_input_sequence_2d[-1, :] = next_features_scaled\n",
        "            current_input_sequence_scaled_2d = new_input_sequence_2d\n",
        "        y_pred_scaled_future = np.array(y_pred_scaled_future_list); dummy_pred_scaled_for_inverse = np.zeros((forecast_steps, n_features)); dummy_pred_scaled_for_inverse[:, 0] = y_pred_scaled_future\n",
        "        if n_features > 1:\n",
        "            for feat_idx in range(1, n_features):\n",
        "                 last_known_vals = last_known_sequence_scaled_2d[:, feat_idx]\n",
        "                 dummy_pred_scaled_for_inverse[:, feat_idx] = np.nanmean(last_known_vals) if not np.isnan(last_known_vals).all() else 0.0\n",
        "        y_pred_unscaled_full = scaler.inverse_transform(dummy_pred_scaled_for_inverse); y_pred_unscaled_future = y_pred_unscaled_full[:, 0]\n",
        "        y_pred_unscaled_future[~np.isfinite(y_pred_unscaled_future)] = np.nan\n",
        "    except Exception: pass\n",
        "    y_pred_unscaled_future = np.maximum(0, y_pred_unscaled_future)\n",
        "    return y_pred_unscaled_future\n",
        "\n",
        "def forecast_rf_future(\n",
        "    ts_full_history: np.ndarray, n_lag: int, forecast_steps: int, n_features: int, hyperparams: Dict[str, Any]\n",
        ") -> np.ndarray:\n",
        "    scaler = None; y_pred_unscaled_future = np.full(forecast_steps, np.nan)\n",
        "    n_estimators = hyperparams.get(\"n_estimators\", 100)\n",
        "    rf_random_state = hyperparams.get(\"rf_random_state\", 42)\n",
        "    max_depth = hyperparams.get(\"max_depth\", None)\n",
        "    min_samples_split = hyperparams.get(\"min_samples_split\", 2)\n",
        "    min_samples_leaf = hyperparams.get(\"min_samples_leaf\", 1)\n",
        "    if ts_full_history.ndim != 2 or ts_full_history.shape[1] != n_features: return y_pred_unscaled_future\n",
        "    ts_history_nonan_for_scaler = ts_full_history[~np.isnan(ts_full_history).any(axis=1)]\n",
        "    if ts_history_nonan_for_scaler.shape[0] < n_lag + 1: return y_pred_unscaled_future\n",
        "    try:\n",
        "        scaler = MinMaxScaler(); scaler.fit(ts_history_nonan_for_scaler)\n",
        "        ts_scaled = scaler.transform(ts_full_history); ts_scaled_imputed_df = pd.DataFrame(ts_scaled)\n",
        "        ts_scaled_imputed = ts_scaled_imputed_df.ffill().bfill().fillna(0.0).values\n",
        "        X_train_flat_list, y_train_list_scaled = [], []\n",
        "        if len(ts_scaled_imputed) < n_lag + 1: return y_pred_unscaled_future\n",
        "        for k in range(len(ts_scaled_imputed) - n_lag):\n",
        "            input_window = ts_scaled_imputed[k : k + n_lag, :]\n",
        "            target_val_scaled = ts_scaled_imputed[k + n_lag, 0]\n",
        "            X_train_flat_list.append(np.nan_to_num(input_window, nan=0.0).flatten()); y_train_list_scaled.append(target_val_scaled)\n",
        "        if not X_train_flat_list: return y_pred_unscaled_future\n",
        "        X_train_flat_scaled = np.array(X_train_flat_list); y_train_scaled = np.array(y_train_list_scaled)\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=n_estimators, random_state=rf_random_state, max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, n_jobs=1\n",
        "        )\n",
        "        model.fit(X_train_flat_scaled, y_train_scaled)\n",
        "        last_known_sequence_scaled_2d = ts_scaled_imputed[-n_lag:, :]; y_pred_scaled_future_list = []\n",
        "        current_input_sequence_scaled_2d = last_known_sequence_scaled_2d\n",
        "        for _ in range(forecast_steps):\n",
        "            current_input_imputed_2d = np.nan_to_num(current_input_sequence_scaled_2d, nan=0.0)\n",
        "            current_input_flat_for_pred = current_input_imputed_2d.flatten().reshape(1, -1)\n",
        "            next_pred_target_scaled = model.predict(current_input_flat_for_pred)[0]; y_pred_scaled_future_list.append(next_pred_target_scaled)\n",
        "            next_features_scaled = np.zeros(n_features); next_features_scaled[0] = next_pred_target_scaled\n",
        "            for feat_idx in range(1, n_features): next_features_scaled[feat_idx] = current_input_imputed_2d[-1, feat_idx]\n",
        "            new_input_sequence_2d = np.roll(current_input_imputed_2d, -1, axis=0); new_input_sequence_2d[-1, :] = next_features_scaled\n",
        "            current_input_sequence_scaled_2d = new_input_sequence_2d\n",
        "        y_pred_scaled_future = np.array(y_pred_scaled_future_list); dummy_pred_scaled_for_inverse = np.zeros((forecast_steps, n_features)); dummy_pred_scaled_for_inverse[:, 0] = y_pred_scaled_future\n",
        "        if n_features > 1:\n",
        "            for feat_idx in range(1, n_features):\n",
        "                 last_known_vals = last_known_sequence_scaled_2d[:, feat_idx]\n",
        "                 dummy_pred_scaled_for_inverse[:, feat_idx] = np.nanmean(last_known_vals) if not np.isnan(last_known_vals).all() else 0.0\n",
        "        y_pred_unscaled_full = scaler.inverse_transform(dummy_pred_scaled_for_inverse); y_pred_unscaled_future = y_pred_unscaled_full[:, 0]\n",
        "        y_pred_unscaled_future[~np.isfinite(y_pred_unscaled_future)] = np.nan\n",
        "    except Exception: pass\n",
        "    y_pred_unscaled_future = np.maximum(0, y_pred_unscaled_future)\n",
        "    return y_pred_unscaled_future"
      ],
      "metadata": {
        "id": "-hfzav6mpW18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNGSI UTAMA PER MODEL (EVALUASI & FUTURE) ---\n",
        "# Modifikasi run_forecast_per_pixel_eval untuk menangani output baru dari forecast_arima\n",
        "# dan meneruskan hyperparams ke model ML\n",
        "def run_forecast_per_pixel_eval(args):\n",
        "    \"\"\"\n",
        "    Menjalankan fungsi peramalan evaluasi untuk satu piksel.\n",
        "    Menerima semua argumen yang diperlukan termasuk hyperparameter model.\n",
        "    \"\"\"\n",
        "    # Args: i, j, model_func, ts_data_input, n_lag, forecast_steps, ts_test_obs_target, n_features, ljung_box_lags_config, model_hyperparams\n",
        "    i, j, model_func, ts_data_input, n_lag, forecast_steps, ts_test_obs_target, n_features, ljung_box_lags_cfg, model_hyperparams_dict = args\n",
        "    y_pred, rmse, r2 = np.full(forecast_steps, np.nan), np.nan, np.nan\n",
        "    ljung_box_p_val = np.nan\n",
        "    arima_order_or_d_used = None # Akan berisi tuple (p,d,q) untuk ARIMA, atau None untuk ML\n",
        "\n",
        "    try:\n",
        "        if model_func == forecast_arima: # Menggunakan pm.auto_arima\n",
        "            # forecast_arima sekarang mengembalikan order_used (p,d,q)\n",
        "            y_pred, rmse, r2, ljung_box_p_val, arima_order_or_d_used = model_func(\n",
        "                ts_data_input, ts_test_obs_target, forecast_steps, ljung_box_lags_cfg\n",
        "            )\n",
        "        elif model_func in [forecast_lstm, forecast_ann, forecast_rf]:\n",
        "             # Teruskan model_hyperparams_dict ke fungsi peramalan ML\n",
        "             y_pred, rmse, r2, ljung_box_p_val, _ = model_func( # _ untuk scaler yang tidak digunakan di sini\n",
        "                 ts_data_input, n_lag, forecast_steps, n_features, ljung_box_lags_cfg, model_hyperparams_dict\n",
        "             )\n",
        "        else: # Fungsi model tidak dikenal\n",
        "            # logging.warning(f\"Pixel ({i},{j}): Unknown model function {model_func}\")\n",
        "            return i, j, np.full(forecast_steps, np.nan), np.nan, np.nan, np.nan, None\n",
        "\n",
        "        return i, j, y_pred, rmse, r2, ljung_box_p_val, arima_order_or_d_used\n",
        "\n",
        "    except Exception as e_pixel:\n",
        "        # logging.debug(f\"Pixel ({i},{j}), Model {model_func.__name__}: Error in run_forecast_per_pixel_eval: {e_pixel}\")\n",
        "        return i, j, np.full(forecast_steps, np.nan), np.nan, np.nan, np.nan, None\n",
        "\n",
        "def run_generic_forecast_eval(\n",
        "    model_func: Callable, model_name: str, ds: xr.Dataset, feature_list: List[str], target_var: str,\n",
        "    n_lag: int, forecast_steps: int, time_index: pd.DatetimeIndex, time_test: pd.DatetimeIndex,\n",
        "    ny: int, nx: int, sample_coords: List[Tuple[int, int]], output_base_dir: str, ds_ref: xr.Dataset,\n",
        "    # model_hyperparams argumen tidak lagi dibutuhkan di sini karena akan diambil dari global\n",
        "    use_parallel: bool = True, n_jobs: int = -1\n",
        "):\n",
        "    \"\"\"\n",
        "    Menjalankan proses evaluasi peramalan generik untuk model tertentu.\n",
        "    Mengambil hyperparameter dari variabel global HYPERPARAMS_LSTM, HYPERPARAMS_ANN, HYPERPARAMS_RF.\n",
        "    \"\"\"\n",
        "    outdir = os.path.join(output_base_dir, f\"evaluation_{model_name}\"); os.makedirs(outdir, exist_ok=True)\n",
        "    logging.info(f\"--- Starting {model_name} EVALUATION (Target: {target_var}, Features: {feature_list}) ---\")\n",
        "\n",
        "    # Tentukan hyperparameter yang akan digunakan berdasarkan nama model\n",
        "    current_model_hyperparams = None # Default untuk ARIMA\n",
        "    if model_name == \"LSTM\":\n",
        "        current_model_hyperparams = HYPERPARAMS_LSTM\n",
        "        logging.info(f\"Using Hyperparameters for LSTM: {current_model_hyperparams}\")\n",
        "    elif model_name == \"ANN\":\n",
        "        current_model_hyperparams = HYPERPARAMS_ANN\n",
        "        logging.info(f\"Using Hyperparameters for ANN: {current_model_hyperparams}\")\n",
        "    elif model_name == \"RF\":\n",
        "        current_model_hyperparams = HYPERPARAMS_RF\n",
        "        logging.info(f\"Using Hyperparameters for RF: {current_model_hyperparams}\")\n",
        "\n",
        "    if target_var not in feature_list: logging.error(f\"Target var '{target_var}' not in features: {feature_list}\"); return\n",
        "    target_idx = feature_list.index(target_var); n_features = len(feature_list)\n",
        "    forecast_stack = np.full((forecast_steps, ny, nx), np.nan)\n",
        "    metrics_maps = {'rmse': np.full((ny, nx), np.nan), 'r2': np.full((ny, nx), np.nan), 'ljung_box_pvalue': np.full((ny, nx), np.nan)}\n",
        "    arima_d_map = np.full((ny, nx), np.nan) if model_name == \"ARIMA\" else None\n",
        "\n",
        "    try:\n",
        "        all_feature_data = np.stack([ds[feat].values for feat in feature_list], axis=-1)\n",
        "        logging.info(f\"Stacked data shape for evaluation: {all_feature_data.shape}\")\n",
        "    except KeyError as ke: logging.error(f\"Feature key error: {ke}\"); return\n",
        "    except Exception as stack_err: logging.error(f\"Error stacking data: {stack_err}\"); return\n",
        "\n",
        "    n_time_total = all_feature_data.shape[0]; test_start_idx = n_time_total - forecast_steps; train_end_idx = test_start_idx\n",
        "    logging.info(f\"Preparing evaluation tasks for {ny*nx} pixels...\"); tasks = []\n",
        "    for i in range(ny):\n",
        "        for j in range(nx):\n",
        "            ts_multi_feature_pixel = all_feature_data[:, i, j, :]\n",
        "            if np.isnan(ts_multi_feature_pixel[:, target_idx]).all(): continue\n",
        "            ts_test_obs_target = ts_multi_feature_pixel[test_start_idx:, target_idx]\n",
        "\n",
        "            if model_func == forecast_arima:\n",
        "                ts_target_pixel_for_arima = ts_multi_feature_pixel[:, target_idx]\n",
        "                if train_end_idx > 0: # Pastikan ada data training\n",
        "                    ts_train_arima = ts_target_pixel_for_arima[:train_end_idx]\n",
        "                    # auto_arima butuh lebih banyak data, misal minimal 20\n",
        "                    if len(ts_train_arima[~np.isnan(ts_train_arima)]) >= 20 and not np.isnan(ts_test_obs_target).all():\n",
        "                         # Argumen ke-10 adalah model_hyperparams, None untuk ARIMA\n",
        "                         tasks.append((i, j, model_func, ts_train_arima, n_lag, forecast_steps, ts_test_obs_target, 1, LJUNG_BOX_LAGS_CONFIG, None))\n",
        "            elif model_func in [forecast_lstm, forecast_ann, forecast_rf]:\n",
        "                input_start_idx = 0; # Gunakan semua histori untuk input ML\n",
        "                input_end_idx = train_end_idx + forecast_steps # Seluruh data yang dilihat model (train+test)\n",
        "                ts_input_ml = ts_multi_feature_pixel[input_start_idx:input_end_idx, :]\n",
        "                if ts_input_ml.shape[0] >= n_lag + forecast_steps: # Cukup data untuk membentuk sekuens\n",
        "                     train_part_target_for_ml = ts_input_ml[:-forecast_steps, target_idx]\n",
        "                     if np.sum(~np.isnan(train_part_target_for_ml)) >= n_lag +1 : # Cukup data non-NaN untuk training\n",
        "                        spatial_feat_valid = True\n",
        "                        if SPATIAL_FEATURE_NAME in feature_list:\n",
        "                            spatial_feat_idx_in_ml = feature_list.index(SPATIAL_FEATURE_NAME)\n",
        "                            spatial_train_part_ml = ts_input_ml[:-forecast_steps, spatial_feat_idx_in_ml]\n",
        "                            if np.isnan(spatial_train_part_ml).all(): spatial_feat_valid = False\n",
        "                        if spatial_feat_valid and not np.isnan(ts_test_obs_target).all():\n",
        "                            # Teruskan current_model_hyperparams yang sudah ditentukan\n",
        "                            tasks.append((i, j, model_func, ts_input_ml, n_lag, forecast_steps, ts_test_obs_target, n_features, LJUNG_BOX_LAGS_CONFIG, current_model_hyperparams))\n",
        "    results = []\n",
        "    if len(tasks) > 0 :\n",
        "        if use_parallel:\n",
        "            logging.info(f\"Running {model_name} evaluation forecast in parallel (n_jobs={n_jobs}) for {len(tasks)} pixels...\")\n",
        "            if n_jobs == -1: actual_n_jobs = os.cpu_count() or 1\n",
        "            else: actual_n_jobs = min(n_jobs, os.cpu_count() or 1)\n",
        "            results = Parallel(n_jobs=actual_n_jobs, backend='loky', verbose=10)(delayed(run_forecast_per_pixel_eval)(task) for task in tasks)\n",
        "        else:\n",
        "            logging.info(f\"Running {model_name} evaluation forecast sequentially for {len(tasks)} pixels...\")\n",
        "            results = [run_forecast_per_pixel_eval(task) for task in tqdm(tasks, desc=f\"{model_name} Eval Pixels\")]\n",
        "    else: logging.warning(f\"No valid evaluation tasks generated for {model_name}.\")\n",
        "\n",
        "    logging.info(f\"Collecting evaluation results for {model_name}...\"); valid_results_count = 0\n",
        "    all_arima_orders_collected = [] # Untuk menyimpan semua order ARIMA jika diperlukan\n",
        "    for res in results:\n",
        "        if res:\n",
        "            i_res, j_res, y_pred_res, rmse_res, r2_res, ljung_box_pvalue_res, arima_order_or_d = res\n",
        "            if y_pred_res is not None and len(y_pred_res) == forecast_steps:\n",
        "                forecast_stack[:, i_res, j_res] = y_pred_res\n",
        "                if not np.isnan(y_pred_res).all(): valid_results_count += 1\n",
        "            else: forecast_stack[:, i_res, j_res] = np.nan\n",
        "            metrics_maps['rmse'][i_res, j_res] = rmse_res; metrics_maps['r2'][i_res, j_res] = r2_res\n",
        "            metrics_maps['ljung_box_pvalue'][i_res, j_res] = ljung_box_pvalue_res\n",
        "            if model_name == \"ARIMA\" and arima_order_or_d is not None:\n",
        "                if isinstance(arima_order_or_d, tuple) and len(arima_order_or_d) == 3: # (p,d,q)\n",
        "                    arima_d_map[i_res, j_res] = arima_order_or_d[1] # Simpan 'd'\n",
        "                    all_arima_orders_collected.append({'y': i_res, 'x': j_res, 'order_p': arima_order_or_d[0], 'order_d': arima_order_or_d[1], 'order_q': arima_order_or_d[2]})\n",
        "                # elif isinstance(arima_order_or_d, int) : # Jika hanya 'd' yang dikembalikan (versi lama)\n",
        "                #      arima_d_map[i_res, j_res] = arima_order_or_d\n",
        "\n",
        "    logging.info(f\"Finished collecting results. Got valid predictions for {valid_results_count} out of {len(tasks)} processed pixels.\")\n",
        "\n",
        "    if model_name == \"ARIMA\" and all_arima_orders_collected:\n",
        "        try:\n",
        "            orders_df = pd.DataFrame(all_arima_orders_collected)\n",
        "            orders_csv_path = os.path.join(outdir, \"arima_orders_used.csv\")\n",
        "            orders_df.to_csv(orders_csv_path, index=False)\n",
        "            logging.info(f\"✅ ARIMA orders (p,d,q) saved to: {orders_csv_path}\")\n",
        "        except Exception as e_csv:\n",
        "            logging.error(f\"❌ Failed to save ARIMA orders to CSV: {e_csv}\")\n",
        "\n",
        "    logging.info(f\"--- {model_name} Evaluation Forecasting Complete ---\"); logging.info(f\"--- Saving {model_name} Evaluation Outputs to {outdir} ---\")\n",
        "    plot_accuracy_maps(metrics_maps, outdir); save_evaluation_metrics_geotiff(metrics_maps, ds_ref, outdir)\n",
        "    plot_sample_timeseries(ds, forecast_stack, time_index, time_test, n_lag, forecast_steps, sample_coords, outdir, target_var=target_var)\n",
        "    save_animation(forecast_stack, time_test, outdir, model_name, var_name=f\"Forecast_{target_var}\", prefix=\"eval\")\n",
        "    save_forecast_geotiff(forecast_stack, ds_ref, time_test, outdir, var_name=f\"Forecast_{target_var}\", prefix=\"eval\")\n",
        "    save_forecast_netcdf(forecast_stack, metrics_maps, ds_ref, time_test, os.path.join(outdir, f\"forecast_eval_{model_name}.nc\"), target_var=target_var, is_future=False)\n",
        "    if arima_d_map is not None and not np.isnan(arima_d_map).all():\n",
        "        try:\n",
        "            d_map_da = xr.DataArray(arima_d_map, coords={\"y\": ds_ref[\"y\"], \"x\": ds_ref[\"x\"]}, dims=(\"y\", \"x\"), name=\"arima_d_order_auto\", attrs={'long_name': 'ARIMA Differencing Order Used (auto_arima)'})\n",
        "            if hasattr(ds_ref, 'rio') and ds_ref.rio.crs: d_map_da = d_map_da.rio.write_crs(ds_ref.rio.crs)\n",
        "            if hasattr(ds_ref, 'rio') and ds_ref.rio.transform(): d_map_da = d_map_da.rio.write_transform(ds_ref.rio.transform())\n",
        "            d_map_da = d_map_da.rio.set_spatial_dims(x_dim='x', y_dim='y')\n",
        "            d_map_path = os.path.join(outdir, \"arima_d_order_auto_map.tif\")\n",
        "            d_map_da.rio.to_raster(d_map_path, tiled=True, compress='LZW', num_threads='ALL_CPUS', nodata=np.nan, dtype='int8')\n",
        "            logging.info(f\"✅ ARIMA differencing order (d) map saved to: {d_map_path}\")\n",
        "        except Exception as e_dmap: logging.error(f\"❌ Failed to save ARIMA d order map: {e_dmap}\")\n",
        "    logging.info(f\"✅ {model_name} evaluation process completed and outputs saved to: {outdir}\")\n",
        "\n",
        "def run_forecast_per_pixel_future(args):\n",
        "    # Args: i, j, model_func_future, ts_full_history, n_lag, forecast_steps, n_features, model_hyperparams\n",
        "    i, j, model_func_future, ts_full_history, n_lag, forecast_steps, n_features, model_hyperparams_dict = args\n",
        "    try:\n",
        "        if model_func_future == forecast_arima_future:\n",
        "            # ARIMA future tidak memerlukan hyperparams dict dari luar, auto_arima menangani sendiri\n",
        "            y_pred_future = model_func_future(ts_full_history, n_lag, forecast_steps, n_features)\n",
        "        else: # Untuk model ML, teruskan hyperparams_dict\n",
        "            y_pred_future = model_func_future(ts_full_history, n_lag, forecast_steps, n_features, model_hyperparams_dict)\n",
        "        return i, j, y_pred_future\n",
        "    except Exception:\n",
        "        return i, j, np.full(forecast_steps, np.nan)\n",
        "\n",
        "def run_generic_future_forecast(\n",
        "    model_func_future: Callable, model_name: str, ds: xr.Dataset, feature_list: List[str], target_var: str,\n",
        "    n_lag: int, forecast_steps: int, time_future: pd.DatetimeIndex, ny: int, nx: int,\n",
        "    output_base_dir: str, ds_ref: xr.Dataset,\n",
        "    # model_hyperparams argumen tidak lagi jadi argumen di sini, diambil dari global\n",
        "    use_parallel: bool = True, n_jobs: int = -1\n",
        "):\n",
        "    outdir = os.path.join(output_base_dir, f\"future_{model_name}\"); os.makedirs(outdir, exist_ok=True)\n",
        "    logging.info(f\"--- Starting {model_name} FUTURE FORECAST (Target: {target_var}, Features: {feature_list}) ---\")\n",
        "\n",
        "    current_model_hyperparams = None # Default untuk ARIMA\n",
        "    if model_name == \"LSTM\":\n",
        "        current_model_hyperparams = HYPERPARAMS_LSTM\n",
        "        logging.info(f\"Using Hyperparameters for LSTM Future: {current_model_hyperparams}\")\n",
        "    elif model_name == \"ANN\":\n",
        "        current_model_hyperparams = HYPERPARAMS_ANN\n",
        "        logging.info(f\"Using Hyperparameters for ANN Future: {current_model_hyperparams}\")\n",
        "    elif model_name == \"RF\":\n",
        "        current_model_hyperparams = HYPERPARAMS_RF\n",
        "        logging.info(f\"Using Hyperparameters for RF Future: {current_model_hyperparams}\")\n",
        "\n",
        "    if target_var not in feature_list: logging.error(f\"Target var '{target_var}' not in features: {feature_list}\"); return\n",
        "    target_idx = feature_list.index(target_var); n_features = len(feature_list)\n",
        "    future_forecast_stack = np.full((forecast_steps, ny, nx), np.nan)\n",
        "    try:\n",
        "        all_feature_data = np.stack([ds[feat].values for feat in feature_list], axis=-1)\n",
        "        logging.info(f\"Stacked data shape for future forecast: {all_feature_data.shape}\")\n",
        "    except KeyError as ke: logging.error(f\"Feature key error: {ke}\"); return\n",
        "    except Exception as stack_err: logging.error(f\"Error stacking data: {stack_err}\"); return\n",
        "\n",
        "    logging.info(f\"Preparing future forecast tasks for {ny*nx} pixels...\"); tasks = []\n",
        "    for i in range(ny):\n",
        "        for j in range(nx):\n",
        "            ts_multi_feature_full_pixel = all_feature_data[:, i, j, :]\n",
        "            if np.isnan(ts_multi_feature_full_pixel[:, target_idx]).all(): continue\n",
        "            min_len_required = n_lag + 1 if model_func_future != forecast_arima_future else 20 # auto_arima butuh lebih banyak\n",
        "            valid_target_data_count = np.sum(~np.isnan(ts_multi_feature_full_pixel[:, target_idx]))\n",
        "            if valid_target_data_count >= min_len_required:\n",
        "                 spatial_feat_idx = feature_list.index(SPATIAL_FEATURE_NAME) if SPATIAL_FEATURE_NAME in feature_list else -1\n",
        "                 spatial_feat_valid_for_training = True\n",
        "                 if spatial_feat_idx != -1:\n",
        "                     if np.isnan(ts_multi_feature_full_pixel[:, spatial_feat_idx]).all(): spatial_feat_valid_for_training = False\n",
        "                 if spatial_feat_valid_for_training:\n",
        "                      # Teruskan current_model_hyperparams\n",
        "                      tasks.append((i, j, model_func_future, ts_multi_feature_full_pixel, n_lag, forecast_steps, n_features, current_model_hyperparams))\n",
        "    results = []\n",
        "    if len(tasks) > 0 :\n",
        "        if use_parallel:\n",
        "            logging.info(f\"Running {model_name} future forecast in parallel (n_jobs={n_jobs}) for {len(tasks)} pixels...\")\n",
        "            if n_jobs == -1: actual_n_jobs = os.cpu_count() or 1\n",
        "            else: actual_n_jobs = min(n_jobs, os.cpu_count() or 1)\n",
        "            results = Parallel(n_jobs=actual_n_jobs, backend='loky', verbose=10)(delayed(run_forecast_per_pixel_future)(task) for task in tasks)\n",
        "        else:\n",
        "            logging.info(f\"Running {model_name} future forecast sequentially for {len(tasks)} pixels...\")\n",
        "            results = [run_forecast_per_pixel_future(task) for task in tqdm(tasks, desc=f\"{model_name} Future Pixels\")]\n",
        "    else: logging.warning(f\"No valid future forecast tasks generated for {model_name}.\")\n",
        "\n",
        "    logging.info(f\"Collecting future forecast results for {model_name}...\"); valid_results_count = 0\n",
        "    for res in results:\n",
        "        if res:\n",
        "            i_res, j_res, y_pred_future_res = res\n",
        "            if y_pred_future_res is not None and len(y_pred_future_res) == forecast_steps:\n",
        "                future_forecast_stack[:, i_res, j_res] = y_pred_future_res\n",
        "                if not np.isnan(y_pred_future_res).all(): valid_results_count +=1\n",
        "            else: future_forecast_stack[:, i_res, j_res] = np.nan\n",
        "    logging.info(f\"Finished collecting results. Got valid future predictions for {valid_results_count} out of {len(tasks)} processed pixels.\")\n",
        "    logging.info(f\"--- {model_name} Future Forecasting Complete ---\"); logging.info(f\"--- Saving {model_name} Future Forecast Outputs to {outdir} ---\")\n",
        "    save_animation(future_forecast_stack, time_future, outdir, model_name, var_name=f\"Forecast_{target_var}\", prefix=\"future\")\n",
        "    save_forecast_geotiff(future_forecast_stack, ds_ref, time_future, outdir, var_name=f\"Forecast_{target_var}\", prefix=\"future\")\n",
        "    save_forecast_netcdf(future_forecast_stack, None, ds_ref, time_future, os.path.join(outdir, f\"forecast_future_{model_name}.nc\"), target_var=target_var, is_future=True)\n",
        "    logging.info(f\"✅ {model_name} future forecast process completed and outputs saved to: {outdir}\")"
      ],
      "metadata": {
        "id": "iQavpTqBrJOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MAIN EXECUTION BLOCK ---\n",
        "def main():\n",
        "    ds: Optional[xr.Dataset] = None\n",
        "    try:\n",
        "        logging.info(f\"--- Memulai Proses Peramalan Spasial-Temporal (v3.1 - Tahap 4: Optimasi Hyperparameter) ---\")\n",
        "        logging.info(f\"Base Directory: {os.path.abspath(BASE_DIR)}\")\n",
        "        logging.info(f\"NetCDF Input: {NC_FILE}\")\n",
        "        logging.info(f\"Target Variable: {TARGET_VARIABLE}, Target CRS: {TARGET_CRS}\")\n",
        "        logging.info(f\"Forecast Steps: {FORECAST_STEPS}, N Lag (for ML): {N_LAG}\")\n",
        "        logging.info(f\"ADF p-value Threshold: {ADF_P_THRESHOLD}, Max Differencing: {MAX_DIFFERENCING}\")\n",
        "        logging.info(f\"Ljung-Box Lags Config: {LJUNG_BOX_LAGS_CONFIG}\")\n",
        "        logging.info(f\"Hyperparameters LSTM: {HYPERPARAMS_LSTM}\")\n",
        "        logging.info(f\"Hyperparameters ANN: {HYPERPARAMS_ANN}\")\n",
        "        logging.info(f\"Hyperparameters RF: {HYPERPARAMS_RF}\")\n",
        "\n",
        "\n",
        "        logging.info(\"--- Memuat Data Target (NetCDF) ---\")\n",
        "        nc_path = os.path.join(BASE_DIR, NC_FILE)\n",
        "        try:\n",
        "            try: ds = xr.open_dataset(nc_path)\n",
        "            except ValueError:\n",
        "                 logging.warning(\"Gagal engine default saat membuka NetCDF, mencoba h5netcdf...\")\n",
        "                 try: ds = xr.open_dataset(nc_path, engine='h5netcdf')\n",
        "                 except ImportError: raise ImportError(\"Engine 'h5netcdf' tidak ditemukan. Harap install.\")\n",
        "            ds.load()\n",
        "            logging.info(f\"Berhasil memuat NetCDF '{NC_FILE}'.\")\n",
        "            if hasattr(ds, 'rio'):\n",
        "                try:\n",
        "                    ds = ds.rio.set_crs(TARGET_CRS, inplace=True)\n",
        "                    if 'x' in ds.coords and 'y' in ds.coords:\n",
        "                        current_x_dim = ds.rio.x_dim\n",
        "                        current_y_dim = ds.rio.y_dim\n",
        "                        if current_x_dim not in ds.dims or current_y_dim not in ds.dims:\n",
        "                             ds = ds.rio.set_spatial_dims(x_dim='x', y_dim='y', inplace=True)\n",
        "                             logging.info(f\"Dimensi spasial diatur ke x='{ds.rio.x_dim}', y='{ds.rio.y_dim}'.\")\n",
        "                    logging.info(f\"CRS dataset diatur ke: {ds.rio.crs}\")\n",
        "                    if ds.rio.crs and 'x' in ds.coords and 'y' in ds.coords:\n",
        "                        try:\n",
        "                            if ds.rio.transform().is_identity or not ds.rio.transform():\n",
        "                                ds = ds.rio.write_transform(inplace=True)\n",
        "                                logging.info(f\"Transform dataset dihitung ulang/ditulis: {ds.rio.transform()}\")\n",
        "                        except Exception as e_trans: logging.warning(f\"Tidak dapat menulis/menghitung transform secara otomatis: {e_trans}\")\n",
        "                except Exception as e_crs: logging.error(f\"❌ Gagal mengatur CRS '{TARGET_CRS}' pada dataset: {e_crs}\")\n",
        "            else: logging.warning(\"❌ Atribut 'rio' tidak ditemukan pada dataset. Tidak dapat mengatur CRS.\")\n",
        "        except FileNotFoundError: raise FileNotFoundError(f\"File NetCDF tidak ditemukan: {nc_path}\")\n",
        "        except Exception as e_nc: raise IOError(f\"Tidak dapat membuka atau memproses NetCDF '{NC_FILE}': {e_nc}\")\n",
        "\n",
        "        if TARGET_VARIABLE not in ds.variables: raise ValueError(f\"Variabel target '{TARGET_VARIABLE}' tidak ditemukan.\")\n",
        "        required_dims = ['time', 'y', 'x'];\n",
        "        if not all(d in ds.dims for d in required_dims): raise ValueError(f\"Dimensi {required_dims} harus ada.\")\n",
        "        ny, nx = ds.sizes['y'], ds.sizes['x']; n_time = ds.sizes['time']\n",
        "        logging.info(f\"Data target dimuat: Var '{TARGET_VARIABLE}' shape (time={n_time}, y={ny}, x={nx}).\")\n",
        "        if FORECAST_STEPS <= 0: raise ValueError(\"FORECAST_STEPS > 0.\")\n",
        "        if n_time <= FORECAST_STEPS: raise ValueError(f\"n_time ({n_time}) > FORECAST_STEPS ({FORECAST_STEPS}).\")\n",
        "        if n_time < 20: logging.warning(f\"Data waktu ({n_time}) sangat pendek, hasil auto_arima mungkin tidak andal. Model ML juga mungkin terpengaruh.\")\n",
        "\n",
        "        try: time_index = pd.to_datetime(ds['time'].values)\n",
        "        except Exception as time_err: raise ValueError(f\"Gagal parse 'time': {time_err}\")\n",
        "        logging.info(f\"Periode data historis: {time_index[0].strftime('%Y-%m-%d')} hingga {time_index[-1].strftime('%Y-%m-%d')}\")\n",
        "        test_start_index = n_time - FORECAST_STEPS; time_test = time_index[test_start_index:]\n",
        "        logging.info(f\"Periode evaluasi: {time_test[0].strftime('%Y-%m-%d')} hingga {time_test[-1].strftime('%Y-%m-%d')}\")\n",
        "        time_future = None\n",
        "        try:\n",
        "            inferred_freq = pd.infer_freq(time_index); freq = inferred_freq if inferred_freq else 'MS'\n",
        "            logging.info(f\"Frekuensi waktu digunakan untuk masa depan: {freq}\")\n",
        "            last_historical_date = time_index[-1]\n",
        "            future_start_date = last_historical_date + pd.tseries.frequencies.to_offset(freq)\n",
        "            time_future = pd.date_range(start=future_start_date, periods=FORECAST_STEPS, freq=freq)\n",
        "            logging.info(f\"Periode forecast masa depan: {time_future[0].strftime('%Y-%m-%d')} hingga {time_future[-1].strftime('%Y-%m-%d')}\")\n",
        "        except Exception as future_time_err: logging.warning(f\"Gagal menentukan periode waktu masa depan: {future_time_err}. Forecast masa depan akan dilewati.\")\n",
        "\n",
        "        features_to_use = [TARGET_VARIABLE]\n",
        "        landslide_data_path = os.path.join(BASE_DIR, LANDSLIDE_SHP_FILE)\n",
        "        if os.path.exists(landslide_data_path):\n",
        "            landslide_da = process_landslide_data(landslide_data_path, LANDSLIDE_DATE_COLUMN, ds, LANDSLIDE_FEATURE_NAME)\n",
        "            if landslide_da is not None:\n",
        "                ds[LANDSLIDE_FEATURE_NAME] = landslide_da; features_to_use.append(LANDSLIDE_FEATURE_NAME)\n",
        "                logging.info(f\"Fitur longsor '{LANDSLIDE_FEATURE_NAME}' ditambahkan.\")\n",
        "            else: logging.warning(\"Gagal memproses data longsor.\")\n",
        "        else: logging.warning(f\"File data longsor tidak ditemukan di {landslide_data_path}.\")\n",
        "        try:\n",
        "            spatial_feature_da = calculate_spatial_avg_lag1(ds[TARGET_VARIABLE], SPATIAL_FEATURE_NAME)\n",
        "            ds[SPATIAL_FEATURE_NAME] = spatial_feature_da; features_to_use.append(SPATIAL_FEATURE_NAME)\n",
        "            logging.info(f\"Fitur spasial '{SPATIAL_FEATURE_NAME}' ditambahkan.\")\n",
        "        except Exception as e_spatial:\n",
        "            logging.error(f\"Gagal menghitung fitur spasial: {e_spatial}. Melanjutkan tanpa fitur spasial.\")\n",
        "            if SPATIAL_FEATURE_NAME in features_to_use: features_to_use.remove(SPATIAL_FEATURE_NAME)\n",
        "        logging.info(f\"Fitur akhir yang digunakan untuk model ML: {features_to_use}\")\n",
        "\n",
        "        run_arima_flag = True; run_lstm_flag = True; run_ann_flag = True; run_rf_flag = True\n",
        "        if n_time < N_LAG + 1 + FORECAST_STEPS and any([run_lstm_flag, run_ann_flag, run_rf_flag]):\n",
        "             logging.warning(f\"Jumlah data waktu ({n_time}) mungkin tidak cukup untuk N_LAG ({N_LAG}) + FORECAST_STEPS ({FORECAST_STEPS}) + 1 untuk melatih model ML dengan benar.\")\n",
        "        run_actual_aggregation_only = False; run_evaluation = True; run_future = True\n",
        "        parallel_processing = True; num_jobs = 10 # Sesuaikan dengan CPU Anda\n",
        "        ds_ref = ds # ds_ref adalah dataset utama yang sudah diproses (termasuk CRS)\n",
        "\n",
        "        if run_actual_aggregation_only:\n",
        "            logging.info(\"===== MEMULAI MODE: HANYA AGREGRASI DATA AKTUAL =====\")\n",
        "            target_data = ds[TARGET_VARIABLE].values; actual_test_data = target_data[test_start_index:, :, :]\n",
        "            if actual_test_data.size > 0:\n",
        "                actual_agg_outdir = os.path.join(BASE_DIR, OUTPUT_DIR_ACTUAL_AGG); os.makedirs(actual_agg_outdir, exist_ok=True)\n",
        "                logging.info(f\"Melakukan agregasi (sum) data aktual '{TARGET_VARIABLE}'...\")\n",
        "                aggregated_actual_data = np.nansum(actual_test_data, axis=0)\n",
        "                save_aggregated_geotiff(aggregated_actual_data, ds_ref, time_test, actual_agg_outdir, var_name=f\"Actual_{TARGET_VARIABLE}\", prefix=\"agg\")\n",
        "            else: logging.warning(\"Tidak ada data aktual pada periode tes untuk diagregasi.\")\n",
        "            logging.info(\"===== MODE AGREGRASI DATA AKTUAL SELESAI =====\")\n",
        "        else:\n",
        "            if run_evaluation:\n",
        "                logging.info(\"===== MEMULAI EVALUASI MODEL (Tahap 4) =====\")\n",
        "                eval_output_dir = os.path.join(BASE_DIR, OUTPUT_BASE_DIR_EVAL)\n",
        "                if run_arima_flag: run_generic_forecast_eval(forecast_arima, \"ARIMA\", ds, [TARGET_VARIABLE], TARGET_VARIABLE, N_LAG, FORECAST_STEPS, time_index, time_test, ny, nx, SAMPLE_COORDS, eval_output_dir, ds_ref, use_parallel=parallel_processing, n_jobs=num_jobs)\n",
        "                if run_lstm_flag: run_generic_forecast_eval(forecast_lstm, \"LSTM\", ds, features_to_use, TARGET_VARIABLE, N_LAG, FORECAST_STEPS, time_index, time_test, ny, nx, SAMPLE_COORDS, eval_output_dir, ds_ref, use_parallel=parallel_processing, n_jobs=num_jobs)\n",
        "                if run_ann_flag: run_generic_forecast_eval(forecast_ann, \"ANN\", ds, features_to_use, TARGET_VARIABLE, N_LAG, FORECAST_STEPS, time_index, time_test, ny, nx, SAMPLE_COORDS, eval_output_dir, ds_ref, use_parallel=parallel_processing, n_jobs=num_jobs)\n",
        "                if run_rf_flag: run_generic_forecast_eval(forecast_rf, \"RF\", ds, features_to_use, TARGET_VARIABLE, N_LAG, FORECAST_STEPS, time_index, time_test, ny, nx, SAMPLE_COORDS, eval_output_dir, ds_ref, use_parallel=parallel_processing, n_jobs=num_jobs)\n",
        "                logging.info(\"===== EVALUASI MODEL SELESAI =====\")\n",
        "            else: logging.info(\"Melewati evaluasi model.\")\n",
        "            if run_future:\n",
        "                if time_future is not None:\n",
        "                    logging.info(\"===== MEMULAI FORECAST MASA DEPAN (Tahap 4) =====\")\n",
        "                    future_output_dir = os.path.join(BASE_DIR, OUTPUT_BASE_DIR_FUTURE)\n",
        "                    if run_arima_flag: run_generic_future_forecast(forecast_arima_future, \"ARIMA\", ds, [TARGET_VARIABLE], TARGET_VARIABLE, N_LAG, FORECAST_STEPS, time_future, ny, nx, future_output_dir, ds_ref, use_parallel=parallel_processing, n_jobs=num_jobs)\n",
        "                    if run_lstm_flag: run_generic_future_forecast(forecast_lstm_future, \"LSTM\", ds, features_to_use, TARGET_VARIABLE, N_LAG, FORECAST_STEPS, time_future, ny, nx, future_output_dir, ds_ref, use_parallel=parallel_processing, n_jobs=num_jobs)\n",
        "                    if run_ann_flag: run_generic_future_forecast(forecast_ann_future, \"ANN\", ds, features_to_use, TARGET_VARIABLE, N_LAG, FORECAST_STEPS, time_future, ny, nx, future_output_dir, ds_ref, use_parallel=parallel_processing, n_jobs=num_jobs)\n",
        "                    if run_rf_flag: run_generic_future_forecast(forecast_rf_future, \"RF\", ds, features_to_use, TARGET_VARIABLE, N_LAG, FORECAST_STEPS, time_future, ny, nx, future_output_dir, ds_ref, use_parallel=parallel_processing, n_jobs=num_jobs)\n",
        "                    logging.info(\"===== FORECAST MASA DEPAN SELESAI =====\")\n",
        "                else: logging.warning(\"Melewati forecast masa depan karena gagal menentukan periode waktu masa depan.\")\n",
        "            else: logging.info(\"Melewati forecast masa depan.\")\n",
        "            if not run_evaluation and not run_future: logging.warning(\"Evaluasi dan forecast masa depan keduanya dinonaktifkan.\")\n",
        "        logging.info(\"--- Semua Proses yang Dipilih Selesai ---\")\n",
        "    except FileNotFoundError as fnf_err: logging.error(f\"❌ File Tidak Ditemukan: {fnf_err}\")\n",
        "    except ValueError as ve: logging.error(f\"❌ Error Konfigurasi/Data: {ve}\")\n",
        "    except ImportError as imp_err: logging.error(f\"❌ Error Impor Pustaka: {imp_err}.\")\n",
        "    except IOError as io_err: logging.error(f\"❌ Error IO: {io_err}\")\n",
        "    except Exception as e: logging.critical(f\"❌ Error kritis tidak terduga: {e}\", exc_info=True)\n",
        "    finally:\n",
        "        if ds is not None: ds.close(); logging.info(\"Dataset NetCDF ditutup.\")\n",
        "        try: tf.keras.backend.clear_session(); logging.info(\"Sesi backend TensorFlow dibersihkan.\")\n",
        "        except Exception: pass\n",
        "\n",
        "# --- Titik Masuk Eksekusi ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJsb7hqVrzMG",
        "outputId": "30ae030a-b9ed-480e-8ff9-ab1618674249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Wilicious\\AppData\\Local\\Temp\\ipykernel_11408\\3892933859.py:29: FutureWarning: It is recommended to use 'rio.write_crs()' instead. 'rio.set_crs()' will likelybe removed in a future release.\n",
            "  ds = ds.rio.set_crs(TARGET_CRS, inplace=True)\n",
            "C:\\Users\\Wilicious\\AppData\\Local\\Temp\\ipykernel_11408\\2083888665.py:56: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
            "  ny, nx = ds_template.dims['y'], ds_template.dims['x']\n",
            "Rasterizing Months: 100%|███████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 468.94it/s]\n",
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:    9.0s\n",
            "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    9.1s\n",
            "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    9.6s\n",
            "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    9.8s\n",
            "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed:   10.0s\n",
            "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed:   10.2s\n",
            "[Parallel(n_jobs=10)]: Done  65 tasks      | elapsed:   10.5s\n",
            "[Parallel(n_jobs=10)]: Done  78 tasks      | elapsed:   10.8s\n",
            "[Parallel(n_jobs=10)]: Done  93 tasks      | elapsed:   11.0s\n",
            "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   11.3s\n",
            "[Parallel(n_jobs=10)]: Done 125 tasks      | elapsed:   11.6s\n",
            "[Parallel(n_jobs=10)]: Done 142 tasks      | elapsed:   11.9s\n",
            "[Parallel(n_jobs=10)]: Done 161 tasks      | elapsed:   12.3s\n",
            "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:   12.6s\n",
            "[Parallel(n_jobs=10)]: Done 201 tasks      | elapsed:   13.0s\n",
            "[Parallel(n_jobs=10)]: Done 222 tasks      | elapsed:   13.4s\n",
            "[Parallel(n_jobs=10)]: Done 245 tasks      | elapsed:   13.8s\n",
            "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:   14.3s\n",
            "[Parallel(n_jobs=10)]: Done 293 tasks      | elapsed:   14.8s\n",
            "[Parallel(n_jobs=10)]: Done 318 tasks      | elapsed:   15.3s\n",
            "[Parallel(n_jobs=10)]: Done 345 tasks      | elapsed:   15.9s\n",
            "[Parallel(n_jobs=10)]: Done 372 tasks      | elapsed:   16.5s\n",
            "[Parallel(n_jobs=10)]: Done 401 tasks      | elapsed:   17.0s\n",
            "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:   17.8s\n",
            "[Parallel(n_jobs=10)]: Done 461 tasks      | elapsed:   18.5s\n",
            "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:   19.2s\n",
            "[Parallel(n_jobs=10)]: Done 525 tasks      | elapsed:   19.9s\n",
            "[Parallel(n_jobs=10)]: Done 558 tasks      | elapsed:   20.6s\n",
            "[Parallel(n_jobs=10)]: Done 593 tasks      | elapsed:   21.2s\n",
            "[Parallel(n_jobs=10)]: Done 628 tasks      | elapsed:   22.3s\n",
            "[Parallel(n_jobs=10)]: Done 665 tasks      | elapsed:   23.3s\n",
            "[Parallel(n_jobs=10)]: Done 702 tasks      | elapsed:   24.6s\n",
            "[Parallel(n_jobs=10)]: Done 741 tasks      | elapsed:   25.6s\n",
            "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:   26.4s\n",
            "[Parallel(n_jobs=10)]: Done 821 tasks      | elapsed:   27.4s\n",
            "[Parallel(n_jobs=10)]: Done 862 tasks      | elapsed:   28.4s\n",
            "[Parallel(n_jobs=10)]: Done 905 tasks      | elapsed:   29.6s\n",
            "[Parallel(n_jobs=10)]: Done 948 tasks      | elapsed:   31.0s\n",
            "[Parallel(n_jobs=10)]: Done 993 tasks      | elapsed:   32.1s\n",
            "[Parallel(n_jobs=10)]: Done 1038 tasks      | elapsed:   33.4s\n",
            "[Parallel(n_jobs=10)]: Done 1085 tasks      | elapsed:   34.5s\n",
            "[Parallel(n_jobs=10)]: Done 1132 tasks      | elapsed:   35.6s\n",
            "[Parallel(n_jobs=10)]: Done 1181 tasks      | elapsed:   36.8s\n",
            "[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed:   37.9s\n",
            "[Parallel(n_jobs=10)]: Done 1281 tasks      | elapsed:   38.9s\n",
            "[Parallel(n_jobs=10)]: Done 1332 tasks      | elapsed:   39.9s\n",
            "[Parallel(n_jobs=10)]: Done 1385 tasks      | elapsed:   41.1s\n",
            "[Parallel(n_jobs=10)]: Done 1438 tasks      | elapsed:   42.3s\n",
            "[Parallel(n_jobs=10)]: Done 1493 tasks      | elapsed:   43.8s\n",
            "[Parallel(n_jobs=10)]: Done 1548 tasks      | elapsed:   45.1s\n",
            "[Parallel(n_jobs=10)]: Done 1605 tasks      | elapsed:   46.6s\n",
            "[Parallel(n_jobs=10)]: Done 1662 tasks      | elapsed:   47.8s\n",
            "[Parallel(n_jobs=10)]: Done 1721 tasks      | elapsed:   48.9s\n",
            "[Parallel(n_jobs=10)]: Done 1780 tasks      | elapsed:   49.9s\n",
            "[Parallel(n_jobs=10)]: Done 1841 tasks      | elapsed:   51.1s\n",
            "[Parallel(n_jobs=10)]: Done 1902 tasks      | elapsed:   52.1s\n",
            "[Parallel(n_jobs=10)]: Done 1935 out of 1935 | elapsed:   52.4s finished\n",
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:   11.0s\n",
            "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:   20.7s\n",
            "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:   30.7s\n",
            "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   31.6s\n",
            "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed:   52.2s\n",
            "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=10)]: Done  65 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=10)]: Done  78 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=10)]: Done  93 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=10)]: Done 125 tasks      | elapsed:  2.2min\n",
            "C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "[Parallel(n_jobs=10)]: Done 142 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=10)]: Done 161 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=10)]: Done 201 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=10)]: Done 222 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=10)]: Done 245 tasks      | elapsed:  4.4min\n",
            "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  4.8min\n",
            "[Parallel(n_jobs=10)]: Done 293 tasks      | elapsed:  5.2min\n",
            "[Parallel(n_jobs=10)]: Done 318 tasks      | elapsed:  5.6min\n",
            "[Parallel(n_jobs=10)]: Done 345 tasks      | elapsed:  6.2min\n",
            "[Parallel(n_jobs=10)]: Done 372 tasks      | elapsed:  6.7min\n",
            "[Parallel(n_jobs=10)]: Done 401 tasks      | elapsed:  7.2min\n",
            "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:  7.7min\n",
            "[Parallel(n_jobs=10)]: Done 461 tasks      | elapsed:  8.2min\n",
            "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  8.7min\n",
            "[Parallel(n_jobs=10)]: Done 525 tasks      | elapsed:  9.4min\n",
            "[Parallel(n_jobs=10)]: Done 558 tasks      | elapsed: 10.2min\n",
            "[Parallel(n_jobs=10)]: Done 593 tasks      | elapsed: 10.7min\n",
            "[Parallel(n_jobs=10)]: Done 628 tasks      | elapsed: 11.3min\n",
            "[Parallel(n_jobs=10)]: Done 665 tasks      | elapsed: 11.9min\n",
            "[Parallel(n_jobs=10)]: Done 702 tasks      | elapsed: 12.6min\n",
            "[Parallel(n_jobs=10)]: Done 741 tasks      | elapsed: 13.6min\n",
            "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed: 14.4min\n",
            "[Parallel(n_jobs=10)]: Done 821 tasks      | elapsed: 15.1min\n",
            "[Parallel(n_jobs=10)]: Done 862 tasks      | elapsed: 15.8min\n",
            "[Parallel(n_jobs=10)]: Done 905 tasks      | elapsed: 16.6min\n",
            "[Parallel(n_jobs=10)]: Done 948 tasks      | elapsed: 17.5min\n",
            "[Parallel(n_jobs=10)]: Done 993 tasks      | elapsed: 18.3min\n",
            "[Parallel(n_jobs=10)]: Done 1038 tasks      | elapsed: 19.2min\n",
            "[Parallel(n_jobs=10)]: Done 1085 tasks      | elapsed: 19.9min\n",
            "[Parallel(n_jobs=10)]: Done 1132 tasks      | elapsed: 21.0min\n",
            "[Parallel(n_jobs=10)]: Done 1181 tasks      | elapsed: 21.9min\n",
            "[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed: 22.8min\n",
            "[Parallel(n_jobs=10)]: Done 1281 tasks      | elapsed: 23.7min\n",
            "[Parallel(n_jobs=10)]: Done 1332 tasks      | elapsed: 24.8min\n",
            "[Parallel(n_jobs=10)]: Done 1385 tasks      | elapsed: 25.7min\n",
            "[Parallel(n_jobs=10)]: Done 1438 tasks      | elapsed: 26.6min\n",
            "[Parallel(n_jobs=10)]: Done 1493 tasks      | elapsed: 27.6min\n",
            "[Parallel(n_jobs=10)]: Done 1548 tasks      | elapsed: 28.7min\n",
            "[Parallel(n_jobs=10)]: Done 1605 tasks      | elapsed: 29.6min\n",
            "[Parallel(n_jobs=10)]: Done 1662 tasks      | elapsed: 30.5min\n",
            "[Parallel(n_jobs=10)]: Done 1721 tasks      | elapsed: 31.6min\n",
            "[Parallel(n_jobs=10)]: Done 1780 tasks      | elapsed: 32.5min\n",
            "[Parallel(n_jobs=10)]: Done 1841 tasks      | elapsed: 33.5min\n",
            "[Parallel(n_jobs=10)]: Done 1902 tasks      | elapsed: 34.7min\n",
            "[Parallel(n_jobs=10)]: Done 1935 out of 1935 | elapsed: 35.2min finished\n",
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:    5.5s\n",
            "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:   11.1s\n",
            "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:   17.1s\n",
            "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   18.1s\n",
            "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed:   27.7s\n",
            "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed:   33.3s\n",
            "[Parallel(n_jobs=10)]: Done  65 tasks      | elapsed:   38.7s\n",
            "[Parallel(n_jobs=10)]: Done  78 tasks      | elapsed:   44.2s\n",
            "[Parallel(n_jobs=10)]: Done  93 tasks      | elapsed:   54.4s\n",
            "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:   60.0s\n",
            "[Parallel(n_jobs=10)]: Done 125 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=10)]: Done 142 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=10)]: Done 161 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=10)]: Done 201 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=10)]: Done 222 tasks      | elapsed:  2.1min\n",
            "C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "[Parallel(n_jobs=10)]: Done 245 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=10)]: Done 293 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=10)]: Done 318 tasks      | elapsed:  3.1min\n",
            "[Parallel(n_jobs=10)]: Done 345 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=10)]: Done 372 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=10)]: Done 401 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=10)]: Done 461 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  4.9min\n",
            "[Parallel(n_jobs=10)]: Done 525 tasks      | elapsed:  5.2min\n",
            "[Parallel(n_jobs=10)]: Done 558 tasks      | elapsed:  5.6min\n",
            "[Parallel(n_jobs=10)]: Done 593 tasks      | elapsed:  5.9min\n",
            "[Parallel(n_jobs=10)]: Done 628 tasks      | elapsed:  6.3min\n",
            "[Parallel(n_jobs=10)]: Done 665 tasks      | elapsed:  6.7min\n",
            "[Parallel(n_jobs=10)]: Done 702 tasks      | elapsed:  7.1min\n",
            "[Parallel(n_jobs=10)]: Done 741 tasks      | elapsed:  7.6min\n",
            "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  8.0min\n",
            "[Parallel(n_jobs=10)]: Done 821 tasks      | elapsed:  8.4min\n",
            "[Parallel(n_jobs=10)]: Done 862 tasks      | elapsed:  8.8min\n",
            "[Parallel(n_jobs=10)]: Done 905 tasks      | elapsed:  9.2min\n",
            "[Parallel(n_jobs=10)]: Done 948 tasks      | elapsed:  9.7min\n",
            "[Parallel(n_jobs=10)]: Done 993 tasks      | elapsed: 10.1min\n",
            "[Parallel(n_jobs=10)]: Done 1038 tasks      | elapsed: 10.5min\n",
            "[Parallel(n_jobs=10)]: Done 1085 tasks      | elapsed: 11.1min\n",
            "[Parallel(n_jobs=10)]: Done 1132 tasks      | elapsed: 11.7min\n",
            "[Parallel(n_jobs=10)]: Done 1181 tasks      | elapsed: 12.3min\n",
            "[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed: 12.7min\n",
            "[Parallel(n_jobs=10)]: Done 1281 tasks      | elapsed: 13.2min\n",
            "[Parallel(n_jobs=10)]: Done 1332 tasks      | elapsed: 13.7min\n",
            "[Parallel(n_jobs=10)]: Done 1385 tasks      | elapsed: 14.2min\n",
            "[Parallel(n_jobs=10)]: Done 1438 tasks      | elapsed: 14.7min\n",
            "[Parallel(n_jobs=10)]: Done 1493 tasks      | elapsed: 15.3min\n",
            "[Parallel(n_jobs=10)]: Done 1548 tasks      | elapsed: 16.0min\n",
            "[Parallel(n_jobs=10)]: Done 1605 tasks      | elapsed: 16.6min\n",
            "[Parallel(n_jobs=10)]: Done 1662 tasks      | elapsed: 17.1min\n",
            "[Parallel(n_jobs=10)]: Done 1721 tasks      | elapsed: 17.8min\n",
            "[Parallel(n_jobs=10)]: Done 1780 tasks      | elapsed: 18.3min\n",
            "[Parallel(n_jobs=10)]: Done 1841 tasks      | elapsed: 19.0min\n",
            "[Parallel(n_jobs=10)]: Done 1902 tasks      | elapsed: 19.7min\n",
            "[Parallel(n_jobs=10)]: Done 1935 out of 1935 | elapsed: 20.1min finished\n",
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=10)]: Done  65 tasks      | elapsed:    2.5s\n",
            "[Parallel(n_jobs=10)]: Done  78 tasks      | elapsed:    2.9s\n",
            "[Parallel(n_jobs=10)]: Done  93 tasks      | elapsed:    3.4s\n",
            "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:    3.9s\n",
            "[Parallel(n_jobs=10)]: Done 125 tasks      | elapsed:    4.5s\n",
            "[Parallel(n_jobs=10)]: Done 142 tasks      | elapsed:    5.1s\n",
            "[Parallel(n_jobs=10)]: Done 161 tasks      | elapsed:    5.7s\n",
            "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:    6.4s\n",
            "[Parallel(n_jobs=10)]: Done 201 tasks      | elapsed:    7.1s\n",
            "[Parallel(n_jobs=10)]: Done 222 tasks      | elapsed:    7.8s\n",
            "[Parallel(n_jobs=10)]: Done 245 tasks      | elapsed:    8.5s\n",
            "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:    9.1s\n",
            "[Parallel(n_jobs=10)]: Done 293 tasks      | elapsed:    9.8s\n",
            "[Parallel(n_jobs=10)]: Done 318 tasks      | elapsed:   10.4s\n",
            "[Parallel(n_jobs=10)]: Done 345 tasks      | elapsed:   11.2s\n",
            "[Parallel(n_jobs=10)]: Done 372 tasks      | elapsed:   11.9s\n",
            "[Parallel(n_jobs=10)]: Done 401 tasks      | elapsed:   12.5s\n",
            "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:   13.2s\n",
            "[Parallel(n_jobs=10)]: Done 461 tasks      | elapsed:   14.0s\n",
            "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:   14.8s\n",
            "[Parallel(n_jobs=10)]: Done 525 tasks      | elapsed:   15.5s\n",
            "[Parallel(n_jobs=10)]: Done 558 tasks      | elapsed:   16.4s\n",
            "[Parallel(n_jobs=10)]: Done 593 tasks      | elapsed:   17.3s\n",
            "[Parallel(n_jobs=10)]: Done 628 tasks      | elapsed:   18.1s\n",
            "[Parallel(n_jobs=10)]: Done 665 tasks      | elapsed:   19.0s\n",
            "[Parallel(n_jobs=10)]: Done 702 tasks      | elapsed:   19.9s\n",
            "[Parallel(n_jobs=10)]: Done 741 tasks      | elapsed:   20.9s\n",
            "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:   21.9s\n",
            "[Parallel(n_jobs=10)]: Done 821 tasks      | elapsed:   22.9s\n",
            "[Parallel(n_jobs=10)]: Done 862 tasks      | elapsed:   23.8s\n",
            "[Parallel(n_jobs=10)]: Done 905 tasks      | elapsed:   24.9s\n",
            "[Parallel(n_jobs=10)]: Done 948 tasks      | elapsed:   25.9s\n",
            "[Parallel(n_jobs=10)]: Done 993 tasks      | elapsed:   27.0s\n",
            "[Parallel(n_jobs=10)]: Done 1038 tasks      | elapsed:   28.1s\n",
            "[Parallel(n_jobs=10)]: Done 1085 tasks      | elapsed:   29.3s\n",
            "[Parallel(n_jobs=10)]: Done 1132 tasks      | elapsed:   30.5s\n",
            "[Parallel(n_jobs=10)]: Done 1181 tasks      | elapsed:   31.6s\n",
            "[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed:   32.8s\n",
            "[Parallel(n_jobs=10)]: Done 1281 tasks      | elapsed:   34.3s\n",
            "[Parallel(n_jobs=10)]: Done 1332 tasks      | elapsed:   35.8s\n",
            "[Parallel(n_jobs=10)]: Done 1385 tasks      | elapsed:   37.3s\n",
            "[Parallel(n_jobs=10)]: Done 1438 tasks      | elapsed:   38.7s\n",
            "[Parallel(n_jobs=10)]: Done 1493 tasks      | elapsed:   40.3s\n",
            "[Parallel(n_jobs=10)]: Done 1548 tasks      | elapsed:   41.8s\n",
            "[Parallel(n_jobs=10)]: Done 1605 tasks      | elapsed:   43.4s\n",
            "[Parallel(n_jobs=10)]: Done 1662 tasks      | elapsed:   44.9s\n",
            "[Parallel(n_jobs=10)]: Done 1721 tasks      | elapsed:   46.6s\n",
            "[Parallel(n_jobs=10)]: Done 1780 tasks      | elapsed:   48.2s\n",
            "[Parallel(n_jobs=10)]: Done 1841 tasks      | elapsed:   49.9s\n",
            "[Parallel(n_jobs=10)]: Done 1902 tasks      | elapsed:   51.6s\n",
            "[Parallel(n_jobs=10)]: Done 1935 out of 1935 | elapsed:   52.1s finished\n",
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=10)]: Batch computation too fast (0.19685268568610226s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=10)]: Batch computation too fast (0.15025591850280762s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=10)]: Done  54 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=10)]: Done  80 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=10)]: Batch computation too fast (0.17841458320617676s.) Setting batch_size=8.\n",
            "[Parallel(n_jobs=10)]: Done 126 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=10)]: Done 198 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=10)]: Done 314 tasks      | elapsed:    2.5s\n",
            "[Parallel(n_jobs=10)]: Batch computation too slow (2.11026363083186s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=10)]: Done 450 tasks      | elapsed:    5.3s\n",
            "[Parallel(n_jobs=10)]: Done 530 tasks      | elapsed:    6.7s\n",
            "[Parallel(n_jobs=10)]: Done 556 tasks      | elapsed:    7.5s\n",
            "[Parallel(n_jobs=10)]: Done 596 tasks      | elapsed:    7.8s\n",
            "[Parallel(n_jobs=10)]: Done 617 tasks      | elapsed:    8.2s\n",
            "[Parallel(n_jobs=10)]: Done 645 tasks      | elapsed:    8.9s\n",
            "[Parallel(n_jobs=10)]: Done 675 tasks      | elapsed:    9.4s\n",
            "[Parallel(n_jobs=10)]: Done 698 tasks      | elapsed:   10.1s\n",
            "[Parallel(n_jobs=10)]: Done 723 tasks      | elapsed:   10.6s\n",
            "[Parallel(n_jobs=10)]: Done 748 tasks      | elapsed:   11.3s\n",
            "[Parallel(n_jobs=10)]: Done 775 tasks      | elapsed:   11.7s\n",
            "C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "[Parallel(n_jobs=10)]: Done 802 tasks      | elapsed:   13.4s\n",
            "[Parallel(n_jobs=10)]: Done 831 tasks      | elapsed:   14.0s\n",
            "[Parallel(n_jobs=10)]: Done 860 tasks      | elapsed:   14.7s\n",
            "[Parallel(n_jobs=10)]: Done 891 tasks      | elapsed:   15.7s\n",
            "[Parallel(n_jobs=10)]: Done 922 tasks      | elapsed:   16.6s\n",
            "[Parallel(n_jobs=10)]: Done 955 tasks      | elapsed:   17.4s\n",
            "[Parallel(n_jobs=10)]: Done 988 tasks      | elapsed:   18.0s\n",
            "[Parallel(n_jobs=10)]: Done 1023 tasks      | elapsed:   18.8s\n",
            "[Parallel(n_jobs=10)]: Done 1058 tasks      | elapsed:   19.5s\n",
            "[Parallel(n_jobs=10)]: Batch computation too fast (0.189481868329283s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=10)]: Done 1101 tasks      | elapsed:   20.4s\n",
            "[Parallel(n_jobs=10)]: Done 1174 tasks      | elapsed:   21.4s\n",
            "[Parallel(n_jobs=10)]: Done 1252 tasks      | elapsed:   22.6s\n",
            "[Parallel(n_jobs=10)]: Done 1330 tasks      | elapsed:   23.1s\n",
            "[Parallel(n_jobs=10)]: Done 1412 tasks      | elapsed:   24.0s\n",
            "[Parallel(n_jobs=10)]: Done 1494 tasks      | elapsed:   25.1s\n",
            "[Parallel(n_jobs=10)]: Done 1580 tasks      | elapsed:   26.6s\n",
            "[Parallel(n_jobs=10)]: Done 1666 tasks      | elapsed:   27.5s\n",
            "[Parallel(n_jobs=10)]: Batch computation too fast (0.19355303147867756s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=10)]: Batch computation too fast (0.17447209358215332s.) Setting batch_size=8.\n",
            "[Parallel(n_jobs=10)]: Done 1764 tasks      | elapsed:   28.0s\n",
            "[Parallel(n_jobs=10)]: Done 1916 out of 1935 | elapsed:   28.4s remaining:    0.2s\n",
            "[Parallel(n_jobs=10)]: Done 1935 out of 1935 | elapsed:   28.5s finished\n",
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:    8.5s\n",
            "C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:   16.7s\n",
            "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:   24.9s\n",
            "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   33.6s\n",
            "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed:   44.0s\n",
            "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed:   53.3s\n",
            "[Parallel(n_jobs=10)]: Done  65 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=10)]: Done  78 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=10)]: Done  93 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=10)]: Done 125 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=10)]: Done 142 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=10)]: Done 161 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=10)]: Done 201 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=10)]: Done 222 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=10)]: Done 245 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=10)]: Done 293 tasks      | elapsed:  4.7min\n",
            "[Parallel(n_jobs=10)]: Done 318 tasks      | elapsed:  5.1min\n",
            "[Parallel(n_jobs=10)]: Done 345 tasks      | elapsed:  5.5min\n",
            "[Parallel(n_jobs=10)]: Done 372 tasks      | elapsed:  6.0min\n",
            "[Parallel(n_jobs=10)]: Done 401 tasks      | elapsed:  6.4min\n",
            "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:  6.9min\n",
            "[Parallel(n_jobs=10)]: Done 461 tasks      | elapsed:  7.4min\n",
            "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  7.9min\n",
            "[Parallel(n_jobs=10)]: Done 525 tasks      | elapsed:  8.4min\n",
            "[Parallel(n_jobs=10)]: Done 558 tasks      | elapsed:  8.9min\n",
            "[Parallel(n_jobs=10)]: Done 593 tasks      | elapsed:  9.5min\n",
            "[Parallel(n_jobs=10)]: Done 628 tasks      | elapsed: 10.1min\n",
            "[Parallel(n_jobs=10)]: Done 665 tasks      | elapsed: 10.7min\n",
            "[Parallel(n_jobs=10)]: Done 702 tasks      | elapsed: 11.2min\n",
            "[Parallel(n_jobs=10)]: Done 741 tasks      | elapsed: 11.8min\n",
            "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed: 12.5min\n",
            "[Parallel(n_jobs=10)]: Done 821 tasks      | elapsed: 13.1min\n",
            "[Parallel(n_jobs=10)]: Done 862 tasks      | elapsed: 13.8min\n",
            "[Parallel(n_jobs=10)]: Done 905 tasks      | elapsed: 14.4min\n",
            "[Parallel(n_jobs=10)]: Done 948 tasks      | elapsed: 15.1min\n",
            "[Parallel(n_jobs=10)]: Done 993 tasks      | elapsed: 15.8min\n",
            "[Parallel(n_jobs=10)]: Done 1038 tasks      | elapsed: 16.6min\n",
            "[Parallel(n_jobs=10)]: Done 1085 tasks      | elapsed: 17.3min\n",
            "[Parallel(n_jobs=10)]: Done 1132 tasks      | elapsed: 18.0min\n",
            "[Parallel(n_jobs=10)]: Done 1181 tasks      | elapsed: 18.8min\n",
            "[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed: 19.7min\n",
            "[Parallel(n_jobs=10)]: Done 1281 tasks      | elapsed: 20.4min\n",
            "[Parallel(n_jobs=10)]: Done 1332 tasks      | elapsed: 21.2min\n",
            "[Parallel(n_jobs=10)]: Done 1385 tasks      | elapsed: 22.1min\n",
            "[Parallel(n_jobs=10)]: Done 1438 tasks      | elapsed: 22.9min\n",
            "[Parallel(n_jobs=10)]: Done 1493 tasks      | elapsed: 23.7min\n",
            "[Parallel(n_jobs=10)]: Done 1548 tasks      | elapsed: 24.6min\n",
            "[Parallel(n_jobs=10)]: Done 1605 tasks      | elapsed: 25.5min\n",
            "[Parallel(n_jobs=10)]: Done 1662 tasks      | elapsed: 26.4min\n",
            "[Parallel(n_jobs=10)]: Done 1721 tasks      | elapsed: 27.3min\n",
            "[Parallel(n_jobs=10)]: Done 1780 tasks      | elapsed: 28.3min\n",
            "[Parallel(n_jobs=10)]: Done 1841 tasks      | elapsed: 29.3min\n",
            "[Parallel(n_jobs=10)]: Done 1902 tasks      | elapsed: 30.2min\n",
            "[Parallel(n_jobs=10)]: Done 1935 out of 1935 | elapsed: 30.7min finished\n",
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:    5.2s\n",
            "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:   10.1s\n",
            "C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:   15.1s\n",
            "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:   20.0s\n",
            "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed:   25.4s\n",
            "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed:   31.1s\n",
            "[Parallel(n_jobs=10)]: Done  65 tasks      | elapsed:   41.0s\n",
            "[Parallel(n_jobs=10)]: Done  78 tasks      | elapsed:   50.0s\n",
            "[Parallel(n_jobs=10)]: Done  93 tasks      | elapsed:   56.5s\n",
            "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=10)]: Done 125 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=10)]: Done 142 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=10)]: Done 161 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=10)]: Done 201 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=10)]: Done 222 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=10)]: Done 245 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=10)]: Done 293 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=10)]: Done 318 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=10)]: Done 345 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=10)]: Done 372 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=10)]: Done 401 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=10)]: Done 461 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=10)]: Done 525 tasks      | elapsed:  4.9min\n",
            "[Parallel(n_jobs=10)]: Done 558 tasks      | elapsed:  5.2min\n",
            "[Parallel(n_jobs=10)]: Done 593 tasks      | elapsed:  5.6min\n",
            "[Parallel(n_jobs=10)]: Done 628 tasks      | elapsed:  5.9min\n",
            "[Parallel(n_jobs=10)]: Done 665 tasks      | elapsed:  6.2min\n",
            "[Parallel(n_jobs=10)]: Done 702 tasks      | elapsed:  6.5min\n",
            "[Parallel(n_jobs=10)]: Done 741 tasks      | elapsed:  6.9min\n",
            "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:  7.2min\n",
            "[Parallel(n_jobs=10)]: Done 821 tasks      | elapsed:  7.6min\n",
            "[Parallel(n_jobs=10)]: Done 862 tasks      | elapsed:  8.0min\n",
            "[Parallel(n_jobs=10)]: Done 905 tasks      | elapsed:  8.5min\n",
            "[Parallel(n_jobs=10)]: Done 948 tasks      | elapsed:  8.8min\n",
            "[Parallel(n_jobs=10)]: Done 993 tasks      | elapsed:  9.3min\n",
            "[Parallel(n_jobs=10)]: Done 1038 tasks      | elapsed:  9.7min\n",
            "[Parallel(n_jobs=10)]: Done 1085 tasks      | elapsed: 10.1min\n",
            "[Parallel(n_jobs=10)]: Done 1132 tasks      | elapsed: 10.5min\n",
            "[Parallel(n_jobs=10)]: Done 1181 tasks      | elapsed: 11.0min\n",
            "[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed: 11.4min\n",
            "[Parallel(n_jobs=10)]: Done 1281 tasks      | elapsed: 11.9min\n",
            "[Parallel(n_jobs=10)]: Done 1332 tasks      | elapsed: 12.4min\n",
            "[Parallel(n_jobs=10)]: Done 1385 tasks      | elapsed: 12.9min\n",
            "[Parallel(n_jobs=10)]: Done 1438 tasks      | elapsed: 13.4min\n",
            "[Parallel(n_jobs=10)]: Done 1493 tasks      | elapsed: 13.9min\n",
            "[Parallel(n_jobs=10)]: Done 1548 tasks      | elapsed: 14.4min\n",
            "[Parallel(n_jobs=10)]: Done 1605 tasks      | elapsed: 14.9min\n",
            "[Parallel(n_jobs=10)]: Done 1662 tasks      | elapsed: 15.4min\n",
            "[Parallel(n_jobs=10)]: Done 1721 tasks      | elapsed: 16.0min\n",
            "[Parallel(n_jobs=10)]: Done 1780 tasks      | elapsed: 16.6min\n",
            "[Parallel(n_jobs=10)]: Done 1841 tasks      | elapsed: 17.2min\n",
            "[Parallel(n_jobs=10)]: Done 1902 tasks      | elapsed: 17.7min\n",
            "[Parallel(n_jobs=10)]: Done 1935 out of 1935 | elapsed: 18.0min finished\n",
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=10)]: Done  65 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=10)]: Done  78 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=10)]: Done  93 tasks      | elapsed:    1.9s\n",
            "[Parallel(n_jobs=10)]: Done 108 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=10)]: Done 125 tasks      | elapsed:    2.5s\n",
            "[Parallel(n_jobs=10)]: Done 142 tasks      | elapsed:    2.8s\n",
            "[Parallel(n_jobs=10)]: Done 161 tasks      | elapsed:    3.2s\n",
            "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:    3.5s\n",
            "[Parallel(n_jobs=10)]: Done 201 tasks      | elapsed:    3.9s\n",
            "[Parallel(n_jobs=10)]: Done 222 tasks      | elapsed:    4.3s\n",
            "[Parallel(n_jobs=10)]: Done 245 tasks      | elapsed:    4.7s\n",
            "[Parallel(n_jobs=10)]: Done 268 tasks      | elapsed:    5.2s\n",
            "[Parallel(n_jobs=10)]: Done 293 tasks      | elapsed:    5.7s\n",
            "[Parallel(n_jobs=10)]: Done 318 tasks      | elapsed:    6.1s\n",
            "[Parallel(n_jobs=10)]: Done 345 tasks      | elapsed:    6.7s\n",
            "[Parallel(n_jobs=10)]: Done 372 tasks      | elapsed:    7.2s\n",
            "[Parallel(n_jobs=10)]: Done 401 tasks      | elapsed:    7.7s\n",
            "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:    8.3s\n",
            "[Parallel(n_jobs=10)]: Done 461 tasks      | elapsed:    8.9s\n",
            "[Parallel(n_jobs=10)]: Done 492 tasks      | elapsed:    9.6s\n",
            "[Parallel(n_jobs=10)]: Done 525 tasks      | elapsed:   10.2s\n",
            "[Parallel(n_jobs=10)]: Done 558 tasks      | elapsed:   10.8s\n",
            "[Parallel(n_jobs=10)]: Done 593 tasks      | elapsed:   11.5s\n",
            "[Parallel(n_jobs=10)]: Done 628 tasks      | elapsed:   12.2s\n",
            "[Parallel(n_jobs=10)]: Done 665 tasks      | elapsed:   12.9s\n",
            "[Parallel(n_jobs=10)]: Done 702 tasks      | elapsed:   13.6s\n",
            "[Parallel(n_jobs=10)]: Done 741 tasks      | elapsed:   14.4s\n",
            "[Parallel(n_jobs=10)]: Done 780 tasks      | elapsed:   15.1s\n",
            "[Parallel(n_jobs=10)]: Done 821 tasks      | elapsed:   15.9s\n",
            "[Parallel(n_jobs=10)]: Done 862 tasks      | elapsed:   16.7s\n",
            "[Parallel(n_jobs=10)]: Done 905 tasks      | elapsed:   17.5s\n",
            "[Parallel(n_jobs=10)]: Done 948 tasks      | elapsed:   18.3s\n",
            "[Parallel(n_jobs=10)]: Done 993 tasks      | elapsed:   19.2s\n",
            "[Parallel(n_jobs=10)]: Done 1038 tasks      | elapsed:   20.1s\n",
            "[Parallel(n_jobs=10)]: Done 1085 tasks      | elapsed:   21.0s\n",
            "[Parallel(n_jobs=10)]: Done 1132 tasks      | elapsed:   21.8s\n",
            "[Parallel(n_jobs=10)]: Done 1181 tasks      | elapsed:   22.8s\n",
            "[Parallel(n_jobs=10)]: Done 1230 tasks      | elapsed:   23.8s\n",
            "[Parallel(n_jobs=10)]: Done 1281 tasks      | elapsed:   24.7s\n",
            "[Parallel(n_jobs=10)]: Done 1332 tasks      | elapsed:   25.7s\n",
            "[Parallel(n_jobs=10)]: Done 1385 tasks      | elapsed:   26.6s\n",
            "[Parallel(n_jobs=10)]: Done 1438 tasks      | elapsed:   27.7s\n",
            "[Parallel(n_jobs=10)]: Done 1493 tasks      | elapsed:   28.7s\n",
            "[Parallel(n_jobs=10)]: Done 1548 tasks      | elapsed:   29.8s\n",
            "[Parallel(n_jobs=10)]: Done 1605 tasks      | elapsed:   30.8s\n",
            "[Parallel(n_jobs=10)]: Done 1662 tasks      | elapsed:   31.9s\n",
            "[Parallel(n_jobs=10)]: Done 1721 tasks      | elapsed:   33.0s\n",
            "[Parallel(n_jobs=10)]: Done 1780 tasks      | elapsed:   34.1s\n",
            "[Parallel(n_jobs=10)]: Done 1841 tasks      | elapsed:   35.3s\n",
            "[Parallel(n_jobs=10)]: Done 1902 tasks      | elapsed:   36.4s\n",
            "[Parallel(n_jobs=10)]: Done 1935 out of 1935 | elapsed:   37.0s finished\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}