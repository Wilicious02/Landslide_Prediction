{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#conda install -c conda-forge \\\n",
        "  #num#py pandas xarray geopandas rasterio matplotlib rioxarray tqdm \\\n",
        "  #scikit-learn joblib statsmodels scipy imageio optuna pmdarima"
      ],
      "metadata": {
        "id": "F2_Us_BduMGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#python -c \"import numpy, pandas, xarray, geopandas, rasterio, matplotlib, rioxarray, tqdm, sklearn, joblib, statsmodels, scipy, imageio, optuna, pmdarima, tensorflow; print('All OK!')\""
      ],
      "metadata": {
        "id": "HiFawvD4w3fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Landslide Prediction Spatio-Temporal Pipeline (v5, improved)\n",
        "------------------------------------------------------------\n",
        "Pipeline otomatis prediksi spasio-temporal kejadian longsor grid dengan ARIMA, LSTM, RF, ANN:\n",
        "- Input: NetCDF grid, shapefile titik longsor (opsional)\n",
        "- Feature engineering: lag, spasial rata-rata, sin/cos waktu, dst\n",
        "- Hyperparameter tuning otomatis (Optuna)\n",
        "- Output: GeoTIFF, NetCDF, PNG, GIF, NPZ, log, residual diagnostics, evaluasi grid lengkap\n",
        "- Logging detail & validasi data\n",
        "- File output otomatis pakai hash param model\n",
        "- Support paralelisme CPU, robust handling data NaN/missing\n",
        "- Penambahan: ANN aktif, residual train grid, colormap configurable, file output pakai hash param, units/desc custom, validasi NaN, improved exception handling, docstring global.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "import rioxarray\n",
        "import warnings\n",
        "import logging\n",
        "from rasterio import features\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from joblib import Parallel, delayed\n",
        "from statsmodels.tsa.stattools import adfuller, acf\n",
        "import pmdarima as pm\n",
        "from scipy.ndimage import convolve\n",
        "import optuna\n",
        "import imageio\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "Day8RcMHt8rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings for clean output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "plt.ioff()\n",
        "\n",
        "# ========================= CONFIG =========================\n",
        "CONFIG = {\n",
        "    \"BASE_DIR\": \"D:\\\\DataPenelitian\\\\Longsor\",\n",
        "    \"NC_FILE\": \"nc_20250426_1M.nc\",\n",
        "    \"LANDSLIDE_SHP_FILE\": \"data_fix/TitikLongsor_Magelang_2025.shp\",\n",
        "    \"LANDSLIDE_DATE_COLUMN\": \"Date\",\n",
        "    \"TARGET_VARIABLE\": \"COUNT\",\n",
        "    \"LANDSLIDE_FEATURE_NAME\": \"LANDSLIDE_COUNT\",\n",
        "    \"SPATIAL_FEATURE_NAME\": \"COUNT_SPATIAL_AVG_LAG1\",\n",
        "    \"TARGET_CRS\": \"EPSG:32749\",\n",
        "    \"SAMPLE_COORDS\": [(0, 1), (35, 14), (0, 0), (27, 6), (35, 11)],\n",
        "    \"N_LAG\": None,\n",
        "    \"FORECAST_STEPS\": None,\n",
        "    \"VALIDATION_GAP\": None,\n",
        "    \"HYPERPARAM_TUNE\": True,\n",
        "    \"OPTUNA_TRIALS\": 15,\n",
        "    \"N_JOBS_PARALLEL\": -1,\n",
        "    \"GIF_FPS\": 2,\n",
        "    \"SEED\": 42,\n",
        "    \"COLORMAP\": \"viridis\",\n",
        "    \"COLORMAP_R2\": \"viridis_r\"\n",
        "}\n",
        "MODEL_LIST = [\"ann\"] #, \"lstm\", \"rf\", \"ann\"]  # ANN sekarang aktif\n",
        "OUTPUT_FOLDERS = {\n",
        "    \"geotiff\": \"output_geotiff\",\n",
        "    \"netcdf\": \"output_netcdf\",\n",
        "    \"png\": \"output_png\",\n",
        "    \"gif\": \"output_gif\",\n",
        "    \"logs\": \"logs\",\n",
        "    \"intermediate\": \"output_intermediate\"\n",
        "}"
      ],
      "metadata": {
        "id": "_b4RP8ZEt_XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========== Setup Seed ==========\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(CONFIG[\"SEED\"])\n",
        "random.seed(CONFIG[\"SEED\"])\n",
        "np.random.seed(CONFIG[\"SEED\"])\n",
        "tf.random.set_seed(CONFIG[\"SEED\"])\n",
        "\n",
        "def hash_params(params: dict) -> str:\n",
        "    if not params: return \"default\"\n",
        "    s = str(sorted(params.items()))\n",
        "    return hashlib.md5(s.encode()).hexdigest()[:8]\n",
        "\n",
        "def setup_folders(base_dir, folders):\n",
        "    out_dirs = {}\n",
        "    for k, v in folders.items():\n",
        "        d = os.path.join(base_dir, v)\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "        out_dirs[k] = d\n",
        "    return out_dirs\n",
        "OUTDIRS = setup_folders(CONFIG[\"BASE_DIR\"], OUTPUT_FOLDERS)\n",
        "\n",
        "def setup_logging(log_dir):\n",
        "    log_path = os.path.join(log_dir, \"run.log\")\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s %(levelname)s:%(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_path, mode='w'),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "    logging.info(\"Logging started. All logs saved to %s\", log_path)\n",
        "setup_logging(OUTDIRS[\"logs\"])\n",
        "\n",
        "def robust_impute(ts):\n",
        "    s = pd.Series(ts)\n",
        "    s = s.interpolate(method='linear', limit_direction='both').fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
        "    return s.values\n",
        "\n",
        "def fallback_crs(ds, config):\n",
        "    try:\n",
        "        if not hasattr(ds, 'rio') or ds.rio.crs is None:\n",
        "            ds = ds.rio.write_crs(config[\"TARGET_CRS\"])\n",
        "            logging.info(f\"Fallback CRS applied: {config['TARGET_CRS']}\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Fallback CRS failed: {e}\")\n",
        "    return ds\n",
        "\n",
        "def calculate_spatial_avg_lag1(data_array, feature_name=\"SPATIAL_AVG_LAG1\"):\n",
        "    kernel = np.array([[1,1,1],[1,0,1],[1,1,1]], dtype=np.float32)\n",
        "    values = data_array.values.astype(np.float32)\n",
        "    spatial_avg_lag1 = np.full_like(values, np.nan)\n",
        "    for t in range(1, values.shape[0]):\n",
        "        data_t_minus_1 = values[t-1,:,:]\n",
        "        nan_mask_t_minus_1 = np.isnan(data_t_minus_1)\n",
        "        neighbor_counts = convolve(~nan_mask_t_minus_1, kernel, mode='constant', cval=0.0)\n",
        "        data_t_minus_1_nan_as_zero = np.nan_to_num(data_t_minus_1, nan=0.0)\n",
        "        neighbor_sum = convolve(data_t_minus_1_nan_as_zero, kernel, mode='constant', cval=0.0)\n",
        "        valid_neighbors_mask = neighbor_counts > 0\n",
        "        avg_values = np.full_like(neighbor_sum, np.nan)\n",
        "        avg_values[valid_neighbors_mask] = neighbor_sum[valid_neighbors_mask] / neighbor_counts[valid_neighbors_mask]\n",
        "        spatial_avg_lag1[t,:,:] = avg_values\n",
        "    spatial_da = xr.DataArray(\n",
        "        spatial_avg_lag1, coords=data_array.coords, dims=data_array.dims, name=feature_name,\n",
        "        attrs={'long_name': f'Average of 8 spatial neighbors at lag 1 for {data_array.name}',\n",
        "               'units': data_array.attrs.get('units', 'unknown')}\n",
        "    )\n",
        "    logging.info(f\"Calculated spatial feature: {feature_name}\")\n",
        "    return spatial_da\n",
        "\n",
        "def add_time_features(ds, time_var='time'):\n",
        "    try:\n",
        "        months = pd.to_datetime(ds[time_var].values).month\n",
        "        ds['month'] = (ds[time_var].dims, months)\n",
        "        ds['sin_month'] = (ds[time_var].dims, np.sin(2 * np.pi * months / 12))\n",
        "        ds['cos_month'] = (ds[time_var].dims, np.cos(2 * np.pi * months / 12))\n",
        "        logging.info(\"Added time features: month, sin_month, cos_month.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Could not add time features: {e}\")\n",
        "    return ds\n",
        "\n",
        "def detect_agg_label(ds):\n",
        "    time = pd.to_datetime(ds['time'].values)\n",
        "    if len(time) < 2:\n",
        "        return \"1m\"\n",
        "    delta = (time[1] - time[0]).days\n",
        "    if 25 <= delta < 35:\n",
        "        return \"1m\"\n",
        "    elif 55 <= delta < 65:\n",
        "        return \"2m\"\n",
        "    elif 85 <= delta < 95:\n",
        "        return \"3m\"\n",
        "    else:\n",
        "        return f\"{delta}d\"\n",
        "\n",
        "def forecast_filename(model, agg_label, start_date, end_date, step_ext):\n",
        "    return f\"{model}_forecast_{agg_label}_{start_date}_to_{end_date}_{step_ext}\"\n",
        "\n",
        "def evalmap_filename(model, agg_label, metric, ext):\n",
        "    return f\"{model}_eval_{metric}_{agg_label}.{ext}\"\n",
        "\n",
        "def difforder_filename(model, agg_label, ext):\n",
        "    return f\"{model}_orderdiff_{agg_label}.{ext}\"\n",
        "\n",
        "def residualdiag_filename(model, agg_label, ext):\n",
        "    return f\"{model}_residual_diag_{agg_label}.{ext}\"\n",
        "\n",
        "def process_landslide_data(shp_path, date_column, ds_template, feature_name=\"LANDSLIDE_COUNT\"):\n",
        "    try:\n",
        "        landslide_gdf = gpd.read_file(shp_path)\n",
        "        logging.info(f\"Loaded {len(landslide_gdf)} landslide points from {os.path.basename(shp_path)}.\")\n",
        "        if date_column not in landslide_gdf.columns:\n",
        "            logging.error(f\"Date column '{date_column}' not found in Shapefile.\"); return None\n",
        "        landslide_gdf[date_column] = pd.to_datetime(landslide_gdf[date_column], errors='coerce')\n",
        "        original_count = len(landslide_gdf)\n",
        "        landslide_gdf = landslide_gdf.dropna(subset=[date_column])\n",
        "        if len(landslide_gdf) < original_count:\n",
        "            logging.warning(f\"Removed {original_count - len(landslide_gdf)} points with invalid dates.\")\n",
        "        if landslide_gdf.empty:\n",
        "            logging.warning(\"No valid landslide points. Returning zero array.\")\n",
        "            return xr.DataArray(\n",
        "                np.zeros((len(ds_template['time']), ds_template.dims['y'], ds_template.dims['x']), dtype=np.int32),\n",
        "                coords=ds_template.coords, dims=['time', 'y', 'x'], name=feature_name\n",
        "            )\n",
        "        ds_template_crs = ds_template.rio.crs if hasattr(ds_template, 'rio') and ds_template.rio.crs else None\n",
        "        if ds_template_crs and landslide_gdf.crs and landslide_gdf.crs != ds_template_crs:\n",
        "            logging.info(f\"Reprojecting landslide data from {landslide_gdf.crs} to {ds_template_crs}...\")\n",
        "            landslide_gdf = landslide_gdf.to_crs(ds_template_crs)\n",
        "        ny, nx = ds_template.dims['y'], ds_template.dims['x']\n",
        "        grid_shape = (ny, nx)\n",
        "        transform = ds_template.rio.transform()\n",
        "        time_coords = ds_template['time']\n",
        "        landslide_grid_monthly = np.zeros((len(time_coords), ny, nx), dtype=np.int32)\n",
        "        landslide_gdf['YearMonth'] = landslide_gdf[date_column].dt.to_period('M')\n",
        "        for t_idx, timestamp in enumerate(tqdm(time_coords.values, desc=\"Rasterizing Landslides\")):\n",
        "            current_month = pd.Timestamp(timestamp).to_period('M')\n",
        "            monthly_points = landslide_gdf[landslide_gdf['YearMonth'] == current_month]\n",
        "            if not monthly_points.empty:\n",
        "                shapes = [(geom, 1) for geom in monthly_points.geometry]\n",
        "                try:\n",
        "                    monthly_raster = features.rasterize(\n",
        "                        shapes=shapes, out_shape=grid_shape, transform=transform,\n",
        "                        fill=0, merge_alg=rasterio.enums.MergeAlg.add, dtype=np.int32)\n",
        "                    landslide_grid_monthly[t_idx, :, :] = monthly_raster\n",
        "                except Exception as raster_err:\n",
        "                    logging.warning(f\"Rasterize failed for month {current_month}: {raster_err}\")\n",
        "        landslide_da = xr.DataArray(\n",
        "            landslide_grid_monthly, coords={'time': time_coords, 'y': ds_template['y'], 'x': ds_template['x']},\n",
        "            dims=['time', 'y', 'x'], name=feature_name,\n",
        "            attrs={'long_name': 'Monthly landslide event count', 'units': 'count'}\n",
        "        )\n",
        "        logging.info(f\"Landslide data processing complete. Feature: {feature_name}\")\n",
        "        return landslide_da\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in process_landslide_data: {e}\")\n",
        "        return None\n",
        "\n",
        "def set_window_params(ds, config):\n",
        "    n_time = ds[config[\"TARGET_VARIABLE\"]].shape[0]\n",
        "    forecast_steps = config.get(\"FORECAST_STEPS\")\n",
        "    if forecast_steps is None:\n",
        "        forecast_steps = max(1, int(0.1 * n_time))\n",
        "    forecast_steps = min(forecast_steps, max(1, int(0.5 * n_time)))\n",
        "    n_lag = config.get(\"N_LAG\")\n",
        "    if n_lag is None:\n",
        "        n_lag = max(1, int(0.25 * n_time))\n",
        "    n_lag = min(n_lag, max(1, int(1/3 * n_time)))\n",
        "    validation_gap = config.get(\"VALIDATION_GAP\")\n",
        "    if validation_gap is None:\n",
        "        validation_gap = max(0, int(0.05 * n_time))\n",
        "    validation_gap = min(validation_gap, int(0.25 * n_time))\n",
        "    if n_time - forecast_steps - validation_gap < n_lag:\n",
        "        logging.warning(\"Adjusting n_lag due to insufficient data for train/validation split.\")\n",
        "        n_lag = max(1, n_time - forecast_steps - validation_gap -1)\n",
        "    logging.info(f\"Window params set: n_lag={n_lag}, forecast_steps={forecast_steps}, validation_gap={validation_gap}\")\n",
        "    return n_lag, forecast_steps, validation_gap, n_time\n",
        "\n",
        "def build_lstm(input_shape, hyperparams):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.LSTM(hyperparams.get('lstm_units', 32),\n",
        "                             activation=hyperparams.get('lstm_activation', 'relu'),\n",
        "                             input_shape=input_shape),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparams.get('learning_rate', 0.01)),\n",
        "        loss='mse'\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def build_rf(hyperparams):\n",
        "    return RandomForestRegressor(\n",
        "        n_estimators=hyperparams.get('n_estimators', 100),\n",
        "        random_state=hyperparams.get('rf_random_state', CONFIG[\"SEED\"]),\n",
        "        max_depth=hyperparams.get('max_depth', None),\n",
        "        min_samples_split=hyperparams.get('min_samples_split', 2),\n",
        "        min_samples_leaf=hyperparams.get('min_samples_leaf', 1),\n",
        "        n_jobs=1\n",
        "    )\n",
        "\n",
        "def build_ann(input_shape_flat, hyperparams):\n",
        "    layers_units = hyperparams.get('ann_layers_units', [64, 32])\n",
        "    layers = [\n",
        "        tf.keras.layers.Dense(layers_units[0],\n",
        "                              activation=hyperparams.get('ann_activation', 'relu'),\n",
        "                              input_shape=(input_shape_flat,))\n",
        "    ]\n",
        "    for units in layers_units[1:]:\n",
        "        layers.append(tf.keras.layers.Dense(units, activation=hyperparams.get('ann_activation', 'relu')))\n",
        "    layers.append(tf.keras.layers.Dense(1))\n",
        "    model = tf.keras.Sequential(layers)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparams.get('learning_rate', 0.01)),\n",
        "        loss='mse'\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def forecast_arima(ts_train, forecast_steps, max_d=2, return_order=False, return_residual=False):\n",
        "    y_pred = np.full(forecast_steps, np.nan)\n",
        "    d_out = np.nan\n",
        "    residuals = np.full_like(ts_train, np.nan)\n",
        "    try:\n",
        "        ts_train_valid = robust_impute(ts_train)\n",
        "        if len(ts_train_valid) < 10:\n",
        "            return (y_pred, d_out, residuals) if return_residual else ((y_pred, d_out) if return_order else y_pred)\n",
        "        auto_model = pm.auto_arima(ts_train_valid,\n",
        "                                   start_p=1, start_q=1, max_p=3, max_q=3,\n",
        "                                   d=None, max_d=max_d,\n",
        "                                   seasonal=False,\n",
        "                                   stepwise=True, suppress_warnings=True,\n",
        "                                   error_action='ignore', trace=False)\n",
        "        y_pred = auto_model.predict(n_periods=forecast_steps)\n",
        "        y_pred[~np.isfinite(y_pred)] = np.nan\n",
        "        y_pred = np.maximum(0, y_pred)\n",
        "        d_out = auto_model.order[1]\n",
        "        residuals = ts_train_valid - auto_model.predict_in_sample()\n",
        "    except Exception as e:\n",
        "        logging.debug(f\"ARIMA forecast failed for a pixel: {e}\")\n",
        "    if return_residual:\n",
        "        return y_pred, d_out, residuals\n",
        "    if return_order:\n",
        "        return y_pred, d_out\n",
        "    return y_pred\n",
        "\n",
        "def forecast_ml(ts_input_features, target_column_index, n_lag, forecast_steps, model_type, hyperparams, return_residual=False):\n",
        "    if ts_input_features.ndim != 2 or ts_input_features.shape[0] < n_lag + 1:\n",
        "        logging.debug(f\"Not enough data for ML forecast: shape {ts_input_features.shape}, n_lag {n_lag}\")\n",
        "        if return_residual:\n",
        "            return np.full(forecast_steps, np.nan), np.full(ts_input_features.shape[0], np.nan)\n",
        "        return np.full(forecast_steps, np.nan)\n",
        "    n_features = ts_input_features.shape[1]\n",
        "    ts_imputed_features = np.array([robust_impute(ts_input_features[:, i]) for i in range(n_features)]).T\n",
        "    scaler_features = MinMaxScaler()\n",
        "    ts_scaled_features = scaler_features.fit_transform(ts_imputed_features)\n",
        "    scaler_target = MinMaxScaler()\n",
        "    scaler_target.fit(ts_imputed_features[:, target_column_index].reshape(-1, 1))\n",
        "    X_list, y_list = [], []\n",
        "    for i in range(len(ts_scaled_features) - n_lag):\n",
        "        X_list.append(ts_scaled_features[i : i + n_lag, :])\n",
        "        y_list.append(ts_scaled_features[i + n_lag, target_column_index])\n",
        "    if not X_list or len(X_list) < 2:\n",
        "        if return_residual:\n",
        "            return np.full(forecast_steps, np.nan), np.full(ts_input_features.shape[0], np.nan)\n",
        "        return np.full(forecast_steps, np.nan)\n",
        "    X_train = np.array(X_list)\n",
        "    y_train = np.array(y_list)\n",
        "    tf.keras.backend.clear_session()\n",
        "    residuals = np.full(ts_input_features.shape[0], np.nan)\n",
        "    if model_type == 'lstm':\n",
        "        model = build_lstm((n_lag, n_features), hyperparams)\n",
        "        model.fit(X_train, y_train, epochs=hyperparams.get('epochs', 50), batch_size=hyperparams.get('batch_size', 16), verbose=0)\n",
        "        y_train_pred = model.predict(X_train, verbose=0).flatten()\n",
        "        residuals[-len(y_train_pred):] = scaler_target.inverse_transform(y_train.reshape(-1, 1)).flatten() - scaler_target.inverse_transform(y_train_pred.reshape(-1, 1)).flatten()\n",
        "    elif model_type == 'ann':\n",
        "        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "        model = build_ann(X_train_flat.shape[1], hyperparams)\n",
        "        model.fit(X_train_flat, y_train, epochs=hyperparams.get('epochs', 50), batch_size=hyperparams.get('batch_size', 16), verbose=0)\n",
        "        y_train_pred = model.predict(X_train_flat, verbose=0).flatten()\n",
        "        residuals[-len(y_train_pred):] = scaler_target.inverse_transform(y_train.reshape(-1, 1)).flatten() - scaler_target.inverse_transform(y_train_pred.reshape(-1, 1)).flatten()\n",
        "    elif model_type == 'rf':\n",
        "        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "        model = build_rf(hyperparams)\n",
        "        model.fit(X_train_flat, y_train)\n",
        "        y_train_pred = model.predict(X_train_flat)\n",
        "        residuals[-len(y_train_pred):] = scaler_target.inverse_transform(y_train.reshape(-1, 1)).flatten() - scaler_target.inverse_transform(y_train_pred.reshape(-1, 1)).flatten()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
        "    predictions_scaled = []\n",
        "    current_sequence_scaled = ts_scaled_features[-n_lag:, :]\n",
        "    for _ in range(forecast_steps):\n",
        "        if model_type == 'lstm':\n",
        "            input_for_pred = current_sequence_scaled.reshape(1, n_lag, n_features)\n",
        "            pred_scaled = model.predict(input_for_pred, verbose=0)[0, 0]\n",
        "        elif model_type in ['ann', 'rf']:\n",
        "            input_for_pred = current_sequence_scaled.flatten().reshape(1, -1)\n",
        "            if model_type == 'ann':\n",
        "                pred_scaled = model.predict(input_for_pred, verbose=0)[0, 0]\n",
        "            else:\n",
        "                pred_scaled = model.predict(input_for_pred)[0]\n",
        "        predictions_scaled.append(pred_scaled)\n",
        "        new_row_scaled = current_sequence_scaled[-1, :].copy()\n",
        "        new_row_scaled[target_column_index] = pred_scaled\n",
        "        current_sequence_scaled = np.roll(current_sequence_scaled, -1, axis=0)\n",
        "        current_sequence_scaled[-1, :] = new_row_scaled\n",
        "    predictions_unscaled = scaler_target.inverse_transform(np.array(predictions_scaled).reshape(-1, 1)).flatten()\n",
        "    predictions_unscaled = np.maximum(0, predictions_unscaled)\n",
        "    if return_residual:\n",
        "        return predictions_unscaled, residuals\n",
        "    return predictions_unscaled\n"
      ],
      "metadata": {
        "id": "lypUeXkMttfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ebba169-126a-49ef-d2b2-0cc8eb840119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-17 13:45:14,117 INFO:Logging started. All logs saved to D:\\DataPenelitian\\Longsor\\logs\\run.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Optuna Hyperparameter Tuning ==========\n",
        "def optuna_objective(trial, ts_features_pixel, target_col_idx, n_lag, forecast_steps, model_type, validation_gap_tune):\n",
        "    \"\"\"\n",
        "    Fungsi objektif untuk Optuna, bertujuan meminimalkan RMSE pada set validasi kecil.\n",
        "    ts_features_pixel adalah data fitur untuk satu piksel (time, n_features).\n",
        "    \"\"\"\n",
        "    # Split data untuk tuning: train_tune dan val_tune\n",
        "    # Pastikan ada cukup data setelah validation_gap_tune dan forecast_steps\n",
        "    if len(ts_features_pixel) < n_lag + forecast_steps + validation_gap_tune + 1:\n",
        "        logging.debug(f\"Optuna: Not enough data for trial split. Len: {len(ts_features_pixel)}\")\n",
        "        return np.inf # Kembalikan nilai besar jika data tidak cukup\n",
        "\n",
        "    train_tune_data = ts_features_pixel[:-(forecast_steps + validation_gap_tune)]\n",
        "\n",
        "    # y_true_val adalah data target aktual untuk periode validasi\n",
        "    # Indeks awal untuk y_true_val adalah setelah train_tune_data dan validation_gap_tune\n",
        "    idx_start_val_true = len(train_tune_data) + validation_gap_tune\n",
        "    y_true_val = ts_features_pixel[idx_start_val_true : idx_start_val_true + forecast_steps, target_col_idx]\n",
        "\n",
        "    if len(train_tune_data) < n_lag + 1 or len(y_true_val) != forecast_steps :\n",
        "        logging.debug(f\"Optuna: Not enough data after splitting for train_tune or y_true_val. Train_tune len: {len(train_tune_data)}, y_true_val len: {len(y_true_val)}\")\n",
        "        return np.inf\n",
        "\n",
        "    # Definisikan hyperparameter yang akan di-tune\n",
        "    if model_type == 'lstm':\n",
        "        params = {\n",
        "            \"lstm_units\": trial.suggest_int(\"lstm_units\", 16, 64, step=8),\n",
        "            \"lstm_activation\": trial.suggest_categorical(\"lstm_activation\", ['relu', 'tanh']),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True),\n",
        "            \"epochs\": trial.suggest_int(\"epochs\", 20, 80, step=10),\n",
        "            \"batch_size\": trial.suggest_categorical(\"batch_size\", [8, 16, 32]),\n",
        "        }\n",
        "    elif model_type == 'ann':\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "        ann_layers_units = []\n",
        "        for i in range(n_layers):\n",
        "            ann_layers_units.append(trial.suggest_int(f\"units_layer{i+1}\", 16, 128, step=16))\n",
        "        params = {\n",
        "            \"ann_layers_units\": ann_layers_units,\n",
        "            \"ann_activation\": trial.suggest_categorical(\"ann_activation\", ['relu', 'tanh']),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True),\n",
        "            \"epochs\": trial.suggest_int(\"epochs\", 20, 80, step=10),\n",
        "            \"batch_size\": trial.suggest_categorical(\"batch_size\", [8, 16, 32]),\n",
        "        }\n",
        "    elif model_type == 'rf':\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200, step=25),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
        "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
        "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 8),\n",
        "            \"rf_random_state\": CONFIG[\"SEED\"], # Jaga random state untuk RF\n",
        "        }\n",
        "    else:\n",
        "        raise ValueError(f\"Model type {model_type} not supported for Optuna.\")\n",
        "\n",
        "    try:\n",
        "        # Lakukan peramalan dengan hyperparameter yang di-trial\n",
        "        preds_val = forecast_ml(train_tune_data, target_col_idx, n_lag, forecast_steps, model_type, params)\n",
        "\n",
        "        # Hitung RMSE (atau metrik lain)\n",
        "        # Pastikan y_true_val dan preds_val memiliki panjang yang sama dan tidak semua NaN\n",
        "        if len(y_true_val) == len(preds_val) and not np.isnan(y_true_val).all() and not np.isnan(preds_val).all():\n",
        "            # Imputasi jika ada NaN individual setelah peramalan (jarang terjadi jika input sudah diimputasi)\n",
        "            y_true_val_imp = robust_impute(y_true_val)\n",
        "            preds_val_imp = robust_impute(preds_val)\n",
        "            rmse = np.sqrt(mean_squared_error(y_true_val_imp, preds_val_imp))\n",
        "        else:\n",
        "            rmse = np.inf # Kembalikan nilai besar jika prediksi gagal atau tidak valid\n",
        "    except Exception as e:\n",
        "        logging.debug(f\"Optuna trial failed for {model_type} with params {params}: {e}\")\n",
        "        rmse = np.inf # Jika terjadi error saat training/prediksi\n",
        "    return rmse\n",
        "\n",
        "\n",
        "def tune_hyperparameters(ds_pixel_features, target_col_idx, n_lag, forecast_steps, model_type, n_trials=15, validation_gap_tune_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Melakukan tuning hyperparameter menggunakan Optuna untuk satu piksel sampel.\n",
        "    ds_pixel_features adalah DataArray (time, features) untuk satu piksel.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Starting hyperparameter tuning for {model_type} with {n_trials} trials...\")\n",
        "\n",
        "    # Tentukan validation_gap untuk tuning berdasarkan rasio dari panjang data piksel\n",
        "    # Ini adalah gap antara akhir data training Optuna dan awal periode validasi Optuna\n",
        "    validation_gap_tune = max(0, int(len(ds_pixel_features) * validation_gap_tune_ratio))\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\") # Tujuan: minimalkan RMSE\n",
        "    # Fungsi lambda untuk meneruskan argumen tambahan ke objective function\n",
        "    objective_func = lambda trial: optuna_objective(trial, ds_pixel_features.values, target_col_idx, n_lag, forecast_steps, model_type, validation_gap_tune)\n",
        "\n",
        "    try:\n",
        "        study.optimize(objective_func, n_trials=n_trials, show_progress_bar=True)\n",
        "        logging.info(f\"Best {model_type} hyperparams: {study.best_params}, Best RMSE from tuning: {study.best_value:.4f}\")\n",
        "        return study.best_params\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Optuna study failed for {model_type}: {e}\")\n",
        "        return {} # Kembalikan dict kosong jika tuning gagal total\n",
        "\n",
        "\n",
        "# ========== Parallel Grid Forecasting ==========\n",
        "def forecast_grid(ds, model_type, hyperparams, feature_names, target_var, n_lag, forecast_steps, validation_gap, n_jobs, agg_label, return_inter=False):\n",
        "    \"\"\"\n",
        "    Melakukan peramalan untuk seluruh grid secara paralel.\n",
        "\n",
        "    Args:\n",
        "        ds (xr.Dataset): Dataset input.\n",
        "        model_type (str): Tipe model ('arima', 'lstm', 'ann', 'rf').\n",
        "        hyperparams (dict): Hyperparameter untuk model ML (None untuk ARIMA).\n",
        "        feature_names (list): Daftar nama fitur yang akan digunakan (termasuk target).\n",
        "        target_var (str): Nama variabel target.\n",
        "        n_lag (int): Jumlah lag.\n",
        "        forecast_steps (int): Jumlah langkah peramalan.\n",
        "        validation_gap (int): Jarak validasi.\n",
        "        n_jobs (int): Jumlah job paralel.\n",
        "        agg_label (str): Label agregasi waktu.\n",
        "        return_inter (bool): Jika True dan model ARIMA, kembalikan juga peta orde differencing.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray or tuple: Array prediksi (forecast_steps, y, x), atau (prediksi, order_diff_map) untuk ARIMA.\n",
        "    \"\"\"\n",
        "    y_dim, x_dim = ds[target_var].shape[1:]\n",
        "    results = np.full((forecast_steps, y_dim, x_dim), np.nan) # Inisialisasi array hasil prediksi\n",
        "    order_diff_map = np.full((y_dim, x_dim), np.nan) if model_type == \"arima\" else None\n",
        "\n",
        "    # Tentukan indeks kolom target\n",
        "    try:\n",
        "        target_column_index = feature_names.index(target_var)\n",
        "    except ValueError:\n",
        "        logging.error(f\"Target variable '{target_var}' not found in feature_names: {feature_names}. Cannot proceed with ML model.\")\n",
        "        if return_inter and model_type == \"arima\": return results, order_diff_map\n",
        "        return results\n",
        "\n",
        "    # Fungsi untuk memproses satu piksel\n",
        "    def single_pixel(i, j):\n",
        "        # Ambil data time series untuk semua fitur pada piksel (i,j)\n",
        "        # ds[feature_names] akan menghasilkan Dataset, .to_array() mengubahnya jadi DataArray (variable, time, y, x)\n",
        "        # lalu kita select y=i, x=j, dan transpose agar menjadi (time, variable/features)\n",
        "        ts_all_features_pixel = ds[feature_names].isel(y=i, x=j).to_array(dim=\"feature_dim\").transpose(\"time\", \"feature_dim\").values\n",
        "\n",
        "        # Data training adalah semua data sebelum periode validasi dan peramalan\n",
        "        # (forecast_steps + validation_gap) adalah panjang periode yang tidak digunakan untuk training\n",
        "        split_idx = len(ts_all_features_pixel) - (forecast_steps + validation_gap)\n",
        "        if split_idx <= 0: # Tidak cukup data untuk training\n",
        "            logging.debug(f\"Pixel ({i},{j}): Not enough data for train split. Total len: {len(ts_all_features_pixel)}, split_idx: {split_idx}\")\n",
        "            return (np.full(forecast_steps, np.nan), np.nan) if model_type == \"arima\" and return_inter else np.full(forecast_steps, np.nan)\n",
        "\n",
        "        ts_train_features = ts_all_features_pixel[:split_idx, :]\n",
        "\n",
        "        if model_type == \"arima\":\n",
        "            # ARIMA hanya menggunakan variabel target untuk training\n",
        "            ts_train_target_only = ts_train_features[:, target_column_index]\n",
        "            preds, d_val = forecast_arima(ts_train_target_only, forecast_steps, return_order=True)\n",
        "            return preds, d_val\n",
        "        else: # Model ML\n",
        "            # Pastikan ada cukup data training untuk membentuk lag\n",
        "            if len(ts_train_features) < n_lag + 1:\n",
        "                 logging.debug(f\"Pixel ({i},{j}) for {model_type}: Not enough training data ({len(ts_train_features)}) for n_lag ({n_lag}).\")\n",
        "                 return np.full(forecast_steps, np.nan)\n",
        "\n",
        "            preds = forecast_ml(ts_train_features, target_column_index, n_lag, forecast_steps, model_type, hyperparams)\n",
        "            return preds\n",
        "\n",
        "    all_pixels = [(i, j) for i in range(y_dim) for j in range(x_dim)] # Daftar semua koordinat piksel\n",
        "\n",
        "    # Jalankan peramalan secara paralel\n",
        "    if model_type == \"arima\":\n",
        "        # Untuk ARIMA, single_pixel mengembalikan (preds, d_val)\n",
        "        pred_and_order = Parallel(n_jobs=n_jobs)(\n",
        "            delayed(single_pixel)(i, j) for i, j in tqdm(all_pixels, desc=f\"Forecast grid {model_type} [{agg_label}]\")\n",
        "        )\n",
        "        for idx, (i, j) in enumerate(all_pixels):\n",
        "            preds, d_val = pred_and_order[idx]\n",
        "            results[:, i, j] = preds\n",
        "            if order_diff_map is not None: order_diff_map[i, j] = d_val\n",
        "        if return_inter:\n",
        "            return results, order_diff_map\n",
        "        return results\n",
        "    else: # Model ML\n",
        "        # Untuk ML, single_pixel hanya mengembalikan preds\n",
        "        pred_list = Parallel(n_jobs=n_jobs)(\n",
        "            delayed(single_pixel)(i, j) for i, j in tqdm(all_pixels, desc=f\"Forecast grid {model_type} [{agg_label}]\")\n",
        "        )\n",
        "        for idx, (i, j) in enumerate(all_pixels):\n",
        "            results[:, i, j] = pred_list[idx]\n",
        "        if return_inter: # Walaupun untuk ML tidak ada order_diff_map, jaga konsistensi return\n",
        "            return results, None\n",
        "        return results\n",
        "\n",
        "# ========== Evaluation ==========\n",
        "def evaluate_grid(ds, preds, target_var, forecast_steps, validation_gap):\n",
        "    \"\"\"\n",
        "    Mengevaluasi prediksi grid terhadap data observasi.\n",
        "\n",
        "    Args:\n",
        "        ds (xr.Dataset): Dataset input (mengandung data observasi).\n",
        "        preds (np.ndarray): Array prediksi (forecast_steps, y, x).\n",
        "        target_var (str): Nama variabel target.\n",
        "        forecast_steps (int): Jumlah langkah peramalan.\n",
        "        validation_gap (int): Jarak validasi.\n",
        "\n",
        "    Returns:\n",
        "        tuple: r2_map, rmse_map, mae_map (semua berbentuk array 2D (y,x)).\n",
        "    \"\"\"\n",
        "    y_dim, x_dim = ds[target_var].shape[1:]\n",
        "    r2_map = np.full((y_dim, x_dim), np.nan)\n",
        "    rmse_map = np.full((y_dim, x_dim), np.nan)\n",
        "    mae_map = np.full((y_dim, x_dim), np.nan)\n",
        "\n",
        "    # Tentukan indeks waktu untuk data observasi yang akan dibandingkan\n",
        "    # Data observasi dimulai setelah periode training dan validation_gap\n",
        "    # dan berakhir setelah forecast_steps\n",
        "    n_time_total = ds[target_var].shape[0]\n",
        "    idx_start_true = n_time_total - forecast_steps # Indeks awal data observasi untuk perbandingan\n",
        "    idx_end_true = n_time_total # Indeks akhir (eksklusif)\n",
        "\n",
        "    if validation_gap > 0 : # Jika ada gap, data observasi ada setelah gap tersebut\n",
        "        idx_start_true = n_time_total - forecast_steps - validation_gap + validation_gap # Ini salah, harusnya:\n",
        "        idx_start_true = n_time_total - forecast_steps # Prediksi dibandingkan dengan data aktual *setelah* gap\n",
        "        # Misal: T=100, FS=10, VG=5. Train: 0-84. Pred: 85-94. True: 95-99 (Ini jika VG adalah antara train dan test)\n",
        "        # Jika VG adalah antara akhir data dan awal forecast:\n",
        "        # Train: 0-(T-FS-VG-1). Forecast: (T-FS-VG) -> (T-VG-1). True: (T-FS) -> (T-1)\n",
        "        # Dalam skrip ini, VG adalah antara akhir training dan awal periode validasi/peramalan.\n",
        "        # Jadi, y_true adalah data aktual pada periode peramalan.\n",
        "        idx_start_true = n_time_total - forecast_steps\n",
        "        idx_end_true = n_time_total\n",
        "\n",
        "\n",
        "    if idx_start_true < 0 or idx_end_true > n_time_total or idx_start_true >= idx_end_true :\n",
        "        logging.error(f\"Invalid time indices for evaluation. Start: {idx_start_true}, End: {idx_end_true}, Total: {n_time_total}\")\n",
        "        return r2_map, rmse_map, mae_map\n",
        "\n",
        "    y_true_all_pixels = ds[target_var][idx_start_true:idx_end_true, :, :].values # (forecast_steps, y, x)\n",
        "\n",
        "    for i in range(y_dim):\n",
        "        for j in range(x_dim):\n",
        "            y_true_pixel = y_true_all_pixels[:, i, j] # (forecast_steps,)\n",
        "            y_pred_pixel = preds[:, i, j] # (forecast_steps,)\n",
        "\n",
        "            if np.isnan(y_true_pixel).all() or np.isnan(y_pred_pixel).all():\n",
        "                continue # Lewati jika semua observasi atau prediksi adalah NaN\n",
        "\n",
        "            # Imputasi untuk kasus NaN sporadis (seharusnya sudah ditangani sebelumnya)\n",
        "            y_true_imp = robust_impute(y_true_pixel)\n",
        "            y_pred_imp = robust_impute(y_pred_pixel)\n",
        "\n",
        "            # Hanya hitung metrik jika ada lebih dari 1 titik data valid dan ada varians\n",
        "            if len(y_true_imp[~np.isnan(y_true_imp)]) > 1 and np.std(y_true_imp[~np.isnan(y_true_imp)]) > 1e-6 :\n",
        "                try:\n",
        "                    r2_map[i, j] = r2_score(y_true_imp, y_pred_imp)\n",
        "                except ValueError: pass # Terjadi jika y_true konstan\n",
        "            else: # Jika data konstan atau hanya 1 titik, R2 tidak terdefinisi atau 0\n",
        "                 if len(y_true_imp[~np.isnan(y_true_imp)]) > 0 and len(y_pred_imp[~np.isnan(y_pred_imp)]) > 0:\n",
        "                     if np.allclose(y_true_imp, y_pred_imp): r2_map[i,j] = 1.0\n",
        "                     else: r2_map[i,j] = 0.0 # Atau NaN, tergantung definisi yang diinginkan\n",
        "\n",
        "            if len(y_true_imp[~np.isnan(y_true_imp)]) > 0: # Cukup ada data untuk RMSE dan MAE\n",
        "                rmse_map[i, j] = np.sqrt(mean_squared_error(y_true_imp, y_pred_imp))\n",
        "                mae_map[i, j] = mean_absolute_error(y_true_imp, y_pred_imp)\n",
        "\n",
        "    return r2_map, rmse_map, mae_map\n",
        "\n",
        "# ========== Diagnostic: Ljung-Box ==========\n",
        "def ljungbox_pvals(ts_residuals, lags=10, model_df=0):\n",
        "    \"\"\"\n",
        "    Menghitung p-value dari uji Ljung-Box pada residual.\n",
        "\n",
        "    Args:\n",
        "        ts_residuals (np.ndarray): Time series residual.\n",
        "        lags (int): Jumlah lag untuk diuji.\n",
        "        model_df (int): Jumlah parameter model yang diestimasi (untuk penyesuaian df).\n",
        "\n",
        "    Returns:\n",
        "        float: p-value, atau NaN jika gagal.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import statsmodels.api as sm # Impor di dalam fungsi untuk menghindari error jika tidak terinstall global\n",
        "        ts_residuals_clean = robust_impute(ts_residuals) # Bersihkan residual\n",
        "        ts_residuals_clean = ts_residuals_clean[~np.isnan(ts_residuals_clean)] # Hapus NaN sisa\n",
        "\n",
        "        if len(ts_residuals_clean) <= lags or len(ts_residuals_clean) < 2 or np.std(ts_residuals_clean) < 1e-9:\n",
        "            return np.nan # Tidak cukup data atau tidak ada varians\n",
        "\n",
        "        # Pastikan lags tidak melebihi jumlah observasi - model_df - 1\n",
        "        actual_lags = min(lags, len(ts_residuals_clean) - model_df - 1)\n",
        "        if actual_lags <= 0: return np.nan\n",
        "\n",
        "        lb_test_df = sm.stats.acorr_ljungbox(ts_residuals_clean, lags=[actual_lags], return_df=True, model_df=model_df)\n",
        "        return lb_test_df['lb_pvalue'].values[0] if not lb_test_df.empty else np.nan\n",
        "    except Exception as e:\n",
        "        logging.debug(f\"Ljung-Box test failed: {e}\")\n",
        "        return np.nan\n",
        "\n",
        "def diagnostic_ljungbox_grid(ds_train_residuals_map, lags=10, model_df_map=None):\n",
        "    \"\"\"\n",
        "    Melakukan uji Ljung-Box pada residual training untuk seluruh grid.\n",
        "    ds_train_residuals_map: dict, model -> np.ndarray (y,x,time_residuals)\n",
        "    model_df_map: dict, model -> int (jumlah parameter, misal p+q untuk ARIMA)\n",
        "    \"\"\"\n",
        "    if not ds_train_residuals_map: return {}\n",
        "\n",
        "    pval_maps_dict = {}\n",
        "\n",
        "    first_model = list(ds_train_residuals_map.keys())[0]\n",
        "    y_dim, x_dim, _ = ds_train_residuals_map[first_model].shape # (y, x, time_residuals)\n",
        "\n",
        "    for model_name, residuals_grid in ds_train_residuals_map.items():\n",
        "        pval_map = np.full((y_dim, x_dim), np.nan)\n",
        "        current_model_df = model_df_map.get(model_name, 0) if model_df_map else 0\n",
        "        for i in range(y_dim):\n",
        "            for j in range(x_dim):\n",
        "                ts_resid_pixel = residuals_grid[i, j, :]\n",
        "                pval_map[i, j] = ljungbox_pvals(ts_resid_pixel, lags=lags, model_df=current_model_df)\n",
        "        pval_maps_dict[model_name] = pval_map\n",
        "        logging.info(f\"Ljung-Box p-value map calculated for {model_name}.\")\n",
        "    return pval_maps_dict\n",
        "\n",
        "\n",
        "# ========== Output: GeoTIFF, NetCDF, PNG, GIF ==========\n",
        "def ensure_extension(filename, ext):\n",
        "    if not ext.startswith('.'):\n",
        "        ext = '.' + ext\n",
        "    if not filename.lower().endswith(ext):\n",
        "        filename += ext\n",
        "    return filename\n",
        "\n",
        "def hash_params(params: dict) -> str:\n",
        "    if not params: return \"default\"\n",
        "    s = str(sorted(params.items()))\n",
        "    return hashlib.md5(s.encode()).hexdigest()[:8]\n",
        "\n",
        "def setup_folders(base_dir, folders):\n",
        "    out_dirs = {}\n",
        "    for k, v in folders.items():\n",
        "        d = os.path.join(base_dir, v)\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "        out_dirs[k] = d\n",
        "    return out_dirs\n",
        "OUTDIRS = setup_folders(CONFIG[\"BASE_DIR\"], OUTPUT_FOLDERS)\n",
        "\n",
        "def setup_logging(log_dir):\n",
        "    log_path = os.path.join(log_dir, \"run.log\")\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s %(levelname)s:%(message)s',\n",
        "        handlers=[logging.FileHandler(log_path, mode='w'), logging.StreamHandler()]\n",
        "    )\n",
        "    logging.info(\"Logging started. All logs saved to %s\", log_path)\n",
        "setup_logging(OUTDIRS[\"logs\"])\n",
        "\n",
        "def robust_impute(ts):\n",
        "    s = pd.Series(ts)\n",
        "    s = s.interpolate(method='linear', limit_direction='both').fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
        "    return s.values\n",
        "\n",
        "def fallback_crs(ds, config):\n",
        "    try:\n",
        "        if not hasattr(ds, 'rio') or ds.rio.crs is None:\n",
        "            ds = ds.rio.write_crs(config[\"TARGET_CRS\"])\n",
        "            logging.info(f\"Fallback CRS applied: {config['TARGET_CRS']}\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Fallback CRS failed: {e}\")\n",
        "    return ds\n",
        "\n",
        "def calculate_spatial_avg_lag1(data_array, feature_name=\"SPATIAL_AVG_LAG1\"):\n",
        "    kernel = np.array([[1,1,1],[1,0,1],[1,1,1]], dtype=np.float32)\n",
        "    values = data_array.values.astype(np.float32)\n",
        "    spatial_avg_lag1 = np.full_like(values, np.nan)\n",
        "    for t in range(1, values.shape[0]):\n",
        "        data_t_minus_1 = values[t-1,:,:]\n",
        "        nan_mask_t_minus_1 = np.isnan(data_t_minus_1)\n",
        "        neighbor_counts = convolve(~nan_mask_t_minus_1, kernel, mode='constant', cval=0.0)\n",
        "        data_t_minus_1_nan_as_zero = np.nan_to_num(data_t_minus_1, nan=0.0)\n",
        "        neighbor_sum = convolve(data_t_minus_1_nan_as_zero, kernel, mode='constant', cval=0.0)\n",
        "        valid_neighbors_mask = neighbor_counts > 0\n",
        "        avg_values = np.full_like(neighbor_sum, np.nan)\n",
        "        avg_values[valid_neighbors_mask] = neighbor_sum[valid_neighbors_mask] / neighbor_counts[valid_neighbors_mask]\n",
        "        spatial_avg_lag1[t,:,:] = avg_values\n",
        "    spatial_da = xr.DataArray(\n",
        "        spatial_avg_lag1, coords=data_array.coords, dims=data_array.dims, name=feature_name,\n",
        "        attrs={'long_name': f'Average of 8 spatial neighbors at lag 1 for {data_array.name}',\n",
        "               'units': data_array.attrs.get('units', 'unknown')}\n",
        "    )\n",
        "    logging.info(f\"Calculated spatial feature: {feature_name}\")\n",
        "    return spatial_da\n",
        "\n",
        "def add_time_features(ds, time_var='time'):\n",
        "    try:\n",
        "        months = pd.to_datetime(ds[time_var].values).month\n",
        "        ds['month'] = (ds[time_var].dims, months)\n",
        "        ds['sin_month'] = (ds[time_var].dims, np.sin(2 * np.pi * months / 12))\n",
        "        ds['cos_month'] = (ds[time_var].dims, np.cos(2 * np.pi * months / 12))\n",
        "        logging.info(\"Added time features: month, sin_month, cos_month.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Could not add time features: {e}\")\n",
        "    return ds\n",
        "\n",
        "def detect_agg_label(ds):\n",
        "    time = pd.to_datetime(ds['time'].values)\n",
        "    if len(time) < 2:\n",
        "        return \"1m\"\n",
        "    delta = (time[1] - time[0]).days\n",
        "    if 25 <= delta < 35:\n",
        "        return \"1m\"\n",
        "    elif 55 <= delta < 65:\n",
        "        return \"2m\"\n",
        "    elif 85 <= delta < 95:\n",
        "        return \"3m\"\n",
        "    else:\n",
        "        return f\"{delta}d\"\n",
        "\n",
        "def forecast_filename(model, agg_label, start_date, end_date, step_ext):\n",
        "    return f\"{model}_forecast_{agg_label}_{start_date}_to_{end_date}_{step_ext}\"\n",
        "\n",
        "def evalmap_filename(model, agg_label, metric, ext):\n",
        "    return f\"{model}_eval_{metric}_{agg_label}.{ext}\"\n",
        "\n",
        "def difforder_filename(model, agg_label, ext):\n",
        "    return f\"{model}_orderdiff_{agg_label}.{ext}\"\n",
        "\n",
        "def save_geotiff(arr, ds_ref, outdir, fname, descr=None, nodata_val=np.nan):\n",
        "    fname = ensure_extension(fname, \"tif\")\n",
        "    coords = {\"y\": ds_ref[\"y\"], \"x\": ds_ref[\"x\"]}\n",
        "    da = xr.DataArray(arr, dims=(\"y\", \"x\"), coords=coords)\n",
        "    if hasattr(ds_ref, 'rio') and ds_ref.rio.crs:\n",
        "        da = da.rio.write_crs(ds_ref.rio.crs, inplace=True)\n",
        "    if hasattr(ds_ref, 'rio') and ds_ref.rio.transform():\n",
        "        if not ds_ref.rio.transform().is_identity:\n",
        "             da = da.rio.write_transform(ds_ref.rio.transform(), inplace=True)\n",
        "    if descr:\n",
        "        da.attrs['long_name'] = descr\n",
        "    try:\n",
        "        da = da.rio.set_spatial_dims(x_dim='x', y_dim='y', inplace=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "    path = os.path.join(outdir, fname)\n",
        "    try:\n",
        "        da.rio.to_raster(path, nodata=nodata_val, tiled=True, compress='LZW', num_threads='ALL_CPUS')\n",
        "        logging.info(f\"Saved GeoTIFF: {path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save GeoTIFF {path}: {e}\")\n",
        "\n",
        "def save_gif(preds_stack, time_labels, outdir, base_fname, title_prefix=\"\", vmin=None, vmax=None, fps=2):\n",
        "    base_fname = ensure_extension(base_fname, \"gif\")\n",
        "    imgs = []\n",
        "    if vmin is None: vmin = np.nanmin(preds_stack)\n",
        "    if vmax is None: vmax = np.nanmax(preds_stack)\n",
        "    if np.isnan(vmin): vmin = 0\n",
        "    if np.isnan(vmax): vmax = 1\n",
        "    if vmin == vmax : vmax = vmin + 1\n",
        "    temp_dir_frames = os.path.join(outdir, \"temp_frames_for_gif\")\n",
        "    os.makedirs(temp_dir_frames, exist_ok=True)\n",
        "    for t in range(preds_stack.shape[0]):\n",
        "        fig, ax = plt.subplots(figsize=(7,6))\n",
        "        frame_data = np.nan_to_num(preds_stack[t], nan=vmin-abs(vmin*0.1+1))\n",
        "        im = ax.imshow(frame_data, vmin=vmin, vmax=vmax, cmap=CONFIG[\"COLORMAP\"], origin=\"upper\")\n",
        "        date_str = pd.to_datetime(time_labels[t]).strftime('%Y-%m-%d')\n",
        "        ax.set_title(f\"{title_prefix} Forecast: {date_str}\", fontsize=10)\n",
        "        ax.set_xlabel(\"X-coordinate index\")\n",
        "        ax.set_ylabel(\"Y-coordinate index\")\n",
        "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label=\"Predicted Value\")\n",
        "        tmp_path = os.path.join(temp_dir_frames, f\"frame_{t:03d}.png\")\n",
        "        plt.savefig(tmp_path, bbox_inches=\"tight\", dpi=100)\n",
        "        plt.close(fig)\n",
        "        imgs.append(imageio.v2.imread(tmp_path))\n",
        "    gif_path = os.path.join(outdir, base_fname)\n",
        "    try:\n",
        "        imageio.mimsave(gif_path, imgs, fps=fps, loop=0)\n",
        "        logging.info(f\"Saved GIF: {gif_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save GIF {gif_path}: {e}\")\n",
        "    finally:\n",
        "        for t in range(preds_stack.shape[0]):\n",
        "            tmp_path = os.path.join(temp_dir_frames, f\"frame_{t:03d}.png\")\n",
        "            if os.path.exists(tmp_path):\n",
        "                os.remove(tmp_path)\n",
        "        if os.path.exists(temp_dir_frames):\n",
        "            os.rmdir(temp_dir_frames)\n",
        "\n",
        "def save_eval_map_png_and_geotiff(arr, ds_ref, outdir_png, outdir_geotiff, base_fname, title, cmap=\"viridis\", nodata_val=np.nan):\n",
        "    png_fname = ensure_extension(base_fname, \"png\")\n",
        "    tif_fname = ensure_extension(base_fname, \"tif\")\n",
        "    save_geotiff(arr, ds_ref, outdir_geotiff, tif_fname, descr=title, nodata_val=nodata_val)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    valid_arr = arr[~np.isnan(arr)]\n",
        "    vmin_plot, vmax_plot = (np.min(valid_arr), np.max(valid_arr)) if len(valid_arr) > 0 else (0,1)\n",
        "    if vmin_plot == vmax_plot and len(valid_arr)>0 : vmax_plot = vmin_plot + 1\n",
        "    if not len(valid_arr)>0 : vmin_plot, vmax_plot = 0,1\n",
        "    im = plt.imshow(arr, origin=\"upper\", cmap=cmap, vmin=vmin_plot, vmax=vmax_plot)\n",
        "    plt.title(title, fontsize=12)\n",
        "    plt.xlabel(\"X-coordinate index\")\n",
        "    plt.ylabel(\"Y-coordinate index\")\n",
        "    plt.colorbar(im, label=\"Metric Value\")\n",
        "    plt.tight_layout()\n",
        "    png_path = os.path.join(outdir_png, png_fname)\n",
        "    try:\n",
        "        plt.savefig(png_path, dpi=150)\n",
        "        logging.info(f\"Saved PNG map: {png_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save PNG map {png_path}: {e}\")\n",
        "    plt.close()\n",
        "\n",
        "def save_netcdf_output(ds_out, outdir, fname_base, agg_label, time_labels_forecast):\n",
        "    fname = ensure_extension(fname_base, \"nc\")\n",
        "    path = os.path.join(outdir, fname)\n",
        "    encoding = {}\n",
        "    for var_name in ds_out.data_vars:\n",
        "        encoding[var_name] = {'_FillValue': np.nan, 'zlib': True, 'complevel': 4}\n",
        "    for coord_name in ds_out.coords:\n",
        "        if ds_out[coord_name].dtype.kind in 'ifc':\n",
        "             encoding[coord_name] = {'_FillValue': None}\n",
        "        elif ds_out[coord_name].dtype.kind == 'M':\n",
        "             encoding[coord_name] = {'_FillValue': None, 'dtype': 'double', 'units': \"days since 1970-01-01\"}\n",
        "    try:\n",
        "        ds_out.to_netcdf(path, encoding=encoding, format='NETCDF4')\n",
        "        logging.info(f\"Saved NetCDF: {path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save NetCDF {path}: {e}\")\n",
        "\n",
        "\n",
        "# ========== Plotting: Time Series Agregat ==========\n",
        "def plot_ts_aggregate(ds, preds_dict, target_var, forecast_steps, validation_gap, outdir, n_time, agg_label, time_index_all):\n",
        "    \"\"\"\n",
        "    Membuat plot time series agregat (total nilai target di seluruh grid).\n",
        "\n",
        "    Args:\n",
        "        ds (xr.Dataset): Dataset input.\n",
        "        preds_dict (dict): Dictionary prediksi {model_name: preds_array}.\n",
        "        target_var (str): Nama variabel target.\n",
        "        forecast_steps (int): Jumlah langkah peramalan.\n",
        "        validation_gap (int): Jarak validasi.\n",
        "        outdir (str): Direktori output untuk plot.\n",
        "        n_time (int): Jumlah total langkah waktu.\n",
        "        agg_label (str): Label agregasi waktu.\n",
        "        time_index_all (pd.DatetimeIndex): Indeks waktu untuk seluruh periode.\n",
        "    \"\"\"\n",
        "    obs = ds[target_var].values # (time, y, x)\n",
        "    obs_total = np.nansum(obs, axis=(1,2)) # Agregasi spasial (sum)\n",
        "\n",
        "    # Indeks waktu di mana peramalan dimulai pada data observasi\n",
        "    # Ini adalah setelah periode training dan validation_gap\n",
        "    idx_forecast_start_on_obs = n_time - forecast_steps\n",
        "\n",
        "    plt.figure(figsize=(14, 7)) # Ukuran disesuaikan\n",
        "\n",
        "    # Plot data observasi keseluruhan\n",
        "    plt.plot(time_index_all, obs_total, label=\"Observed (Total Grid)\", color=\"gray\", linewidth=1.5, alpha=0.7)\n",
        "\n",
        "    # Garis vertikal menandai awal periode peramalan\n",
        "    if idx_forecast_start_on_obs < len(time_index_all) and idx_forecast_start_on_obs >=0:\n",
        "        plt.axvline(time_index_all[idx_forecast_start_on_obs], color=\"black\", linestyle=\"--\", linewidth=1.5, label=f\"Forecast/Validation Start ({time_index_all[idx_forecast_start_on_obs].strftime('%Y-%m')})\")\n",
        "\n",
        "    # Indeks waktu untuk plot prediksi dan observasi pada periode validasi/peramalan\n",
        "    time_index_forecast_period = time_index_all[idx_forecast_start_on_obs : idx_forecast_start_on_obs + forecast_steps]\n",
        "\n",
        "    # Plot prediksi dari setiap model\n",
        "    colors = plt.cm.get_cmap('tab10', len(preds_dict)) # Palet warna\n",
        "    for i, (model, preds_model) in enumerate(preds_dict.items()):\n",
        "        preds_total = np.nansum(preds_model, axis=(1,2)) # Agregasi spasial prediksi\n",
        "        if len(preds_total) == len(time_index_forecast_period):\n",
        "            plt.plot(time_index_forecast_period, preds_total, marker='o', linestyle='-', markersize=5, label=f\"{model.upper()} Forecast\", color=colors(i))\n",
        "        else:\n",
        "            logging.warning(f\"Length mismatch for {model} aggregate plot. Preds: {len(preds_total)}, Time: {len(time_index_forecast_period)}\")\n",
        "\n",
        "\n",
        "    # Plot data observasi pada periode training dan validasi (jika ada)\n",
        "    if idx_forecast_start_on_obs > 0: # Jika ada periode training\n",
        "        plt.plot(time_index_all[:idx_forecast_start_on_obs], obs_total[:idx_forecast_start_on_obs], color=\"blue\", linestyle='-', marker='.', markersize=4, label=\"Training Period (Observed)\")\n",
        "\n",
        "    # Plot data observasi pada periode validasi (yang dibandingkan dengan forecast)\n",
        "    obs_validation_period = obs_total[idx_forecast_start_on_obs : idx_forecast_start_on_obs + forecast_steps]\n",
        "    if len(obs_validation_period) == len(time_index_forecast_period):\n",
        "        plt.plot(time_index_forecast_period, obs_validation_period, color=\"red\", linestyle='-', marker='x', markersize=5, label=\"Validation Period (Observed)\")\n",
        "\n",
        "    plt.legend(loc=\"best\", fontsize=10)\n",
        "    plt.xlabel(\"Time\", fontsize=12)\n",
        "    plt.ylabel(f\"Total {target_var} (Aggregated over Grid)\", fontsize=12)\n",
        "    plt.title(f\"Aggregated Time Series Forecast vs. Observation [{agg_label}]\", fontsize=14)\n",
        "    plt.grid(True, linestyle=':', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fname = f\"aggregate_timeseries_plot_{agg_label}.png\"\n",
        "    path = os.path.join(outdir, fname)\n",
        "    try:\n",
        "        plt.savefig(path, dpi=150)\n",
        "        logging.info(f\"Saved aggregate time series plot: {path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save aggregate plot {path}: {e}\")\n",
        "    plt.close()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-17 13:45:14,190 INFO:Logging started. All logs saved to D:\\DataPenelitian\\Longsor\\logs\\run.log\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "P1151ZmUtrym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1828d1db-bc51-4789-ebe5-23a2d557e346"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== MAIN ==========\n",
        "def main():\n",
        "    logging.info(\"===== PIPELINE STARTED =====\")\n",
        "    try:\n",
        "        # Set working dir\n",
        "        if os.path.exists(CONFIG[\"BASE_DIR\"]):\n",
        "            os.chdir(CONFIG[\"BASE_DIR\"])\n",
        "            logging.info(f\"Changed working directory to: {CONFIG['BASE_DIR']}\")\n",
        "        else:\n",
        "            logging.error(f\"BASE_DIR {CONFIG['BASE_DIR']} does not exist. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Load Data\n",
        "        logging.info(f\"Loading NetCDF data from: {CONFIG['NC_FILE']}\")\n",
        "        ds = xr.open_dataset(CONFIG[\"NC_FILE\"])\n",
        "        ds = fallback_crs(ds, CONFIG) # Pastikan CRS ada\n",
        "        agg_label = detect_agg_label(ds) # Deteksi label agregasi waktu\n",
        "        time_index_all = pd.to_datetime(ds['time'].values) # Indeks waktu keseluruhan\n",
        "\n",
        "        # Preprocessing & Feature Engineering\n",
        "        logging.info(\"Starting preprocessing and feature engineering...\")\n",
        "        # 1. Proses data longsor jika ada\n",
        "        if CONFIG[\"LANDSLIDE_SHP_FILE\"] and os.path.exists(CONFIG[\"LANDSLIDE_SHP_FILE\"]):\n",
        "            landslide_da = process_landslide_data(CONFIG[\"LANDSLIDE_SHP_FILE\"], CONFIG[\"LANDSLIDE_DATE_COLUMN\"], ds, CONFIG[\"LANDSLIDE_FEATURE_NAME\"])\n",
        "            if landslide_da is not None:\n",
        "                ds[CONFIG[\"LANDSLIDE_FEATURE_NAME\"]] = landslide_da\n",
        "        else:\n",
        "            logging.info(f\"Landslide SHP file not found or not specified. Skipping landslide feature.\")\n",
        "            # Buat array nol jika tidak ada data longsor, agar struktur fitur konsisten\n",
        "            ds[CONFIG[\"LANDSLIDE_FEATURE_NAME\"]] = xr.DataArray(\n",
        "                np.zeros((ds.dims['time'], ds.dims['y'], ds.dims['x']), dtype=np.int32),\n",
        "                coords=ds[CONFIG[\"TARGET_VARIABLE\"]].coords, dims=ds[CONFIG[\"TARGET_VARIABLE\"]].dims\n",
        "            )\n",
        "\n",
        "\n",
        "        # 2. Hitung fitur spasial\n",
        "        ds[CONFIG[\"SPATIAL_FEATURE_NAME\"]] = calculate_spatial_avg_lag1(ds[CONFIG[\"TARGET_VARIABLE\"]], CONFIG[\"SPATIAL_FEATURE_NAME\"])\n",
        "\n",
        "        # 3. Tambah fitur waktu (sin/cos bulan)\n",
        "        ds = add_time_features(ds) # Menambahkan 'month', 'sin_month', 'cos_month'\n",
        "\n",
        "        # Daftar fitur yang akan digunakan oleh model ML\n",
        "        # ARIMA hanya akan menggunakan TARGET_VARIABLE\n",
        "        feature_names_for_ml = [\n",
        "            CONFIG[\"TARGET_VARIABLE\"],\n",
        "            CONFIG[\"LANDSLIDE_FEATURE_NAME\"],\n",
        "            CONFIG[\"SPATIAL_FEATURE_NAME\"],\n",
        "            'sin_month',\n",
        "            'cos_month'\n",
        "        ]\n",
        "        # Pastikan semua fitur ada di dataset, jika tidak, hapus dari daftar\n",
        "        feature_names_for_ml = [f for f in feature_names_for_ml if f in ds]\n",
        "        logging.info(f\"Features for ML models: {feature_names_for_ml}\")\n",
        "        if CONFIG[\"TARGET_VARIABLE\"] not in feature_names_for_ml:\n",
        "            logging.error(f\"Target variable {CONFIG['TARGET_VARIABLE']} is missing after feature engineering. Exiting.\")\n",
        "            return\n",
        "        target_column_idx_ml = feature_names_for_ml.index(CONFIG[\"TARGET_VARIABLE\"])\n",
        "\n",
        "\n",
        "        # Tentukan parameter jendela (lag, forecast steps, validation gap)\n",
        "        n_lag, forecast_steps, validation_gap, n_time = set_window_params(ds, CONFIG)\n",
        "        logging.info(f\"Final window params: n_lag={n_lag}, forecast_steps={forecast_steps}, validation_gap={validation_gap}, n_time_total={n_time}\")\n",
        "\n",
        "        # Inisialisasi dictionary untuk menyimpan prediksi dan hyperparameter terbaik\n",
        "        all_model_predictions = {} # {model_name: prediction_array (steps, y, x)}\n",
        "        all_model_eval_metrics = {} # {model_name: {metric_name: metric_map (y,x)}}\n",
        "        all_model_residuals_train = {} # {model_name: residuals_array_train (y,x,time)}\n",
        "        best_hyperparams_all_models = {}\n",
        "        arima_order_diff_map = None # Khusus untuk ARIMA\n",
        "\n",
        "        # Hyperparameter tuning (jika diaktifkan)\n",
        "        if CONFIG[\"HYPERPARAM_TUNE\"]:\n",
        "            logging.info(\"--- Starting Hyperparameter Tuning ---\")\n",
        "            # Ambil data dari piksel sampel untuk tuning\n",
        "            # Gunakan semua fitur yang relevan untuk piksel sampel tersebut\n",
        "            sample_y, sample_x = CONFIG[\"SAMPLE_COORDS\"][0]\n",
        "            ds_sample_pixel_features = ds[feature_names_for_ml].isel(y=sample_y, x=sample_x).to_array(dim=\"feature_dim\").transpose(\"time\", \"feature_dim\")\n",
        "\n",
        "            for model_name in MODEL_LIST:\n",
        "                if model_name != \"arima\": # ARIMA tidak di-tune dengan Optuna di sini\n",
        "                    best_hyperparams_all_models[model_name] = tune_hyperparameters(\n",
        "                        ds_sample_pixel_features, target_column_idx_ml, n_lag, forecast_steps, model_name,\n",
        "                        CONFIG[\"OPTUNA_TRIALS\"]\n",
        "                    )\n",
        "                else:\n",
        "                    best_hyperparams_all_models[model_name] = {} # Tidak ada hyperparams eksternal untuk ARIMA auto\n",
        "            logging.info(\"--- Hyperparameter Tuning Finished ---\")\n",
        "        else:\n",
        "            logging.info(\"Skipping hyperparameter tuning. Using default or pre-defined parameters.\")\n",
        "            # Isi dengan hyperparameter default jika tidak tuning (atau biarkan kosong jika model punya default sendiri)\n",
        "            for model_name in MODEL_LIST: best_hyperparams_all_models[model_name] = {}\n",
        "\n",
        "\n",
        "        # Forecast grid per model\n",
        "        logging.info(\"--- Starting Grid Forecasting ---\")\n",
        "        for model_name in MODEL_LIST:\n",
        "            logging.info(f\"Processing model: {model_name.upper()}\")\n",
        "            current_hyperparams = best_hyperparams_all_models.get(model_name, {}) # Ambil hyperparams hasil tuning atau default\n",
        "\n",
        "            if model_name == \"arima\":\n",
        "                # ARIMA hanya menggunakan target variable, jadi feature_names tidak relevan untuk forecast_grid-nya\n",
        "                preds_grid, arima_order_diff_map_current = forecast_grid(\n",
        "                    ds, model_name, None, [CONFIG[\"TARGET_VARIABLE\"]], CONFIG[\"TARGET_VARIABLE\"],\n",
        "                    n_lag, forecast_steps, validation_gap, CONFIG[\"N_JOBS_PARALLEL\"], agg_label, return_inter=True\n",
        "                )\n",
        "                arima_order_diff_map = arima_order_diff_map_current # Simpan peta orde 'd'\n",
        "            else: # Model ML\n",
        "                preds_grid, _ = forecast_grid( # _ untuk order_diff_map yang None untuk ML\n",
        "                    ds, model_name, current_hyperparams, feature_names_for_ml, CONFIG[\"TARGET_VARIABLE\"],\n",
        "                    n_lag, forecast_steps, validation_gap, CONFIG[\"N_JOBS_PARALLEL\"], agg_label, return_inter=True\n",
        "                )\n",
        "            all_model_predictions[model_name] = preds_grid\n",
        "            logging.info(f\"Grid forecast for {model_name} complete.\")\n",
        "\n",
        "            # Simpan prediksi GeoTIFF per langkah waktu\n",
        "            # Tentukan tanggal mulai dan akhir periode peramalan\n",
        "            # Periode peramalan dimulai setelah data training dan validation_gap\n",
        "            idx_forecast_period_start = n_time - forecast_steps\n",
        "            time_labels_forecast = time_index_all[idx_forecast_period_start : idx_forecast_period_start + forecast_steps]\n",
        "\n",
        "            if len(time_labels_forecast) == preds_grid.shape[0]:\n",
        "                for t_step in range(preds_grid.shape[0]):\n",
        "                    fname_geotiff_step = forecast_filename(model_name, agg_label, time_labels_forecast[t_step].strftime(\"%Y%m%d\"), time_labels_forecast[t_step].strftime(\"%Y%m%d\"), f\"step{t_step+1}.tif\")\n",
        "                    save_geotiff(preds_grid[t_step, :, :], ds, OUTDIRS[\"geotiff\"], fname_geotiff_step,\n",
        "                                 descr=f\"{model_name.upper()} Forecast Step {t_step+1} ({time_labels_forecast[t_step].strftime('%Y-%m-%d')}) [{agg_label}]\")\n",
        "                # Simpan animasi GIF\n",
        "                gif_fname_base = forecast_filename(model_name, agg_label, time_labels_forecast[0].strftime(\"%Y%m%d\"), time_labels_forecast[-1].strftime(\"%Y%m%d\"), \"gif\")\n",
        "                save_gif(preds_grid, time_labels_forecast, OUTDIRS[\"gif\"], gif_fname_base, title_prefix=f\"{model_name.upper()}\", fps=CONFIG[\"GIF_FPS\"])\n",
        "            else:\n",
        "                logging.warning(f\"Skipping GeoTIFF/GIF saving for {model_name} due to time label mismatch.\")\n",
        "\n",
        "            # Simpan prediksi mentah (npz) untuk analisis lebih lanjut jika perlu\n",
        "            npz_fname_base = forecast_filename(model_name, agg_label, time_labels_forecast[0].strftime(\"%Y%m%d\"), time_labels_forecast[-1].strftime(\"%Y%m%d\"), \"npz\")\n",
        "            np.savez_compressed(os.path.join(OUTDIRS[\"intermediate\"], npz_fname_base), preds=preds_grid, allow_pickle=False)\n",
        "            logging.info(f\"Saved raw predictions (npz) for {model_name}.\")\n",
        "\n",
        "        # Simpan peta orde differencing ARIMA jika ada\n",
        "        if arima_order_diff_map is not None:\n",
        "            diff_fname_base = difforder_filename(\"arima\", agg_label, \"\")[:-1] # Hapus titik terakhir\n",
        "            save_eval_map_png_and_geotiff(arima_order_diff_map, ds, OUTDIRS[\"png\"], OUTDIRS[\"geotiff\"],\n",
        "                                          diff_fname_base, f\"ARIMA Differencing Order (d) [{agg_label}]\", cmap=\"coolwarm\")\n",
        "\n",
        "        # Evaluasi Model\n",
        "        logging.info(\"--- Starting Model Evaluation ---\")\n",
        "        for model_name, preds_model in all_model_predictions.items():\n",
        "            r2_map, rmse_map, mae_map = evaluate_grid(ds, preds_model, CONFIG[\"TARGET_VARIABLE\"], forecast_steps, validation_gap)\n",
        "            all_model_eval_metrics[model_name] = {\"r2\": r2_map, \"rmse\": rmse_map, \"mae\": mae_map}\n",
        "            logging.info(f\"Evaluation metrics calculated for {model_name}.\")\n",
        "            for metric_name, metric_map_data in all_model_eval_metrics[model_name].items():\n",
        "                eval_fname_base = evalmap_filename(model_name, agg_label, metric_name, \"\")[:-1]\n",
        "                save_eval_map_png_and_geotiff(metric_map_data, ds, OUTDIRS[\"png\"], OUTDIRS[\"geotiff\"],\n",
        "                                              eval_fname_base, f\"{model_name.upper()} - {metric_name.upper()} Map [{agg_label}]\",\n",
        "                                              cmap='viridis' if metric_name != 'r2' else 'viridis_r') # r2 lebih baik jika dibalik\n",
        "\n",
        "        # Diagnostik Residual (Ljung-Box pada residual training)\n",
        "        # Ini memerlukan penyimpanan residual training dari `forecast_grid` atau menjalankannya lagi.\n",
        "        # Untuk saat ini, kita akan skip bagian ini karena `forecast_grid` tidak mengembalikan residual.\n",
        "        # Jika ingin diimplementasikan, `single_pixel` perlu dimodifikasi untuk mengembalikan residual training.\n",
        "        logging.info(\"Skipping Ljung-Box on training residuals for now (requires modification to return residuals).\")\n",
        "        # pval_maps_all_models = diagnostic_ljungbox_grid(all_model_residuals_train, ...)\n",
        "        # for model_name, pval_map_model in pval_maps_all_models.items():\n",
        "        #     diag_fname_base = residualdiag_filename(model_name, agg_label, \"\")[:-1]\n",
        "        #     save_eval_map_png_and_geotiff(pval_map_model, ds, OUTDIRS[\"png\"], OUTDIRS[\"geotiff\"],\n",
        "        #                                   diag_fname_base, f\"{model_name.upper()} Ljung-Box p-value (Train Resid) [{agg_label}]\", cmap='coolwarm_r')\n",
        "\n",
        "\n",
        "        # Membuat Dataset Output NetCDF Gabungan\n",
        "        logging.info(\"--- Creating Combined Output NetCDF ---\")\n",
        "        ds_output_combined = xr.Dataset(\n",
        "            coords={'time_forecast': time_labels_forecast,\n",
        "                    'y': ds['y'],\n",
        "                    'x': ds['x']}\n",
        "        )\n",
        "        # Tambahkan prediksi dari setiap model\n",
        "        for model_name, preds_array in all_model_predictions.items():\n",
        "            da_pred = xr.DataArray(\n",
        "                preds_array,\n",
        "                coords={'time_forecast': time_labels_forecast, 'y': ds['y'], 'x': ds['x']},\n",
        "                dims=('time_forecast', 'y', 'x'),\n",
        "                name=f'{model_name}_prediction'\n",
        "            )\n",
        "            da_pred.attrs['long_name'] = f'{model_name.upper()} {forecast_steps}-step ahead forecast'\n",
        "            da_pred.attrs['units'] = ds[CONFIG[\"TARGET_VARIABLE\"]].attrs.get('units', 'unknown')\n",
        "            ds_output_combined[f'{model_name}_prediction'] = da_pred\n",
        "\n",
        "        # Tambahkan metrik evaluasi\n",
        "        for model_name, metrics_dict in all_model_eval_metrics.items():\n",
        "            for metric_name, metric_data in metrics_dict.items():\n",
        "                da_metric = xr.DataArray(\n",
        "                    metric_data,\n",
        "                    coords={'y': ds['y'], 'x': ds['x']},\n",
        "                    dims=('y', 'x'),\n",
        "                    name=f'{model_name}_{metric_name}'\n",
        "                )\n",
        "                da_metric.attrs['long_name'] = f'{model_name.upper()} {metric_name.upper()} on validation period'\n",
        "                ds_output_combined[f'{model_name}_{metric_name}'] = da_metric\n",
        "\n",
        "        # Tambahkan peta orde 'd' ARIMA\n",
        "        if arima_order_diff_map is not None:\n",
        "            da_arima_d = xr.DataArray(\n",
        "                arima_order_diff_map,\n",
        "                coords={'y': ds['y'], 'x': ds['x']},\n",
        "                dims=('y', 'x'),\n",
        "                name='arima_differencing_order_d'\n",
        "            )\n",
        "            da_arima_d.attrs['long_name'] = 'ARIMA differencing order (d) determined by auto_arima'\n",
        "            ds_output_combined['arima_differencing_order_d'] = da_arima_d\n",
        "\n",
        "        # Tambahkan CRS\n",
        "        if hasattr(ds, 'rio') and ds.rio.crs:\n",
        "            ds_output_combined.rio.write_crs(ds.rio.crs, inplace=True)\n",
        "            ds_output_combined.rio.write_coordinate_system(inplace=True)\n",
        "\n",
        "\n",
        "        # Simpan NetCDF gabungan\n",
        "        save_netcdf_output(ds_output_combined, OUTDIRS[\"netcdf\"], \"all_models_forecast_and_eval\", agg_label, time_labels_forecast)\n",
        "\n",
        "\n",
        "        # Plot time series agregat\n",
        "        logging.info(\"Plotting aggregated time series...\")\n",
        "        plot_ts_aggregate(ds, all_model_predictions, CONFIG[\"TARGET_VARIABLE\"], forecast_steps, validation_gap, OUTDIRS[\"png\"], n_time, agg_label, time_index_all)\n",
        "\n",
        "        logging.info(\"===== PIPELINE FINISHED SUCCESSFULLY =====\")\n",
        "\n",
        "    except FileNotFoundError as fnf_err:\n",
        "        logging.critical(f\"CRITICAL ERROR - File Not Found: {fnf_err}\")\n",
        "    except ValueError as val_err:\n",
        "        logging.critical(f\"CRITICAL ERROR - Value Error: {val_err}\")\n",
        "    except ImportError as imp_err:\n",
        "        logging.critical(f\"CRITICAL ERROR - Import Error: {imp_err}. Please ensure all dependencies are installed.\")\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"CRITICAL ERROR - An unexpected error occurred: {e}\", exc_info=True)\n",
        "    finally:\n",
        "        if 'ds' in locals() and ds is not None:\n",
        "            ds.close()\n",
        "            logging.info(\"Input NetCDF dataset closed.\")\n",
        "        if 'ds_output_combined' in locals() and ds_output_combined is not None:\n",
        "            ds_output_combined.close()\n",
        "            logging.info(\"Output NetCDF dataset closed.\")\n",
        "        logging.info(\"===== END OF SCRIPT =====\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "GVgY98mtuEbx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7c9a3a5673364c208514e925fec18482",
            "4b47712971af465fa4475f9f83f9e100",
            "746ce13448ca4fa7a5e1df46ffc0fdcb",
            "d8e7489b9a714ff68361f769b159a12c",
            "dfbc25ac0d4d450c88ae1ab3b83987e6",
            "a76a4dab989148e78d8cb2879758e47e",
            "ef3485b5196e4d58b9849c9b57c62f11",
            "47a05c782ff14713b3111b03f3770c22",
            "b42011dcdb58498aa994d25752e7c2fd",
            "dea02d13c84c43fcabb3d895439b1c6a",
            "4526ce3d18574a5b8631bb1c924c120a"
          ]
        },
        "outputId": "78eef11d-02e3-4bf7-a625-f37d3e479819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-17 14:27:41,976 INFO:===== PIPELINE STARTED =====\n",
            "2025-05-17 14:27:41,978 INFO:Changed working directory to: D:\\DataPenelitian\\Longsor\n",
            "2025-05-17 14:27:41,979 INFO:Loading NetCDF data from: nc_20250426_1M.nc\n",
            "2025-05-17 14:27:41,996 INFO:Fallback CRS applied: EPSG:32749\n",
            "2025-05-17 14:27:41,998 INFO:Starting preprocessing and feature engineering...\n",
            "2025-05-17 14:27:42,060 INFO:Loaded 1743 landslide points from TitikLongsor_Magelang_2025.shp.\n",
            "\n",
            "Rasterizing Landslides:   0%|                                                                  | 0/100 [00:00<?, ?it/s]\u001b[A\n",
            "Rasterizing Landslides:  42%|███████████████████████▌                                | 42/100 [00:00<00:00, 416.69it/s]\u001b[A\n",
            "Rasterizing Landslides: 100%|███████████████████████████████████████████████████████| 100/100 [00:00<00:00, 414.13it/s]\n",
            "2025-05-17 14:27:42,314 INFO:Landslide data processing complete. Feature: LANDSLIDE_COUNT\n",
            "2025-05-17 14:27:42,333 INFO:Calculated spatial feature: COUNT_SPATIAL_AVG_LAG1\n",
            "2025-05-17 14:27:42,337 INFO:Added time features: month, sin_month, cos_month.\n",
            "2025-05-17 14:27:42,338 INFO:Features for ML models: ['COUNT', 'LANDSLIDE_COUNT', 'COUNT_SPATIAL_AVG_LAG1', 'sin_month', 'cos_month']\n",
            "2025-05-17 14:27:42,338 INFO:Window params set: n_lag=25, forecast_steps=10, validation_gap=5\n",
            "2025-05-17 14:27:42,339 INFO:Final window params: n_lag=25, forecast_steps=10, validation_gap=5, n_time_total=100\n",
            "2025-05-17 14:27:42,340 INFO:--- Starting Hyperparameter Tuning ---\n",
            "2025-05-17 14:27:42,341 INFO:Starting hyperparameter tuning for ann with 15 trials...\n",
            "[I 2025-05-17 14:27:42,343] A new study created in memory with name: no-name-7d0773c1-c1bc-4fca-9bde-3e1e894f290a\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c9a3a5673364c208514e925fec18482"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Forecast grid ann [1m]:  56%|█████████████████████████████                       | 1080/1935 [1:12:32<09:00,  1.58it/s]2025-05-17 14:27:44,368 INFO:Input NetCDF dataset closed.\n",
            "2025-05-17 14:27:44,369 INFO:===== END OF SCRIPT =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[W 2025-05-17 14:27:44,360] Trial 0 failed with parameters: {'n_layers': 2, 'units_layer1': 48, 'units_layer2': 112, 'ann_activation': 'tanh', 'learning_rate': 0.0014415291861597397, 'epochs': 40, 'batch_size': 16} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Wilicious\\AppData\\Local\\Temp\\ipykernel_16880\\2049637189.py\", line 88, in <lambda>\n",
            "    objective_func = lambda trial: optuna_objective(trial, ds_pixel_features.values, target_col_idx, n_lag, forecast_steps, model_type, validation_gap_tune)\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Wilicious\\AppData\\Local\\Temp\\ipykernel_16880\\2049637189.py\", line 58, in optuna_objective\n",
            "    preds_val = forecast_ml(train_tune_data, target_col_idx, n_lag, forecast_steps, model_type, params)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Wilicious\\AppData\\Local\\Temp\\ipykernel_16880\\2246258574.py\", line 276, in forecast_ml\n",
            "    model.fit(X_train_flat, y_train, epochs=hyperparams.get('epochs', 50), batch_size=hyperparams.get('batch_size', 16), verbose=0)\n",
            "  File \"C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 369, in fit\n",
            "    for step, iterator in epoch_iterator:\n",
            "  File \"C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 734, in __next__\n",
            "    return next(self._epoch_iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py\", line 102, in _enumerate_iterator\n",
            "    self._current_iterator = iter(self._get_iterator())\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 501, in __iter__\n",
            "    return iterator_ops.OwnedIterator(self)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 709, in __init__\n",
            "    self._create_iterator(dataset)\n",
            "  File \"C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 748, in _create_iterator\n",
            "    gen_dataset_ops.make_iterator(ds_variant, self._iterator_resource)\n",
            "  File \"C:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 3478, in make_iterator\n",
            "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-05-17 14:27:44,364] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 244\u001b[39m\n\u001b[32m    240\u001b[39m         logging.info(\u001b[33m\"\u001b[39m\u001b[33m===== END OF SCRIPT =====\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m MODEL_LIST:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_name != \u001b[33m\"\u001b[39m\u001b[33marima\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;66;03m# ARIMA tidak di-tune dengan Optuna di sini\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         best_hyperparams_all_models[model_name] = \u001b[43mtune_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m            \u001b[49m\u001b[43mds_sample_pixel_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_column_idx_ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_lag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOPTUNA_TRIALS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     86\u001b[39m         best_hyperparams_all_models[model_name] = {} \u001b[38;5;66;03m# Tidak ada hyperparams eksternal untuk ARIMA auto\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mtune_hyperparameters\u001b[39m\u001b[34m(ds_pixel_features, target_col_idx, n_lag, forecast_steps, model_type, n_trials, validation_gap_tune_ratio)\u001b[39m\n\u001b[32m     88\u001b[39m objective_func = \u001b[38;5;28;01mlambda\u001b[39;00m trial: optuna_objective(trial, ds_pixel_features.values, target_col_idx, n_lag, forecast_steps, model_type, validation_gap_tune)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m hyperparams: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Best RMSE from tuning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m study.best_params\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mtune_hyperparameters.<locals>.<lambda>\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     86\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# Tujuan: minimalkan RMSE\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Fungsi lambda untuk meneruskan argumen tambahan ke objective function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m objective_func = \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43moptuna_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_pixel_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_lag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_gap_tune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     91\u001b[39m     study.optimize(objective_func, n_trials=n_trials, show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36moptuna_objective\u001b[39m\u001b[34m(trial, ts_features_pixel, target_col_idx, n_lag, forecast_steps, model_type, validation_gap_tune)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not supported for Optuna.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Lakukan peramalan dengan hyperparameter yang di-trial\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     preds_val = \u001b[43mforecast_ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tune_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_lag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# Hitung RMSE (atau metrik lain)\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# Pastikan y_true_val dan preds_val memiliki panjang yang sama dan tidak semua NaN\u001b[39;00m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_true_val) == \u001b[38;5;28mlen\u001b[39m(preds_val) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isnan(y_true_val).all() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isnan(preds_val).all():\n\u001b[32m     63\u001b[39m         \u001b[38;5;66;03m# Imputasi jika ada NaN individual setelah peramalan (jarang terjadi jika input sudah diimputasi)\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 276\u001b[39m, in \u001b[36mforecast_ml\u001b[39m\u001b[34m(ts_input_features, target_column_index, n_lag, forecast_steps, model_type, hyperparams, return_residual)\u001b[39m\n\u001b[32m    274\u001b[39m X_train_flat = X_train.reshape(X_train.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n\u001b[32m    275\u001b[39m model = build_ann(X_train_flat.shape[\u001b[32m1\u001b[39m], hyperparams)\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m y_train_pred = model.predict(X_train_flat, verbose=\u001b[32m0\u001b[39m).flatten()\n\u001b[32m    278\u001b[39m residuals[-\u001b[38;5;28mlen\u001b[39m(y_train_pred):] = scaler_target.inverse_transform(y_train.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)).flatten() - scaler_target.inverse_transform(y_train_pred.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)).flatten()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:369\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    367\u001b[39m callbacks.on_epoch_begin(epoch)\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator.catch_stop_iteration():\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:734\u001b[39m, in \u001b[36mTFEpochIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m._epoch_iterator)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:102\u001b[39m, in \u001b[36mEpochIterator._enumerate_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch > \u001b[32m0\u001b[39m:\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._current_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m         \u001b[38;5;28mself\u001b[39m._current_iterator = \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m._get_iterator())\n\u001b[32m    103\u001b[39m         \u001b[38;5;28mself\u001b[39m._steps_seen = \u001b[32m0\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, steps_per_epoch, \u001b[38;5;28mself\u001b[39m.steps_per_execution):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[39m, in \u001b[36mDatasetV2.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops.inside_function():\n\u001b[32m    500\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33miteration in eager mode or within tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[39m, in \u001b[36mOwnedIterator.__init__\u001b[39m\u001b[34m(self, dataset, components, element_spec)\u001b[39m\n\u001b[32m    705\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    707\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    708\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnot be specified.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[38;5;28mself\u001b[39m._get_next_call_count = \u001b[32m0\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[39m, in \u001b[36mOwnedIterator._create_iterator\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    745\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype.args[\u001b[32m0\u001b[39m].args[\u001b[32m0\u001b[39m].args) == \u001b[38;5;28mlen\u001b[39m(\n\u001b[32m    746\u001b[39m       \u001b[38;5;28mself\u001b[39m._flat_output_types)\n\u001b[32m    747\u001b[39m   \u001b[38;5;28mself\u001b[39m._iterator_resource.op.experimental_set_type(fulltype)\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\miniforge3\\envs\\proyek_skripsi_lokal\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3478\u001b[39m, in \u001b[36mmake_iterator\u001b[39m\u001b[34m(dataset, iterator, name)\u001b[39m\n\u001b[32m   3476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m   3477\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3478\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3479\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMakeIterator\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   3481\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c9a3a5673364c208514e925fec18482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b47712971af465fa4475f9f83f9e100",
              "IPY_MODEL_746ce13448ca4fa7a5e1df46ffc0fdcb",
              "IPY_MODEL_d8e7489b9a714ff68361f769b159a12c"
            ],
            "layout": "IPY_MODEL_dfbc25ac0d4d450c88ae1ab3b83987e6",
            "tabbable": null,
            "tooltip": null
          }
        },
        "4b47712971af465fa4475f9f83f9e100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_a76a4dab989148e78d8cb2879758e47e",
            "placeholder": "​",
            "style": "IPY_MODEL_ef3485b5196e4d58b9849c9b57c62f11",
            "tabbable": null,
            "tooltip": null,
            "value": "  0%"
          }
        },
        "746ce13448ca4fa7a5e1df46ffc0fdcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_47a05c782ff14713b3111b03f3770c22",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b42011dcdb58498aa994d25752e7c2fd",
            "tabbable": null,
            "tooltip": null,
            "value": 0
          }
        },
        "d8e7489b9a714ff68361f769b159a12c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_dea02d13c84c43fcabb3d895439b1c6a",
            "placeholder": "​",
            "style": "IPY_MODEL_4526ce3d18574a5b8631bb1c924c120a",
            "tabbable": null,
            "tooltip": null,
            "value": " 0/15 [00:02&lt;?, ?it/s]"
          }
        },
        "dfbc25ac0d4d450c88ae1ab3b83987e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a76a4dab989148e78d8cb2879758e47e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef3485b5196e4d58b9849c9b57c62f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "47a05c782ff14713b3111b03f3770c22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b42011dcdb58498aa994d25752e7c2fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dea02d13c84c43fcabb3d895439b1c6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4526ce3d18574a5b8631bb1c924c120a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}